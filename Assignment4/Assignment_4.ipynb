{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from IPython import display\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you will reuse the dataset you downloaded in assignment 2. This dataset contains a very large set of images, approximately 80K training images and 100 validation images, with multiple tags for each image. However that data *lacks captions* for the images, which is **vital** for this assignment. To obtain the captions for this assignment, download a few data files as shown below and add them to your `data/annotations` folder from assignment 2.\n",
    "\n",
    "`wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a4_data.zip`\n",
    "\n",
    "Following the data downloading and unzipping, the code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  0.0039  0.0078  0.0039  ...   0.0471  0.0471  0.0314\n",
       "  0.0039  0.0039  0.0039  ...   0.0353  0.0353  0.0392\n",
       "  0.0039  0.0039  0.0039  ...   0.0392  0.0392  0.0510\n",
       "           ...             ⋱             ...          \n",
       "  0.7137  0.7294  0.7137  ...   0.1686  0.1843  0.1686\n",
       "  0.7059  0.6902  0.6863  ...   0.1765  0.1804  0.2039\n",
       "  0.6784  0.6667  0.6706  ...   0.1922  0.2157  0.2275\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0.1490  0.1490  0.1412  ...   0.0039  0.0039  0.0039\n",
       "  0.1451  0.1412  0.1373  ...   0.0039  0.0039  0.0039\n",
       "  0.1412  0.1373  0.1373  ...   0.0039  0.0039  0.0039\n",
       "           ...             ⋱             ...          \n",
       "  0.4392  0.4667  0.4549  ...   0.2588  0.2745  0.2863\n",
       "  0.4353  0.4235  0.4196  ...   0.2745  0.2980  0.3137\n",
       "  0.4118  0.4000  0.4000  ...   0.3020  0.3176  0.3020\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0.5294  0.5294  0.5294  ...   0.1451  0.1412  0.1333\n",
       "  0.5255  0.5333  0.5373  ...   0.1725  0.1451  0.1412\n",
       "  0.5373  0.5490  0.5451  ...   0.2314  0.1843  0.1608\n",
       "           ...             ⋱             ...          \n",
       "  0.0118  0.0078  0.0078  ...   0.5216  0.5294  0.5137\n",
       "  0.0078  0.0078  0.0118  ...   0.5098  0.5216  0.5216\n",
       "  0.0078  0.0118  0.0039  ...   0.5294  0.5255  0.4784\n",
       "[torch.cuda.FloatTensor of size 1x3x224x224 (GPU 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load annotations file for the training images.\n",
    "mscoco_train = json.load(open('data/annotations/train_captions.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "\n",
    "# Extract out the captions for the training images\n",
    "train_id_set = set(train_ids)\n",
    "train_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    if entry['image_id'] in train_id_set:\n",
    "        train_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the validation images.\n",
    "mscoco_val = json.load(open('data/annotations/val_captions.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# Extract out the captions for the validation images\n",
    "val_id_set = set(val_ids)\n",
    "val_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    if entry['image_id'] in val_id_set:\n",
    "        val_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the testing images\n",
    "mscoco_test = json.load(open('data/annotations/test_captions.json'))\n",
    "test_ids = [entry['id'] for entry in mscoco_test['images']]\n",
    "test_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_test['images']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We do the same preprocessing done in assignment 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [sentence for caption_set in train_id_to_captions.values() for sentence in caption_set]\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentences, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a list of reference sentences, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = [word_tokenize(ref_sent.lower()) for ref_sent in reference_sentences]\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu(reference_tokenized, predicted_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Image Encoder\n",
    "\n",
    "We load in the pre-trained VGG-16 model, and remove the final layer, as done in assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VggMinusOneModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "vgg_model = models.vgg16(pretrained=True).cuda()\n",
    "\n",
    "class VggMinusOneModel(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        \"\"\"\n",
    "        When constructing the model, we initialize two linear modules and assign them\n",
    "        as class fields. We also, as done earlier, remove the final layer of the vgg model.\n",
    "        \"\"\"\n",
    "        super(VggMinusOneModel, self).__init__()\n",
    "        self.features = vgg_model.features\n",
    "        self.classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the network, applying the sigmoid activation function after each layer.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup a Language Decoder\n",
    "\n",
    "We're going to reuse our decoder from Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "use_cuda = True\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output.squeeze())\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, init_size, image_features):\n",
    "        self.project = nn.Linear(init_size, self.hidden_size).cuda()\n",
    "        result = self.project(image_features)\n",
    "        # result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train encoder-decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "def train(input_image,\n",
    "          input_variables, \n",
    "          target_variables, \n",
    "          input_lens,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings, \n",
    "          teacher_force=True,\n",
    "          train_encoder=False):\n",
    "    if train_encoder:\n",
    "        encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variables.size()[0]\n",
    "    target_length = target_variables.size()[0]\n",
    "\n",
    "    # Pass through the encoder\n",
    "    image_features = encoder(input_image)\n",
    "    \n",
    "    \n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]\n",
    "                                                for i in range(input_variables.size(1))]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    # Set the initial hidden state of the decoder to be the last hidden state of the encoder\n",
    "    last_hidden = torch.stack([decoder.initHidden(image_features.size(1), image_features).squeeze() \n",
    "                               for i,length in enumerate(input_lens)]).unsqueeze(0)\n",
    "    decoder_hidden = (last_hidden, last_hidden)\n",
    "\n",
    "    # Prepare the results tensor\n",
    "    all_decoder_outputs = Variable(torch.zeros(*input_variables.size()))\n",
    "    if use_cuda:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        \n",
    "    all_decoder_outputs[0] = decoder_input\n",
    "        \n",
    "    # Iterate over the indices after the first.\n",
    "    for t in range(1,target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        if random.random() <= 0.3:\n",
    "            decoder_input = input_variables[t].unsqueeze(0)\n",
    "        else:\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "                       \n",
    "            #Prepare the inputs\n",
    "            decoder_input = torch.stack([Variable(torch.FloatTensor(embeddings[ni])).cuda()\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "        \n",
    "        # Save the decoder output\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        \n",
    "    loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        target_variables.transpose(0,1).contiguous(), \n",
    "                        Variable(torch.LongTensor(input_lens)).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 10.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "    if train_encoder:\n",
    "        encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "def pad_seq(arr, length, pad_token):\n",
    "    \"\"\"\n",
    "    Pad an array to a length with a token.\n",
    "    \"\"\"\n",
    "    if len(arr) == length:\n",
    "        return np.array(arr)\n",
    "    \n",
    "    return np.concatenate((arr, [pad_token]*(length - len(arr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = VggMinusOneModel(vgg_model)\n",
    "decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.658143520355225\n",
      "a with with with with with <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "and <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "on on with with with with <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "tables long with with with with with <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "items with with with with with <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "100 5.81161642074585\n",
      "200 4.560781478881836\n",
      "300 4.709859848022461\n",
      "400 4.135005474090576\n",
      "500 4.423890113830566\n",
      "600 4.515563011169434\n",
      "700 3.8934741020202637\n",
      "800 4.984233856201172\n",
      "900 4.646512508392334\n",
      "1000 4.798314571380615\n",
      "a man of a <UNK> a a <UNK> <UNK> . <EOS>\n",
      "a <UNK> of <UNK> a a <UNK> a <UNK> . <EOS>\n",
      "a <UNK> of a <UNK> a <UNK> a <UNK> . <EOS>\n",
      "a <UNK> of a <UNK> a a <UNK> <UNK> . <EOS>\n",
      "a <UNK> of a <UNK> a <UNK> a <UNK> . <EOS>\n",
      "1100 4.486855506896973\n",
      "1200 4.9522528648376465\n",
      "1300 4.615684986114502\n",
      "1400 4.49144983291626\n",
      "1500 4.500072002410889\n",
      "1600 4.135714530944824\n",
      "1700 3.6661367416381836\n",
      "1800 4.135376930236816\n",
      "1900 4.249927520751953\n",
      "2000 4.044854164123535\n",
      "a <UNK> is <UNK> a <UNK> <UNK> <UNK> a <UNK> . <EOS>\n",
      "a <UNK> of with a <UNK> <UNK> <UNK> . <EOS>\n",
      "a <UNK> of with a <UNK> <UNK> <UNK> . <EOS>\n",
      "a man is <UNK> a <UNK> <UNK> <UNK> a <UNK> . <EOS>\n",
      "a <UNK> is <UNK> a <UNK> <UNK> <UNK> a <UNK> . <EOS>\n",
      "2100 3.484940767288208\n",
      "2200 4.393788814544678\n",
      "2300 3.9206807613372803\n"
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 5\n",
    "for _ in range(num_epochs):\n",
    "    for i, train_id in enumerate(train_ids):\n",
    "        # Get the sentences in the batch\n",
    "        img = load_image(train_id_to_file[train_id])\n",
    "        sentences = train_id_to_captions[train_id]\n",
    "        \n",
    "        # Get the sentence lengths\n",
    "        sentence_lens = [len(preprocess_numberize(sentence)) for sentence in sentences]\n",
    "        \n",
    "        # Sort by the sentence lengths\n",
    "        sorted_indices = sorted(list(range(len(sentence_lens))), key=lambda i: sentence_lens[i], reverse=True)\n",
    "        sentences = [sentences[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "        \n",
    "        # Filter out 0 sentence lengths\n",
    "        sentence_lens = [sentence_lens[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "        \n",
    "        # Determine length to pad everything to\n",
    "        max_len = max(sentence_lens)\n",
    "        \n",
    "        # Preprocess all of the sentences in each batch\n",
    "        one_hot_embedded_list = [preprocess_one_hot(sentence) for sentence in sentences]\n",
    "        one_hot_embedded_list_padded = [pad_seq(embed, max_len, np.zeros(len(vocabulary))) \n",
    "                                        for embed in one_hot_embedded_list]\n",
    "                \n",
    "        numberized_list = [preprocess_numberize(sentence) for sentence in sentences]\n",
    "        numberized_list_padded = [pad_seq(numb, max_len, 0).astype(torch.LongTensor) for numb in numberized_list]\n",
    "                \n",
    "        # Convert to variables\n",
    "        input_variable = Variable(torch.FloatTensor(one_hot_embedded_list_padded)).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(numberized_list_padded)).cuda()\n",
    "        \n",
    "        # Transpose from batch_size x max_seq_len x vocab_size to max_seq_len x batch_size x vocab_size\n",
    "        input_variable = input_variable.transpose(0, 1)\n",
    "        target_variable = target_variable.transpose(0, 1)\n",
    "\n",
    "        loss = train(img,\n",
    "                     input_variable,\n",
    "                     target_variable, \n",
    "                     sentence_lens,\n",
    "                     encoder,\n",
    "                     decoder, \n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer, \n",
    "                     criterion)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(i,loss)\n",
    "            \n",
    "        if i % 1000 == 0:\n",
    "            print(seq2seq_inference(load_image(train_id_to_file[train_ids[0]])))\n",
    "            print(seq2seq_inference(load_image(train_id_to_file[train_ids[1]])))\n",
    "            print(seq2seq_inference(load_image(train_id_to_file[train_ids[2]])))\n",
    "            print(seq2seq_inference(load_image(train_id_to_file[train_ids[3]])))\n",
    "            print(seq2seq_inference(load_image(train_id_to_file[train_ids[4]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with with with with with with with with with with with with with with with with with with with\n",
      "they top top with with with with with with with with with with with with with with with with\n",
      "no of surrounded with with with with with with with with with with with with with with with with\n",
      "surrounded surrounded surrounded with with with with with with with with with with with with with with with with\n",
      "the the middle with with with with with with with with with with with with with with with with\n"
     ]
    }
   ],
   "source": [
    "def seq2seq_inference(input_image, embeddings=one_hot_embeddings, max_length=20):\n",
    "    image_features = encoder(input_image)\n",
    "\n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    # Set the initial hidden state of the decoder to be the last hidden state of the encoder\n",
    "    last_hidden = decoder.initHidden(image_features.size(1), image_features).unsqueeze(0)\n",
    "    decoder_hidden = (last_hidden, last_hidden)\n",
    "    \n",
    "    # Iterate over the indices after the first.\n",
    "    decoder_outputs = []\n",
    "    for t in range(1,max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        # Get the top result\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        decoder_outputs.append(ni)\n",
    "\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "        \n",
    "        #Prepare the inputs\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return ' '.join(vocabulary[i] for i in decoder_outputs)\n",
    "\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[0]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[1]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[2]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[3]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[4]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MAP and Sampling Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate performance\n",
    "\n",
    "For validation images compute the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
