{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from IPython import display\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you will reuse the dataset you downloaded in assignment 2. This dataset contains a very large set of images, approximately 80K training images and 100 validation images, with multiple tags for each image. However that data *lacks captions* for the images, which is **vital** for this assignment. To obtain the captions for this assignment, download a few data files as shown below and add them to your `data/annotations` folder from assignment 2.\n",
    "\n",
    "`wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a4_data.zip`\n",
    "\n",
    "Following the data downloading and unzipping, the code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  0.0039  0.0078  0.0039  ...   0.0471  0.0471  0.0314\n",
       "  0.0039  0.0039  0.0039  ...   0.0353  0.0353  0.0392\n",
       "  0.0039  0.0039  0.0039  ...   0.0392  0.0392  0.0510\n",
       "           ...             ⋱             ...          \n",
       "  0.7137  0.7294  0.7137  ...   0.1686  0.1843  0.1686\n",
       "  0.7059  0.6902  0.6863  ...   0.1765  0.1804  0.2039\n",
       "  0.6784  0.6667  0.6706  ...   0.1922  0.2157  0.2275\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0.1490  0.1490  0.1412  ...   0.0039  0.0039  0.0039\n",
       "  0.1451  0.1412  0.1373  ...   0.0039  0.0039  0.0039\n",
       "  0.1412  0.1373  0.1373  ...   0.0039  0.0039  0.0039\n",
       "           ...             ⋱             ...          \n",
       "  0.4392  0.4667  0.4549  ...   0.2588  0.2745  0.2863\n",
       "  0.4353  0.4235  0.4196  ...   0.2745  0.2980  0.3137\n",
       "  0.4118  0.4000  0.4000  ...   0.3020  0.3176  0.3020\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0.5294  0.5294  0.5294  ...   0.1451  0.1412  0.1333\n",
       "  0.5255  0.5333  0.5373  ...   0.1725  0.1451  0.1412\n",
       "  0.5373  0.5490  0.5451  ...   0.2314  0.1843  0.1608\n",
       "           ...             ⋱             ...          \n",
       "  0.0118  0.0078  0.0078  ...   0.5216  0.5294  0.5137\n",
       "  0.0078  0.0078  0.0118  ...   0.5098  0.5216  0.5216\n",
       "  0.0078  0.0118  0.0039  ...   0.5294  0.5255  0.4784\n",
       "[torch.cuda.FloatTensor of size 1x3x224x224 (GPU 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load annotations file for the training images.\n",
    "mscoco_train = json.load(open('data/annotations/train_captions.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "\n",
    "# Extract out the captions for the training images\n",
    "train_id_set = set(train_ids)\n",
    "train_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    if entry['image_id'] in train_id_set:\n",
    "        train_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the validation images.\n",
    "mscoco_val = json.load(open('data/annotations/val_captions.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# Extract out the captions for the validation images\n",
    "val_id_set = set(val_ids)\n",
    "val_id_to_captions = defaultdict(list)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    if entry['image_id'] in val_id_set:\n",
    "        val_id_to_captions[entry['image_id']].append(entry['caption'])\n",
    "\n",
    "# Load annotations file for the testing images\n",
    "mscoco_test = json.load(open('data/annotations/test_captions.json'))\n",
    "test_ids = [entry['id'] for entry in mscoco_test['images']]\n",
    "test_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_test['images']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We do the same preprocessing done in assignment 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [sentence for caption_set in train_id_to_captions.values() for sentence in caption_set]\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentences, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a list of reference sentences, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = [word_tokenize(ref_sent.lower()) for ref_sent in reference_sentences]\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu(reference_tokenized, predicted_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Image Encoder\n",
    "\n",
    "We load in the pre-trained VGG-16 model, and remove the final layer, as done in assignment 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VggMinusOneModel(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d (3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d (128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d (256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096)\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Linear(in_features=4096, out_features=4096)\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code goes here\n",
    "vgg_model = models.vgg16(pretrained=True).cuda()\n",
    "\n",
    "class VggMinusOneModel(torch.nn.Module):\n",
    "    def __init__(self, vgg_model):\n",
    "        \"\"\"\n",
    "        When constructing the model, we initialize two linear modules and assign them\n",
    "        as class fields. We also, as done earlier, remove the final layer of the vgg model.\n",
    "        \"\"\"\n",
    "        super(VggMinusOneModel, self).__init__()\n",
    "        self.features = vgg_model.features\n",
    "        self.classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the network, applying the sigmoid activation function after each layer.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "vggMinusOne = VggMinusOneModel(vgg_model)\n",
    "vggMinusOne\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Setup a Language Decoder\n",
    "\n",
    "We're going to reuse our decoder from Assignment 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "use_cuda = True\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output.squeeze())\n",
    "        return output.unsqueeze(0), hidden\n",
    "\n",
    "    def initHidden(self, init_size, image_features):\n",
    "        self.project = nn.Linear(init_size, self.hidden_size).cuda()\n",
    "        result = self.project(image_features)\n",
    "        # result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train encoder-decoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "def train(input_image,\n",
    "          input_variables, \n",
    "          target_variables, \n",
    "          input_lens,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings, \n",
    "          teacher_force=True,\n",
    "          train_encoder=False):\n",
    "    if train_encoder:\n",
    "        encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variables.size()[0]\n",
    "    target_length = target_variables.size()[0]\n",
    "\n",
    "    # Pass through the encoder\n",
    "    image_features = encoder(input_image)\n",
    "    \n",
    "    \n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]\n",
    "                                                for i in range(input_variables.size(1))]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    # Set the initial hidden state of the decoder to be the last hidden state of the encoder\n",
    "    last_hidden = torch.stack([decoder.initHidden(image_features.size(1), image_features).squeeze() \n",
    "                               for i,length in enumerate(input_lens)]).unsqueeze(0)\n",
    "    decoder_hidden = (last_hidden, last_hidden)\n",
    "\n",
    "    # Prepare the results tensor\n",
    "    all_decoder_outputs = Variable(torch.zeros(*input_variables.size()))\n",
    "    if use_cuda:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        \n",
    "    all_decoder_outputs[0] = decoder_input\n",
    "        \n",
    "    # Iterate over the indices after the first.\n",
    "    for t in range(1,target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        if random.random() <= 0.3:\n",
    "            decoder_input = input_variables[t].unsqueeze(0)\n",
    "        else:\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "                       \n",
    "            #Prepare the inputs\n",
    "            decoder_input = torch.stack([Variable(torch.FloatTensor(embeddings[ni])).cuda()\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "        \n",
    "        # Save the decoder output\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        \n",
    "    loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        target_variables.transpose(0,1).contiguous(), \n",
    "                        Variable(torch.LongTensor(input_lens)).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 10.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "    if train_encoder:\n",
    "        encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "def pad_seq(arr, length, pad_token):\n",
    "    \"\"\"\n",
    "    Pad an array to a length with a token.\n",
    "    \"\"\"\n",
    "    if len(arr) == length:\n",
    "        return np.array(arr)\n",
    "    \n",
    "    return np.concatenate((arr, [pad_token]*(length - len(arr))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = VggMinusOneModel(vgg_model)\n",
    "decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.9257  1.1044 -0.9747  ...   0.6481 -0.3852  0.9975\n",
      " -0.6596 -0.7308  0.5060  ...   1.1433  0.7249  0.2053\n",
      " -1.3106  0.7272 -0.8236  ...   0.4914 -0.1970  0.1099\n",
      "  0.9525 -0.4777 -0.0022  ...   1.3745 -0.0420  0.1207\n",
      " -1.1089 -0.4431  1.0188  ...   0.6058  0.0268 -0.4125\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.510356903076172\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.6390 -0.1767 -0.1796  ...  -0.0269 -0.3592 -0.7425\n",
      " -0.9217 -0.2325 -0.6827  ...  -0.1601  0.7287 -0.2234\n",
      "  0.4745 -1.2511 -1.5534  ...   0.3004  0.3927  0.1781\n",
      "  0.3072  0.6949 -0.3287  ...   0.2721 -0.0474  0.2230\n",
      " -0.3866 -0.2335 -0.6049  ...  -0.3465 -0.3086  0.6670\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.3433  0.5123 -0.2208  ...  -0.0675  0.3179  0.4164\n",
      "  0.4636 -1.3132  1.2998  ...  -0.0678 -0.3261 -0.4601\n",
      " -0.5256  1.1358 -1.1867  ...   0.3347 -0.1623 -0.2360\n",
      " -0.0414  0.0781  0.2953  ...   0.8919  0.2110 -0.6048\n",
      "  0.2293 -0.6745  0.1576  ...   1.1106  0.0052  0.2283\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.7908  0.2930  0.1519  ...  -0.3262 -0.1838  0.4456\n",
      "  0.0562 -0.0975  0.2607  ...  -0.2443  0.1781 -0.0278\n",
      "  0.3330 -0.1083  0.3781  ...   0.0572  0.6487  0.1218\n",
      "  0.7294  0.6381  0.4648  ...   0.0119  0.4912  0.1718\n",
      "  0.3615  0.1762 -0.2457  ...  -0.6255 -0.8823 -0.3050\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.2810  0.3516 -0.2617  ...   0.7297 -0.2166  0.3651\n",
      "  0.0395 -0.1944  0.1129  ...   0.1895  0.6292  0.4497\n",
      " -0.7323 -0.0437  0.0081  ...   0.0877  0.3755  0.1742\n",
      "  0.3524  0.1988 -0.0787  ...  -0.4635 -0.6018  0.3455\n",
      " -0.2055 -0.3313  0.3221  ...   0.3806  0.0690 -0.0333\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.1692 -0.1582  0.4953  ...  -0.7090  0.2198 -0.4207\n",
      "  0.1585 -0.0096 -0.3563  ...  -0.3277  0.0585 -0.3680\n",
      "  0.2390  0.4097  0.5243  ...   0.3868 -0.1988 -0.2190\n",
      "  0.0496  0.5038  0.0384  ...  -0.1514 -0.4570  0.1642\n",
      "  0.5871 -0.2527 -0.2568  ...   0.1626  0.6755  0.0726\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.8798 -0.2698 -0.7935  ...   0.1098  0.4650 -0.2330\n",
      " -0.1606  0.1649  0.0592  ...  -0.2265 -0.2384  0.4423\n",
      "  0.2435 -0.5249 -0.4129  ...  -1.5713 -0.1240  0.0638\n",
      " -0.0578  0.5040 -0.6761  ...   0.6558  0.0046  0.4144\n",
      " -0.1594  0.3676  0.8853  ...  -0.4038 -0.3903  0.3348\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.0214  0.8167  0.2643  ...   0.2105 -0.1610  0.2241\n",
      "  0.1374 -0.0127 -0.1504  ...  -0.1609  0.2600  0.0457\n",
      " -0.3574 -0.0755 -0.1585  ...   0.8526 -0.5719  0.3977\n",
      "  0.5362  0.2574  0.2803  ...  -0.4450 -0.3623  0.0583\n",
      " -0.2920 -0.6174  0.3833  ...   0.3240  0.6575 -0.6844\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.6300 -0.4727  0.4181  ...  -0.4103 -0.3777  0.8188\n",
      " -0.3028  0.5646 -0.1757  ...   0.1658 -0.3544 -0.6459\n",
      "  0.0196 -0.3251  0.1208  ...  -0.0805  1.4941  0.5251\n",
      " -0.7000  0.1998 -0.1429  ...   0.4248 -0.5604  0.1766\n",
      " -0.0071  0.1903  0.8651  ...  -0.1632  0.3578 -0.3024\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.4934  0.1560 -0.8656  ...  -0.0264 -0.0391  0.0960\n",
      "  0.0547  0.7583  0.2736  ...  -0.2328  0.3063 -0.3744\n",
      "  0.9332  0.5388 -0.3373  ...  -0.4944  0.3708  0.5625\n",
      " -0.0894  0.7882 -0.1855  ...  -0.1550 -0.0583 -0.1355\n",
      " -0.6518 -0.2989  0.2311  ...   0.3635  0.5266  0.6155\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.0408 -0.1656 -0.1240  ...  -0.4009 -0.3569  0.3003\n",
      "  0.5524  1.2178  0.4007  ...   0.1376  0.0127 -0.0281\n",
      "  0.0390  0.4540 -0.7903  ...  -0.0209  0.3988 -0.0736\n",
      " -0.3440  0.0489  0.2778  ...  -0.1001  0.4598  0.0067\n",
      " -0.2549 -0.0717  0.2335  ...  -0.6122  0.3480  0.1661\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.1504 -0.1277  0.3937  ...   0.0009 -1.1976 -0.5364\n",
      " -0.0008 -0.2163 -0.2494  ...   0.3187 -0.8065 -0.1752\n",
      "  0.0094 -0.2548  0.0834  ...  -0.5039  0.3678  0.0920\n",
      " -0.3893  0.2957  0.0783  ...  -0.1631 -1.1224 -0.3921\n",
      " -0.2611 -0.4867 -0.2476  ...  -0.1716 -0.3827  0.8433\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.2362 -0.0642 -0.1566  ...  -0.4324  0.7619 -0.2599\n",
      " -0.2606  0.1235 -0.5901  ...   0.7361  0.6187 -1.1658\n",
      " -0.3779 -0.0725  0.3623  ...  -0.4582  0.5448 -0.0221\n",
      " -1.0164 -0.3813 -0.1918  ...   0.3206 -0.1979 -0.5842\n",
      " -0.2845  0.2314  0.2119  ...   0.0367  0.4980 -0.6982\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      " -0.1479  0.1503 -0.5486  ...   0.4508  0.0016 -0.1517\n",
      "  0.2422  0.1086 -0.1717  ...   0.0897 -0.2359  0.3040\n",
      " -0.2257  0.3409 -0.1053  ...  -0.1255  0.6745  0.3846\n",
      " -0.0940  0.3135 -0.2280  ...   0.0549  0.1921  0.5106\n",
      "  0.1215  0.4084 -0.1455  ...  -0.0822  0.0966  0.1474\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "( 0 ,.,.) = \n",
      "  0.2259  0.6988 -0.4053  ...   0.2359 -0.0894  0.4143\n",
      "  0.6391 -0.2220  0.1553  ...   0.0512  0.3007 -0.1115\n",
      "  0.3140 -0.4573  0.4411  ...  -0.4088  0.1413 -0.2455\n",
      " -0.1900  0.3000 -0.4569  ...   0.1445  0.0221 -0.7526\n",
      " -0.4540  0.2475 -0.2035  ...   0.4827  0.6354 -0.0196\n",
      "[torch.cuda.FloatTensor of size 1x5x300 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-e943f1cc39c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m                      \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                      \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                      criterion)\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-90-19f816f3802d>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_image, input_variables, target_variables, input_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, embeddings, teacher_force, train_encoder)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtrain_encoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[1;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m             \u001b[0mparam_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 5\n",
    "for _ in range(num_epochs):\n",
    "    for i, train_id in enumerate(train_ids):\n",
    "        # Get the sentences in the batch\n",
    "        img = load_image(train_id_to_file[train_id])\n",
    "        sentences = train_id_to_captions[train_id]\n",
    "        \n",
    "        # Get the sentence lengths\n",
    "        sentence_lens = [len(preprocess_numberize(sentence)) for sentence in sentences]\n",
    "        \n",
    "        # Sort by the sentence lengths\n",
    "        sorted_indices = sorted(list(range(len(sentence_lens))), key=lambda i: sentence_lens[i], reverse=True)\n",
    "        sentences = [sentences[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "        \n",
    "        # Filter out 0 sentence lengths\n",
    "        sentence_lens = [sentence_lens[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "        \n",
    "        # Determine length to pad everything to\n",
    "        max_len = max(sentence_lens)\n",
    "        \n",
    "        # Preprocess all of the sentences in each batch\n",
    "        one_hot_embedded_list = [preprocess_one_hot(sentence) for sentence in sentences]\n",
    "        one_hot_embedded_list_padded = [pad_seq(embed, max_len, np.zeros(len(vocabulary))) \n",
    "                                        for embed in one_hot_embedded_list]\n",
    "                \n",
    "        numberized_list = [preprocess_numberize(sentence) for sentence in sentences]\n",
    "        numberized_list_padded = [pad_seq(numb, max_len, 0).astype(torch.LongTensor) for numb in numberized_list]\n",
    "                \n",
    "        # Convert to variables\n",
    "        input_variable = Variable(torch.FloatTensor(one_hot_embedded_list_padded)).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(numberized_list_padded)).cuda()\n",
    "        \n",
    "        # Transpose from batch_size x max_seq_len x vocab_size to max_seq_len x batch_size x vocab_size\n",
    "        input_variable = input_variable.transpose(0, 1)\n",
    "        target_variable = target_variable.transpose(0, 1)\n",
    "\n",
    "        loss = train(img,\n",
    "                     input_variable,\n",
    "                     target_variable, \n",
    "                     sentence_lens,\n",
    "                     encoder,\n",
    "                     decoder, \n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer, \n",
    "                     criterion)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(i,loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6900\n",
      " 0.0247\n",
      "-0.0205\n",
      "-1.0356\n",
      "-0.3491\n",
      "-0.1649\n",
      " 0.2792\n",
      " 0.1113\n",
      "-0.3156\n",
      "-0.2398\n",
      " 0.5470\n",
      " 0.4465\n",
      "-0.6144\n",
      "-0.0315\n",
      "-0.3382\n",
      " 0.1465\n",
      "-1.1278\n",
      " 0.2357\n",
      " 1.1728\n",
      "-0.1929\n",
      " 0.8628\n",
      " 0.0968\n",
      "-0.7863\n",
      " 0.8773\n",
      "-0.5609\n",
      "-0.3402\n",
      " 0.1330\n",
      "-0.3282\n",
      "-0.3592\n",
      "-0.0334\n",
      " 0.3048\n",
      " 0.2350\n",
      "-0.2895\n",
      "-0.1549\n",
      " 2.1194\n",
      " 0.5573\n",
      "-0.3420\n",
      "-1.3992\n",
      "-0.3550\n",
      " 0.8151\n",
      " 0.5322\n",
      "-0.0855\n",
      " 0.1680\n",
      "-0.8578\n",
      "-1.1371\n",
      "-0.3064\n",
      "-1.4733\n",
      " 1.3607\n",
      "-0.4748\n",
      "-0.6319\n",
      " 1.1919\n",
      " 0.2849\n",
      "-0.3120\n",
      " 1.0487\n",
      "-0.4046\n",
      " 0.7699\n",
      " 0.3196\n",
      " 0.2301\n",
      "-0.0129\n",
      "-1.1962\n",
      " 0.1642\n",
      "-0.1505\n",
      " 0.3206\n",
      " 1.0369\n",
      " 0.0443\n",
      "-0.1003\n",
      " 1.0937\n",
      "-0.1749\n",
      " 0.1556\n",
      " 0.5714\n",
      " 0.2870\n",
      " 1.0912\n",
      "-0.3538\n",
      " 0.7328\n",
      " 0.2391\n",
      "-1.5871\n",
      "-0.2994\n",
      "-0.9087\n",
      "-0.1741\n",
      "-0.2047\n",
      "-0.4771\n",
      "-1.1908\n",
      "-0.0373\n",
      " 1.4655\n",
      "-0.2772\n",
      " 1.1481\n",
      "-1.1461\n",
      " 1.2535\n",
      " 0.8086\n",
      "-2.2125\n",
      "-0.0352\n",
      "-0.0641\n",
      " 0.1014\n",
      "-0.8950\n",
      " 0.9669\n",
      "-0.4035\n",
      "-1.6101\n",
      " 0.8782\n",
      " 0.3269\n",
      " 0.4748\n",
      " 0.1439\n",
      "-0.3359\n",
      " 0.8900\n",
      "-0.7650\n",
      "-0.6208\n",
      "-0.1399\n",
      "-0.2916\n",
      " 1.0608\n",
      "-0.0043\n",
      "-0.2952\n",
      " 0.5623\n",
      " 0.1293\n",
      "-0.9467\n",
      "-1.7711\n",
      "-0.7598\n",
      "-0.0122\n",
      "-0.2835\n",
      " 1.0413\n",
      " 0.6737\n",
      "-0.5844\n",
      "-1.1578\n",
      "-0.0541\n",
      " 0.6333\n",
      "-0.7155\n",
      " 0.6150\n",
      "-0.0984\n",
      " 0.5407\n",
      " 0.5910\n",
      "-0.4088\n",
      "-0.6224\n",
      " 0.8298\n",
      "-0.3401\n",
      " 1.2857\n",
      " 1.3528\n",
      "-0.0530\n",
      " 0.3197\n",
      " 0.1144\n",
      " 0.6334\n",
      " 0.0566\n",
      " 0.2569\n",
      " 0.1357\n",
      " 0.2149\n",
      "-0.8333\n",
      "-0.6618\n",
      "-0.0038\n",
      "-0.6946\n",
      "-0.3954\n",
      "-0.3952\n",
      "-0.3105\n",
      " 0.6590\n",
      " 0.0461\n",
      " 0.2812\n",
      " 0.6240\n",
      "-1.0427\n",
      "-0.4871\n",
      " 1.5342\n",
      " 0.3691\n",
      " 1.5273\n",
      " 1.0755\n",
      " 0.5203\n",
      " 0.4742\n",
      "-0.3171\n",
      "-0.0949\n",
      "-0.1887\n",
      " 0.2480\n",
      " 0.0060\n",
      " 0.9739\n",
      "-1.1399\n",
      "-1.5845\n",
      " 0.1711\n",
      "-0.2074\n",
      "-0.4278\n",
      "-0.2452\n",
      "-0.7850\n",
      "-0.4882\n",
      "-0.6932\n",
      "-0.0923\n",
      " 0.3999\n",
      "-0.4645\n",
      "-0.7640\n",
      " 0.1179\n",
      "-0.1543\n",
      " 1.1510\n",
      " 0.4462\n",
      "-0.0666\n",
      "-1.3927\n",
      "-0.2313\n",
      " 0.2618\n",
      " 0.4926\n",
      "-0.0673\n",
      " 0.6963\n",
      " 0.0767\n",
      "-0.2764\n",
      "-0.6076\n",
      " 0.5149\n",
      "-0.0535\n",
      "-0.8046\n",
      " 0.1152\n",
      " 0.4398\n",
      " 0.2211\n",
      "-0.0253\n",
      "-0.5442\n",
      " 0.0378\n",
      " 0.5286\n",
      " 1.1003\n",
      "-0.0897\n",
      "-0.2338\n",
      "-0.7712\n",
      "-1.1149\n",
      "-1.3060\n",
      " 0.2223\n",
      " 0.2694\n",
      "-0.8242\n",
      " 0.5491\n",
      "-0.4444\n",
      "-0.6631\n",
      "-0.8410\n",
      "-0.5412\n",
      "-0.5226\n",
      " 2.2947\n",
      " 0.1734\n",
      "-0.5555\n",
      " 0.0924\n",
      " 0.1282\n",
      " 0.1871\n",
      " 0.0824\n",
      " 0.5308\n",
      " 0.6129\n",
      " 0.8952\n",
      " 1.1952\n",
      " 0.5687\n",
      "-1.0742\n",
      " 0.7033\n",
      " 0.2547\n",
      "-0.5313\n",
      "-0.5975\n",
      " 0.3804\n",
      " 1.3769\n",
      " 0.6599\n",
      "-0.3205\n",
      "-0.2862\n",
      " 0.1563\n",
      " 0.3792\n",
      "-0.3894\n",
      " 0.2237\n",
      "-0.2937\n",
      "-0.5346\n",
      " 0.1376\n",
      "-0.9921\n",
      "-0.6481\n",
      " 0.3354\n",
      " 0.6696\n",
      "-0.2891\n",
      " 0.2179\n",
      " 0.0635\n",
      " 0.5151\n",
      " 1.4491\n",
      " 0.5423\n",
      " 0.0493\n",
      "-1.0155\n",
      " 0.7532\n",
      " 0.7824\n",
      "-2.0077\n",
      " 0.5667\n",
      " 0.4886\n",
      " 0.6002\n",
      "-0.8725\n",
      "-0.6855\n",
      "-0.0844\n",
      "-0.0506\n",
      " 1.5476\n",
      " 1.8041\n",
      "-0.4982\n",
      "-0.5927\n",
      "-0.7313\n",
      " 0.8211\n",
      " 0.2799\n",
      "-0.2925\n",
      " 1.1160\n",
      "-0.5475\n",
      " 0.1829\n",
      "-0.4947\n",
      " 0.9845\n",
      " 0.6674\n",
      " 0.6811\n",
      " 0.7591\n",
      " 1.5379\n",
      "-1.0354\n",
      "-0.2291\n",
      " 1.2251\n",
      " 0.1194\n",
      " 0.3643\n",
      " 0.5033\n",
      "-0.3733\n",
      "-0.4331\n",
      " 0.3837\n",
      " 0.7223\n",
      " 0.3674\n",
      "-0.1963\n",
      " 0.7065\n",
      "[torch.cuda.FloatTensor of size 300 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 1, 300), got (300,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-a23f74f5d64a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq2seq_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_id_to_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq2seq_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_id_to_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq2seq_inference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mload_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_id_to_file\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-89-a23f74f5d64a>\u001b[0m in \u001b[0;36mseq2seq_inference\u001b[1;34m(input_image, embeddings, max_length)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdecoder_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Get the top result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-7874589986ac>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hidden)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 325\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         )\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[1;34m(self, *input)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mflat_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\function.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\_functions\\rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[1;34m(self, input, weight, hx)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m         \u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\backends\\cudnn\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             raise RuntimeError('Expected hidden size {}, got {}'.format(\n\u001b[1;32m--> 266\u001b[1;33m                 hidden_size, tuple(hx.size())))\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m             raise RuntimeError('Expected cell size {}, got {}'.format(\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden size (1, 1, 300), got (300,)"
     ]
    }
   ],
   "source": [
    "def seq2seq_inference(input_image, embeddings=one_hot_embeddings, max_length=20):\n",
    "    image_features = encoder(input_image)\n",
    "\n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    # Set the initial hidden state of the decoder to be the last hidden state of the encoder\n",
    "    last_hidden = decoder.initHidden(image_features.size(1), image_features).squeeze() \n",
    "    print(last_hidden)\n",
    "    decoder_hidden = (last_hidden, last_hidden)\n",
    "    \n",
    "    # Iterate over the indices after the first.\n",
    "    decoder_outputs = []\n",
    "    for t in range(1,max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        # Get the top result\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        decoder_outputs.append(ni)\n",
    "\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "        \n",
    "        #Prepare the inputs\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return ' '.join(vocabulary[i] for i in decoder_outputs)\n",
    "\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[0]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[1]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[2]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[3]])))\n",
    "print(seq2seq_inference(load_image(train_id_to_file[train_ids[4]])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MAP and Sampling Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluate performance\n",
    "\n",
    "For validation images compute the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
