{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the data and extract it into `data/`. The dataset contains two files, both containing a single caption on each line. We should have 415,795 sentences in the training captions and 500 sentences in the validation captions.\n",
    "\n",
    "To download the data, run the following directly on your server: `wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a3_data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data into memory.\n",
    "train_sentences = [line.strip() for line in open(\"data/mscoco_train_captions.txt\").readlines()]\n",
    "val_sentences = [line.strip() for line in open(\"data/mscoco_val_captions.txt\").readlines()]\n",
    "\n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n",
    "\n",
    "All of the packages utilized should be installed on your Azure servers, however you will have to download an NLTK corpus. To do this, follow the instructions below:\n",
    "\n",
    "1. SSH to your Azure server\n",
    "2. Open up Python interpreter\n",
    "3. `import nltk`\n",
    "4. `nltk.download()`\n",
    "\n",
    "    You should now see something that looks like:\n",
    "\n",
    "    ```\n",
    "    >>> nltk.download()\n",
    "    NLTK Downloader\n",
    "    ---------------------------------------------------------------------------\n",
    "        d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
    "    ---------------------------------------------------------------------------\n",
    "    Downloader> \n",
    "\n",
    "    ```\n",
    "\n",
    "5. `d punkt`\n",
    "6. Provided the download finished successfully, you may now exit out of the Python interpreter and close the SSH connection.\n",
    "\n",
    "Please look through the functions provided below **carefully**, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = F.log_softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a Language Decoder\n",
    "\n",
    "We must now train the language decoder we implemented above. An important thing to pay attention to is the [inputs for an LSTM](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(target_variable, \n",
    "          decoder, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings,\n",
    "          teacher_force=True): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[target_variable[0].data[0]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = (decoder.initHidden(), decoder.initHidden())\n",
    "\n",
    "\n",
    "    for di in range(1,target_variable.size(0)):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "        if teacher_force:\n",
    "            ni = target_variable[di].data[0]\n",
    "        else:          \n",
    "            ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_variable.size(0)\n",
    "\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 1\n",
    "for _ in range(num_epochs):\n",
    "    for i,sentence in enumerate(train_sentences):\n",
    "        numberized = preprocess_numberize(sentence)\n",
    "        if len(numberized) == 2:\n",
    "            continue\n",
    "        target_variable = Variable(torch.LongTensor(numberized[1:])).cuda()\n",
    "            \n",
    "        loss = train(target_variable, decoder, decoder_optimizer, criterion)\n",
    "        if i % 100 == 0:\n",
    "            print(i, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder and test it with a few different starting words. This code will be fairly similar to your training function from part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[init_word]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = (decoder.initHidden(),decoder.initHidden())\n",
    "    decoder_outputs = [word2index[init_word]]\n",
    "    \n",
    "    softmax = nn.Softmax()\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        decoder_outputs.append(ni)\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "            print(topi[0][0])\n",
    "\n",
    "    return \" \".join([vocabulary[word] for word in decoder_outputs])\n",
    "\n",
    "print(inference(decoder, init_word=\"man\"))\n",
    "print(inference(decoder, init_word=\"woman\"))\n",
    "print(inference(decoder, init_word=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building Language Decoder Sampling Inference\n",
    "\n",
    "We must now modify the method defined in part 3, to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[init_word]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    decoder_hidden = (decoder.initHidden(),decoder.initHidden())\n",
    "    decoder_outputs = [word2index[init_word]]\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        probs = np.exp(decoder_output.data[0].cpu().numpy())\n",
    "        sample_sum = probs[0]\n",
    "        random_sample = random()\n",
    "        ni = 0\n",
    "        while sample_sum < random_sample:\n",
    "            ni += 1\n",
    "            sample_sum += probs[ni]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        decoder_outputs.append(ni)\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join([vocabulary[word] for word in decoder_outputs])\n",
    "\n",
    "for i in range(5):\n",
    "    print(sampling_inference(decoder, init_word=\"the\"))\n",
    "    print(sampling_inference(decoder, init_word=\"man\"))\n",
    "    print(sampling_inference(decoder, init_word=\"woman\"))\n",
    "    print(sampling_inference(decoder, init_word=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "encoder = EncoderLSTM(input_size=vocabularySize, hidden_size=300).cuda()\n",
    "decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary)).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Connecting Encoder to Decoder and Training End-to-End\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "It's likely that you'll be able to re-use most of your code from part 2. For our purposes, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(input_variable, \n",
    "          target_variable, \n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings, \n",
    "          teacher_force=True):\n",
    "    encoder_hidden = (encoder.initHidden(),encoder.initHidden())\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    encoder_outputs = Variable(torch.zeros(input_length, encoder.hidden_size))\n",
    "    encoder_outputs = encoder_outputs.cuda() if use_cuda else encoder_outputs\n",
    "\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[:,:,ei,:], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0][0]\n",
    "\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    for di in range(target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        \n",
    "        if teacher_force:\n",
    "            ni = target_variable[di].data[0]\n",
    "        else:          \n",
    "            ni = topi[0][0]\n",
    "\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        loss += criterion(decoder_output, target_variable[di])\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0] / target_length\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.001) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 1\n",
    "for _ in range(num_epochs):\n",
    "    for i,sentence in enumerate(train_sentences):\n",
    "        one_hot_embedded = preprocess_one_hot(sentence)\n",
    "        numberized = preprocess_numberize(sentence)\n",
    "        input_variable = Variable(torch.FloatTensor(one_hot_embedded)).cuda().unsqueeze(0).unsqueeze(1)\n",
    "        target_variable = Variable(torch.LongTensor(numberized[1:])).cuda()\n",
    "        loss = train(input_variable,\n",
    "                     target_variable, \n",
    "                     encoder,\n",
    "                     decoder, \n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer, \n",
    "                     criterion)\n",
    "        if i % 100 == 0:\n",
    "            print(i,loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_inference(sentence, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    one_hot_embedded = preprocess_one_hot(sentence)\n",
    "    input_variable = Variable(torch.FloatTensor(one_hot_embedded)).cuda().unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    encoder_hidden = (encoder.initHidden(),encoder.initHidden())\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[:,:,ei,:], encoder_hidden)\n",
    "\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    decoder_outputs = []\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        decoder_outputs.append(ni)\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "    \n",
    "    return \" \".join(vocabulary[word] for word in decoder_outputs)\n",
    "\n",
    "\n",
    "print(seq2seq_inference(\"The cat in the hat\"))\n",
    "print(seq2seq_inference(\"A dog in the parkThe cat in the hatThe cat in the hat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_bleu = 0\n",
    "for sentence in val_sentences:\n",
    "    predicted = \"<SOS>\" + seq2seq_inference(sentence)\n",
    "    total_bleu = compute_bleu(sentence, predicted)\n",
    "    \n",
    "print(total_bleu/len(val_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_encoder_hidden(sentence):\n",
    "    one_hot_embedded = preprocess_one_hot(sentence)\n",
    "    input_variable = Variable(torch.FloatTensor(one_hot_embedded)).cuda().unsqueeze(0).unsqueeze(1)\n",
    "    \n",
    "    encoder_hidden = (encoder.initHidden(),encoder.initHidden())\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    loss = 0\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(input_variable[:,:,ei,:], encoder_hidden)\n",
    "\n",
    "    return encoder_hidden[0][0,0].data.cpu().numpy()\n",
    "\n",
    "train_hiddens = np.stack([final_encoder_hidden(sentence) for sentence in train_sentences[:1000]])\n",
    "val_hiddens = np.stack([final_encoder_hidden(sentence) for sentence in val_sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,val_hidden in enumerate(val_hiddens[:10]):\n",
    "    closest_idx = min(range(len(train_hiddens)), key=lambda i: np.linalg.norm(train_hiddens[i] - val_hidden))\n",
    "    print(val_sentences[i], \"||\", train_sentences[closest_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Effectiveness of word2vec\n",
    "\n",
    "We now repeat everything done above using word2vec embeddings in place of one-hot embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Batching (Fast!)\n",
    "\n",
    "Now we'll do some work to make the code really fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "\n",
    "# The next two functions are part of some other deep learning frameworks, but PyTorch\n",
    "# has not yet implemented them. We can find some commonly-used open source worked arounds\n",
    "# after searching around a bit: https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1.\n",
    "def _sequence_mask(sequence_length, max_len=None):\n",
    "    if max_len is None:\n",
    "        max_len = sequence_length.data.max()\n",
    "    batch_size = sequence_length.size(0)\n",
    "    seq_range = torch.arange(0, max_len).long()\n",
    "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
    "    seq_range_expand = Variable(seq_range_expand)\n",
    "    if sequence_length.is_cuda:\n",
    "        seq_range_expand = seq_range_expand.cuda()\n",
    "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
    "                         .expand_as(seq_range_expand))\n",
    "    return seq_range_expand < seq_length_expand\n",
    "\n",
    "\n",
    "def compute_loss(logits, target, length):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        logits: A Variable containing a FloatTensor of size\n",
    "            (batch, max_len, num_classes) which contains the\n",
    "            unnormalized probability for each class.\n",
    "        target: A Variable containing a LongTensor of size\n",
    "            (batch, max_len) which contains the index of the true\n",
    "            class for each corresponding step.\n",
    "        length: A Variable containing a LongTensor of size (batch,)\n",
    "            which contains the length of each data in a batch.\n",
    "\n",
    "    Returns:\n",
    "        loss: An average loss value masked by the length.\n",
    "    \"\"\"\n",
    "    # logits_flat: (batch * max_len, num_classes)\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    # log_probs_flat: (batch * max_len, num_classes)\n",
    "    log_probs_flat = F.log_softmax(logits_flat)\n",
    "    # target_flat: (batch * max_len, 1)\n",
    "    target_flat = target.view(-1, 1)\n",
    "    # losses_flat: (batch * max_len, 1)\n",
    "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
    "    # losses: (batch, max_len)\n",
    "    losses = losses_flat.view(*target.size())\n",
    "    # mask: (batch, max_len)\n",
    "    mask = _sequence_mask(sequence_length=length, max_len=target.size(1))\n",
    "    losses = losses * mask.float()\n",
    "    loss = losses.sum() / length.float().sum()\n",
    "    return loss\n",
    "\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "\n",
    "    def forward(self, input_seqs, input_lens):\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(input_seqs, input_lens)\n",
    "        outputs, hidden = self.lstm(packed, None)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) \n",
    "        return outputs, hidden\n",
    "    \n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = F.relu(input)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output.squeeze())\n",
    "        return output.unsqueeze(0), hidden\n",
    "    \n",
    "encoder = EncoderLSTM(input_size=vocabularySize, hidden_size=300).cuda()\n",
    "decoder = DecoderLSTM(input_size=len(vocabulary), hidden_size=300, output_size=len(vocabulary)).cuda()\n",
    "\n",
    "def train(input_variables, \n",
    "          target_variables, \n",
    "          input_lens,\n",
    "          encoder, \n",
    "          decoder, \n",
    "          encoder_optimizer, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings, \n",
    "          teacher_force=True):\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Pass through the encoder\n",
    "    encoder_output, encoder_hidden = encoder(input_variables, input_lens)\n",
    "    \n",
    "    \n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]\n",
    "                                                for i in range(input_variables.size(1))]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    # Set the initial hidden state of the decoder to be the last hidden state of the encoder\n",
    "    last_hidden = torch.stack([encoder_output[length-1,i] for i,length in enumerate(input_lens)]).unsqueeze(0)\n",
    "\n",
    "    decoder_hidden = (last_hidden, last_hidden)\n",
    "    #decoder_hidden = encoder_hidden\n",
    "    #print(last_hidden)\n",
    "    # Prepare the results tensor\n",
    "    all_decoder_outputs = Variable(torch.zeros(*input_variables.size()))\n",
    "    if use_cuda:\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "        \n",
    "    all_decoder_outputs[0] = decoder_input\n",
    "        \n",
    "    # Iterate over the indices after the first.\n",
    "    for t in range(1,target_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        if random() <= 0.3:\n",
    "            decoder_input = input_variables[t].unsqueeze(0)\n",
    "        else:\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "                       \n",
    "            #Prepare the inputs\n",
    "            decoder_input = torch.stack([Variable(torch.FloatTensor(embeddings[ni])).cuda()\n",
    "                                         for ni in topi.squeeze()]).unsqueeze(0)\n",
    "        \n",
    "        # Save the decoder output\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        \n",
    "    loss = compute_loss(all_decoder_outputs.transpose(0,1).contiguous(),\n",
    "                        target_variable.transpose(0,1).contiguous(), \n",
    "                        Variable(torch.LongTensor(input_lens)).cuda())\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm(encoder.parameters(), 10.0)\n",
    "    torch.nn.utils.clip_grad_norm(decoder.parameters(), 10.0)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.data[0]\n",
    "\n",
    "def pad_seq(arr, length, pad_token):\n",
    "    \"\"\"\n",
    "    Pad an array to a length with a token.\n",
    "    \"\"\"\n",
    "    if len(arr) == length:\n",
    "        return np.array(arr)\n",
    "    \n",
    "    return np.concatenate((arr, [pad_token]*(length - len(arr))))\n",
    "\n",
    "encoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.01) \n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "for _ in range(num_epochs):\n",
    "    for i in range(len(train_sentences)//batch_size):\n",
    "        # Get the sentences in the batch\n",
    "        sentences = train_sentences[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        # Get the sentence lengths\n",
    "        sentence_lens = [len(preprocess_numberize(sentence)) for sentence in sentences]\n",
    "        \n",
    "        # Sort by the sentence lengths\n",
    "        sorted_indices = sorted(list(range(len(sentence_lens))), key=lambda i: sentence_lens[i], reverse=True)\n",
    "        sentences = [sentences[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "        \n",
    "        # Filter out 0 sentence lengths\n",
    "        sentence_lens = [sentence_lens[i] for i in sorted_indices if sentence_lens[i] > 0]\n",
    "        \n",
    "        # Determine length to pad everything to\n",
    "        max_len = max(sentence_lens)\n",
    "        \n",
    "        # Preprocess all of the sentences in each batch\n",
    "        one_hot_embedded_list = [preprocess_one_hot(sentence) for sentence in sentences]\n",
    "        one_hot_embedded_list_padded = [pad_seq(embed, max_len, np.zeros(len(vocabulary))) \n",
    "                                        for embed in one_hot_embedded_list]\n",
    "                \n",
    "        numberized_list = [preprocess_numberize(sentence) for sentence in sentences]\n",
    "        numberized_list_padded = [pad_seq(numb, max_len, 0).astype(torch.LongTensor) for numb in numberized_list]\n",
    "                \n",
    "        # Convert to variables\n",
    "        input_variable = Variable(torch.FloatTensor(one_hot_embedded_list_padded)).cuda()\n",
    "        target_variable = Variable(torch.LongTensor(numberized_list_padded)).cuda()\n",
    "        \n",
    "        # Transpose from batch_size x max_seq_len x vocab_size to max_seq_len x batch_size x vocab_size\n",
    "        input_variable = input_variable.transpose(0, 1)\n",
    "        target_variable = target_variable.transpose(0, 1)\n",
    "\n",
    "        loss = train(input_variable,\n",
    "                     target_variable, \n",
    "                     sentence_lens,\n",
    "                     encoder,\n",
    "                     decoder, \n",
    "                     encoder_optimizer,\n",
    "                     decoder_optimizer, \n",
    "                     criterion)\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(i,loss)\n",
    "        \n",
    "        if i % 1000 == 0:\n",
    "            print(train_sentences[0], \"||\", seq2seq_inference(train_sentences[0]))\n",
    "            print(train_sentences[1], \"||\", seq2seq_inference(train_sentences[1]))\n",
    "            print(train_sentences[2], \"||\", seq2seq_inference(train_sentences[2]))\n",
    "            print(train_sentences[0], \"||\", seq2seq_inference(train_sentences[0]))\n",
    "            print(train_sentences[4], \"||\", seq2seq_inference(train_sentences[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_inference(sentence, embeddings=one_hot_embeddings, max_length=20):\n",
    "    one_hot_embed = preprocess_one_hot(sentence)            \n",
    "    numberized = preprocess_numberize(sentence)\n",
    "                \n",
    "    # Convert to variables\n",
    "    input_variable = Variable(torch.FloatTensor([one_hot_embed])).cuda()\n",
    "    target_variable = Variable(torch.LongTensor([numberized])).cuda()\n",
    "\n",
    "    # Transpose from batch_size x max_seq_len x vocab_size to max_seq_len x batch_size x vocab_size\n",
    "    input_variable = input_variable.transpose(0, 1)\n",
    "    target_variable = target_variable.transpose(0, 1)\n",
    "    input_lengths = [len(numberized)]\n",
    "\n",
    "    # Pass through the encoder\n",
    "    encoder_output, encoder_hidden = encoder(input_variable, input_lengths)\n",
    "\n",
    "    # Construct the decoder input (initially <SOS> for every batch)\n",
    "    decoder_input = Variable(torch.FloatTensor([[embeddings[word2index[\"<SOS>\"]]]]))\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    # Set the initial hidden state of the decoder to be the last hidden state of the encoder\n",
    "    decoder_hidden = (encoder_hidden[0], encoder_hidden[0])\n",
    "    \n",
    "    # Iterate over the indices after the first.\n",
    "    decoder_outputs = []\n",
    "    for t in range(1,max_length):\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "    \n",
    "        # Get the top result\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi[0][0]\n",
    "        decoder_outputs.append(ni)\n",
    "\n",
    "        if vocabulary[ni] == \"<EOS>\":\n",
    "            break\n",
    "        \n",
    "        #Prepare the inputs\n",
    "        decoder_input = Variable(torch.FloatTensor([[embeddings[ni]]])).cuda()\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    return ' '.join(vocabulary[i] for i in decoder_outputs)\n",
    "\n",
    "print(seq2seq_inference(train_sentences[0]))\n",
    "print(seq2seq_inference(train_sentences[1]))\n",
    "print(seq2seq_inference(train_sentences[2]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
