{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython import display\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import json\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the dataset from [here](http://ec2-52-41-153-66.us-west-2.compute.amazonaws.com:8000/data.zip) and extract it into `data/`. The dataset contains approximately 80K training images and 100 validation images, with multiple captions/tags for each image. For this assignment, we are only concerned with the tags and ignore the captions.\n",
    "\n",
    "Ideally, unless you happen to have **much faster** internet than I do, you will want to download the data directly to your server: `wget http://ec2-52-41-153-66.us-west-2.compute.amazonaws.com:8000/data.zip`.\n",
    "\n",
    "For question two on the assignment, the dataset also contains a JSON file that maps from the ImageNet labels to the category names. \n",
    "\n",
    "Following the data downloading and unzipping, the code below loads in the data into memory accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "( 0 , 0 ,.,.) = \n",
       "  0.0039  0.0078  0.0039  ...   0.0471  0.0471  0.0314\n",
       "  0.0039  0.0039  0.0039  ...   0.0353  0.0353  0.0392\n",
       "  0.0039  0.0039  0.0039  ...   0.0392  0.0392  0.0510\n",
       "           ...             ⋱             ...          \n",
       "  0.7137  0.7294  0.7137  ...   0.1686  0.1843  0.1686\n",
       "  0.7059  0.6902  0.6863  ...   0.1765  0.1804  0.2039\n",
       "  0.6784  0.6667  0.6706  ...   0.1922  0.2157  0.2275\n",
       "\n",
       "( 0 , 1 ,.,.) = \n",
       "  0.1490  0.1490  0.1412  ...   0.0039  0.0039  0.0039\n",
       "  0.1451  0.1412  0.1373  ...   0.0039  0.0039  0.0039\n",
       "  0.1412  0.1373  0.1373  ...   0.0039  0.0039  0.0039\n",
       "           ...             ⋱             ...          \n",
       "  0.4392  0.4667  0.4549  ...   0.2588  0.2745  0.2863\n",
       "  0.4353  0.4235  0.4196  ...   0.2745  0.2980  0.3137\n",
       "  0.4118  0.4000  0.4000  ...   0.3020  0.3176  0.3020\n",
       "\n",
       "( 0 , 2 ,.,.) = \n",
       "  0.5294  0.5294  0.5294  ...   0.1451  0.1412  0.1333\n",
       "  0.5255  0.5333  0.5373  ...   0.1725  0.1451  0.1412\n",
       "  0.5373  0.5490  0.5451  ...   0.2314  0.1843  0.1608\n",
       "           ...             ⋱             ...          \n",
       "  0.0118  0.0078  0.0078  ...   0.5216  0.5294  0.5137\n",
       "  0.0078  0.0078  0.0118  ...   0.5098  0.5216  0.5216\n",
       "  0.0078  0.0118  0.0039  ...   0.5294  0.5255  0.4784\n",
       "[torch.cuda.FloatTensor of size 1x3x224x224 (GPU 0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a global transformer to appropriately scale images and subsequently convert them to a Tensor.\n",
    "img_size = 224\n",
    "loader = transforms.Compose([\n",
    "  transforms.Resize(img_size),\n",
    "  transforms.CenterCrop(img_size),\n",
    "  transforms.ToTensor(),\n",
    "]) \n",
    "def load_image(filename, volatile=False):\n",
    "    \"\"\"\n",
    "    Simple function to load and preprocess the image.\n",
    "\n",
    "    1. Open the image.\n",
    "    2. Scale/crop it and convert it to a float tensor.\n",
    "    3. Convert it to a variable (all inputs to PyTorch models must be variables).\n",
    "    4. Add another dimension to the start of the Tensor (b/c VGG expects a batch).\n",
    "    5. Move the variable onto the GPU.\n",
    "    \"\"\"\n",
    "    image = Image.open(filename).convert('RGB')\n",
    "    image_tensor = loader(image).float()\n",
    "    image_var = Variable(image_tensor, volatile=volatile).unsqueeze(0)\n",
    "    return image_var.cuda()\n",
    "\n",
    "load_image('data/train2014/COCO_train2014_000000000009.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load ImageNet label to category name mapping.\n",
    "imagenet_categories = [value for key,value in sorted(json.load(open('data/imagenet_categories.json')).items(), key=lambda t: int(t[0]))]\n",
    "\n",
    "# Load annotations file for the 100K training images.\n",
    "mscoco_train = json.load(open('data/annotations/train2014.json'))\n",
    "train_ids = [entry['id'] for entry in mscoco_train['images']]\n",
    "train_id_to_file = {entry['id']: 'data/train2014/' + entry['file_name'] for entry in mscoco_train['images']}\n",
    "category_to_name = {entry['id']: entry['name'] for entry in mscoco_train['categories']}\n",
    "category_idx_to_name = [entry['name'] for entry in mscoco_train['categories']]\n",
    "category_to_idx = {entry['id']: i for i,entry in enumerate(mscoco_train['categories'])}\n",
    "\n",
    "# Load annotations file for the 100 validation images.\n",
    "mscoco_val = json.load(open('data/annotations/val2014.json'))\n",
    "val_ids = [entry['id'] for entry in mscoco_val['images']]\n",
    "val_id_to_file = {entry['id']: 'data/val2014/' + entry['file_name'] for entry in mscoco_val['images']}\n",
    "\n",
    "# We extract out all of the category labels for the images in the training set. We use a set to ignore \n",
    "# duplicate labels.\n",
    "train_id_to_categories = defaultdict(set)\n",
    "for entry in mscoco_train['annotations']:\n",
    "    train_id_to_categories[entry['image_id']].add(entry['category_id'])\n",
    "\n",
    "# We extract out all of the category labels for the images in the validation set. We use a set to ignore \n",
    "# duplicate labels.\n",
    "val_id_to_categories = defaultdict(set)\n",
    "for entry in mscoco_val['annotations']:\n",
    "    val_id_to_categories[entry['image_id']].add(entry['category_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a look at an image and its corresponding category labels. We consider the image with the id 391895 and the corresponding filename, `data/val2014/COCO_val2014_000000391895.jpg`. The image is shown below.\n",
    "\n",
    "![image](data/val2014/COCO_val2014_000000391895.jpg)\n",
    "\n",
    "The following code determines the category labels for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,category in enumerate(val_id_to_categories[391895]):\n",
    "    print(\"%d. %s\" % (i, category_to_name[category]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading a Pre-trained Convolutional Neural Network (CNN)\n",
    "\n",
    "We will work with the VGG-16 image classification CNN network first introduced in [Very Deep Convolutional Neural Networks for Large-Scale Image Recognition](https://arxiv.org/pdf/1409.1556.pdf) by K. Simonyan and A. Zisserman.\n",
    "\n",
    "Fairly straightforwardly, we load the pre-trained VGG model and indicate to PyTorch that we are using the model for inference rather than training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG (\n",
       "  (features): Sequential (\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU (inplace)\n",
       "    (4): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU (inplace)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU (inplace)\n",
       "    (9): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU (inplace)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU (inplace)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU (inplace)\n",
       "    (16): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU (inplace)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU (inplace)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU (inplace)\n",
       "    (23): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU (inplace)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU (inplace)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU (inplace)\n",
       "    (30): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
       "  )\n",
       "  (classifier): Sequential (\n",
       "    (0): Linear (25088 -> 4096)\n",
       "    (1): ReLU (inplace)\n",
       "    (2): Dropout (p = 0.5)\n",
       "    (3): Linear (4096 -> 4096)\n",
       "    (4): ReLU (inplace)\n",
       "    (5): Dropout (p = 0.5)\n",
       "    (6): Linear (4096 -> 1000)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_model = models.vgg16(pretrained=True).cuda()\n",
    "vgg_model.eval()\n",
    "\n",
    "# Let's see what the model looks like.\n",
    "vgg_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making Predictions Using VGG-16\n",
    "\n",
    "Given the pre-trained network, we must now write the code to make predictions on the 100 validation images via a forward pass through the network. Typically the final layer of VGG-16 is a softmax layer, however the pre-trained PyTorch model that we are using does not have softmax built into the final layer (instead opting to incorporate it into the loss function) and therefore we must **manually** apply softmax to the output of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax()\n",
    "for image_id in val_ids[:10]:\n",
    "    # Display the image.\n",
    "    display.display(display.Image(val_id_to_file[image_id]))\n",
    "\n",
    "    # Print all of the category labels for this image.\n",
    "    print(\"Ground Truth Labels:\")\n",
    "    for i,category in enumerate(val_id_to_categories[image_id]):\n",
    "        print(\"%d. %s\" % (i, category_to_name[category]))\n",
    "  \n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(val_id_to_file[image_id])\n",
    "\n",
    "    # Run the image through the model and softmax.\n",
    "    label_likelihoods = softmax(vgg_model(img)).squeeze()\n",
    "\n",
    "    # Get the top 5 labels, and their corresponding likelihoods.\n",
    "    probs, indices = label_likelihoods.topk(5)\n",
    "\n",
    "    # Iterate and print out the predictions.\n",
    "    print(\"Predictions:\")\n",
    "    for i in range(5):\n",
    "        print(\"%d. %s (%.3f)\" % (i, imagenet_categories[indices.data[i]], probs.data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Computing Generic Visual Features using CNN\n",
    "\n",
    "Since, rather than the output of VGG, we want a fixed sized vector representation of each image, we remove the last linear layer. The implementation of the forward function for VGG is shown below:\n",
    "\n",
    "```\n",
    "x = self.features(x)\n",
    "x = x.view(x.size(0), -1)\n",
    "x = self.classifier(x)\n",
    "```\n",
    "We aim to preserve everything but the final component of the classifier, meaning we must define an alternative equivalent to `self.classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential (\n",
       "  (0): Linear (25088 -> 4096)\n",
       "  (1): ReLU (inplace)\n",
       "  (2): Dropout (p = 0.5)\n",
       "  (3): Linear (4096 -> 4096)\n",
       "  (4): ReLU (inplace)\n",
       "  (5): Dropout (p = 0.5)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the final layer of the classifier, and indicate to PyTorch that the model is being used for inference\n",
    "# rather than training (most importantly, this disables dropout).\n",
    "modified_classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])\n",
    "modified_classifier.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we vectorize all of the training images and write the results to a file.\n",
    "# TODO: Do more than just the first 500 (don't want to run everything right now)\n",
    "training_vectors = []\n",
    "for i,image_id in enumerate(train_ids[:500]):\n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(train_id_to_file[image_id])\n",
    "\n",
    "    # Run through the convolutional layers and resize the output.\n",
    "    features_output = vgg_model.features(img)\n",
    "    classifier_input = features_output.view(1, -1)\n",
    "\n",
    "    # Run through all but final classifier layers.\n",
    "    output = modified_classifier(classifier_input)\n",
    "    training_vectors.append(np.array(list(output.data.squeeze())))\n",
    "\n",
    "# For simplicity, we convert this to a numpy array and save the result to a file.\n",
    "training_vectors = np.stack(training_vectors, axis=0)\n",
    "np.save(open('outputs/training_vectors', 'wb+'), training_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next we vectorize all of the validation images and write the results to a file.\n",
    "validation_vectors = []\n",
    "for image_id in val_ids:\n",
    "    # Load/preprocess the image.\n",
    "    img = load_image(val_id_to_file[image_id])\n",
    "\n",
    "    # Run through the convolutional layers and resize the output.\n",
    "    features_output = vgg_model.features(img)\n",
    "    classifier_input = features_output.view(1, -1)\n",
    "\n",
    "    # Run through all but final classifier layers.\n",
    "    output = modified_classifier(classifier_input)\n",
    "    validation_vectors.append(list(output.data.squeeze()))\n",
    "\n",
    "# For simplicity, we convert this to a numpy array and save the result to a file.\n",
    "validation_vectors = np.array(validation_vectors)\n",
    "np.save(open('outputs/validation_vectors', 'wb+'), validation_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visual Similarity\n",
    "\n",
    "We now use the generated vectors, to find the closest training image for every validation image. This can easily be done by finding the training vector that minimizes the Euclidean distance for every validation image. We repeat this exercise for the first 10 images in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,(image_id,vector) in enumerate(zip(val_ids, validation_vectors)):\n",
    "    print(\"Processing image %d\" % i)  \n",
    "    \n",
    "    # Identify the index of the closest training vector.\n",
    "    closest_idx = min(range(len(training_vectors)), key=lambda i: np.linalg.norm(training_vectors[i] - vector))\n",
    "\n",
    "    # Show the two images, first the original and then the closest training.\n",
    "    display.display(display.Image(val_id_to_file[image_id]))\n",
    "    display.display(display.Image(train_id_to_file[train_ids[closest_idx]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training a Multi-Class Classification Network\n",
    "\n",
    "We now build a two layer classification network, which takes 4096-dimensional vectors as input and outputs the probabilities of the 80 categories present in MSCOCO. \n",
    "\n",
    "For this purpose, we utilize two layers (both containing sigmoid activation functions) with the hidden dimension set to 512. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we construct a class for the model\n",
    "\n",
    "class MultiClassClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        When constructing the model, we initialize two linear modules and assign them\n",
    "        as class fields.\n",
    "        \"\"\"\n",
    "        super(MultiClassClassifier, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the network, applying the sigmoid activation function after each layer.\n",
    "        \"\"\"\n",
    "        return self.layer2(torch.sigmoid(self.layer1(x)))\n",
    "\n",
    "model = MultiClassClassifier(4096, 512, 80).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomize training vectors\n",
    "training_vectors = np.random.random((len(train_ids), 4096))\n",
    "\n",
    "# Now we prepare the input data, by converting the generated vectors into PyTorch variables.\n",
    "training_input = [Variable(torch.FloatTensor(train_vector)).cuda() for train_vector,train_id in zip(training_vectors,train_ids) if len(list(train_id_to_categories[train_id])) > 0]\n",
    "\n",
    "# Construct the validation input\n",
    "validation_input = [Variable(torch.FloatTensor(val_vector), volatile=True).cuda() for val_vector in validation_vectors]\n",
    "\n",
    "# The output data is prepared by representing each output as a binary vector of categories\n",
    "training_output = []\n",
    "for i in range(len(train_ids)):\n",
    "    categories = list(train_id_to_categories[train_ids[i]])\n",
    "    if len(categories) == 0:\n",
    "        continue\n",
    "  \n",
    "    training_vector = np.zeros(len(category_to_idx))\n",
    "    indices = [category_to_idx[category] for category in categories]\n",
    "    training_vector[indices] = 1\n",
    "    training_output.append(training_vector)\n",
    "\n",
    "training_output = Variable(torch.FloatTensor(training_output)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The output data is prepared by representing each output as a binary vector of categories\n",
    "validation_output = []\n",
    "for i in range(len(val_ids)):\n",
    "    categories = list(val_id_to_categories[val_ids[i]])  \n",
    "    training_vector = np.zeros(len(category_to_idx))\n",
    "    indices = [category_to_idx[category] for category in categories]\n",
    "    training_vector[indices] = 1\n",
    "    validation_output.append(training_vector)\n",
    "\n",
    "validation_output = Variable(torch.FloatTensor(validation_output), volatile=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    \"\"\"\n",
    "    Given a model, return the validation loss.\n",
    "    \"\"\"\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "    # Create the input/output for the model\n",
    "    x = torch.stack(validation_input)\n",
    "    y = validation_output\n",
    "\n",
    "    # Run it through the prediction\n",
    "    y_pred = model(x)\n",
    "        \n",
    "    # Compute and return loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    return loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model, learning_rate=0.001, batch_size=100, epochs=10):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # Define the criterion and optimizer.\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Keep track of the losses, for the purposes of plotting.\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Determine number of minibatches\n",
    "    num_iter = epochs * len(training_input)//batch_size \n",
    "    print(num_iter)\n",
    "    for i in range(num_iter):\n",
    "        start_idx = i * batch_size % len(training_input)\n",
    "\n",
    "        # Retrieve the next batch of training data.\n",
    "        x = torch.stack(training_input[start_idx:start_idx+batch_size])\n",
    "        y = training_output[start_idx:start_idx+batch_size]\n",
    "        \n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        if i % 1000 == 0:\n",
    "            train_losses.append(loss.data[0])\n",
    "            val_losses.append(validate(model))\n",
    "            print(i, train_losses[-1], val_losses[-1])\n",
    "\n",
    "        # Zero gradients, perform backwards pass and update model weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "    return train_losses, val_losses    \n",
    "\n",
    "# Finally train the model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we repeat step two using the two layer classifier.\n",
    "softmax = nn.Softmax()\n",
    "for i,image_id in enumerate(val_ids):\n",
    "    # Display the image.\n",
    "    display.display(display.Image(val_id_to_file[image_id]))\n",
    "\n",
    "    # Print all of the category labels for this image.\n",
    "    print(\"Ground Truth Labels:\")\n",
    "    for i,category in enumerate(val_id_to_categories[image_id]):\n",
    "        print(\"%d. %s\" % (i, category_to_name[category]))\n",
    "  \n",
    "    # Run the image through the model and softmax.\n",
    "    label_likelihoods = softmax(model(validation_input[i].unsqueeze(0))).squeeze()\n",
    "\n",
    "    # Get the top 5 labels, and their corresponding likelihoods.\n",
    "    probs, indices = label_likelihoods.topk(5)\n",
    "\n",
    "    # Iterate and print out the predictions.\n",
    "    print(\"Predictions:\")\n",
    "    for i in range(5):\n",
    "        print(\"%d. %s (%.3f)\" % (i, category_idx_to_name[indices.data[i]], probs.data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. End-to-End Model Fine-tuning\n",
    "\n",
    "Instead of training *only* the final two layers, we now create an end-to-end model and train the entire thing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First we construct a class for the model\n",
    "class EndToEndModel(torch.nn.Module):\n",
    "    def __init__(self, vgg_model, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        When constructing the model, we initialize two linear modules and assign them\n",
    "        as class fields. We also, as done earlier, remove the final layer of the vgg model.\n",
    "        \"\"\"\n",
    "        super(EndToEndModel, self).__init__()\n",
    "        self.features = vgg_model.features\n",
    "        self.classifier = nn.Sequential(*list(vgg_model.classifier.children())[:-1])\n",
    "        self.layer1 = torch.nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = torch.nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pass the input through the network, applying the sigmoid activation function after each layer.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return self.layer2(torch.sigmoid(self.layer1(x)))\n",
    "\n",
    "model = EndToEndModel(vgg_model, 4096, 512, 80).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration:  0\n",
      "0 0.20185235142707825 0.1819806843996048\n",
      "Starting iteration:  1\n",
      "Starting iteration:  2\n",
      "Starting iteration:  3\n",
      "Starting iteration:  4\n",
      "Starting iteration:  5\n",
      "Starting iteration:  6\n",
      "Starting iteration:  7\n",
      "Starting iteration:  8\n",
      "Starting iteration:  9\n",
      "Starting iteration:  10\n",
      "10 0.16861264407634735 0.17164179682731628\n",
      "Starting iteration:  11\n",
      "Starting iteration:  12\n",
      "Starting iteration:  13\n",
      "Starting iteration:  14\n",
      "Starting iteration:  15\n",
      "Starting iteration:  16\n",
      "Starting iteration:  17\n",
      "Starting iteration:  18\n",
      "Starting iteration:  19\n",
      "Starting iteration:  20\n",
      "20 0.2017010748386383 0.16787518560886383\n",
      "Starting iteration:  21\n",
      "Starting iteration:  22\n",
      "Starting iteration:  23\n",
      "Starting iteration:  24\n",
      "Starting iteration:  25\n",
      "Starting iteration:  26\n",
      "Starting iteration:  27\n",
      "Starting iteration:  28\n",
      "Starting iteration:  29\n",
      "Starting iteration:  30\n",
      "30 0.12975534796714783 0.16501444578170776\n",
      "Starting iteration:  31\n",
      "Starting iteration:  32\n",
      "Starting iteration:  33\n",
      "Starting iteration:  34\n",
      "Starting iteration:  35\n",
      "Starting iteration:  36\n",
      "Starting iteration:  37\n",
      "Starting iteration:  38\n",
      "Starting iteration:  39\n",
      "Starting iteration:  40\n",
      "40 0.17910413444042206 0.16428989171981812\n",
      "Starting iteration:  41\n",
      "Starting iteration:  42\n",
      "Starting iteration:  43\n",
      "Starting iteration:  44\n",
      "Starting iteration:  45\n",
      "Starting iteration:  46\n",
      "Starting iteration:  47\n",
      "Starting iteration:  48\n",
      "Starting iteration:  49\n",
      "Starting iteration:  50\n",
      "50 0.1126038059592247 0.16256676614284515\n",
      "Starting iteration:  51\n",
      "Starting iteration:  52\n",
      "Starting iteration:  53\n",
      "Starting iteration:  54\n",
      "Starting iteration:  55\n",
      "Starting iteration:  56\n",
      "Starting iteration:  57\n",
      "Starting iteration:  58\n",
      "Starting iteration:  59\n",
      "Starting iteration:  60\n",
      "60 0.14240427315235138 0.16249407827854156\n",
      "Starting iteration:  61\n",
      "Starting iteration:  62\n",
      "Starting iteration:  63\n",
      "Starting iteration:  64\n",
      "Starting iteration:  65\n",
      "Starting iteration:  66\n",
      "Starting iteration:  67\n",
      "Starting iteration:  68\n",
      "Starting iteration:  69\n",
      "Starting iteration:  70\n",
      "70 0.10921403020620346 0.16142785549163818\n",
      "Starting iteration:  71\n",
      "Starting iteration:  72\n",
      "Starting iteration:  73\n",
      "Starting iteration:  74\n",
      "Starting iteration:  75\n",
      "Starting iteration:  76\n",
      "Starting iteration:  77\n",
      "Starting iteration:  78\n",
      "Starting iteration:  79\n",
      "Starting iteration:  80\n",
      "80 0.13280516862869263 0.16159935295581818\n",
      "Starting iteration:  81\n",
      "Starting iteration:  82\n",
      "Starting iteration:  83\n",
      "Starting iteration:  84\n",
      "Starting iteration:  85\n",
      "Starting iteration:  86\n",
      "Starting iteration:  87\n",
      "Starting iteration:  88\n",
      "Starting iteration:  89\n",
      "Starting iteration:  90\n",
      "90 0.13378436863422394 0.16082455217838287\n",
      "Starting iteration:  91\n",
      "Starting iteration:  92\n",
      "Starting iteration:  93\n",
      "Starting iteration:  94\n",
      "Starting iteration:  95\n",
      "Starting iteration:  96\n",
      "Starting iteration:  97\n",
      "Starting iteration:  98\n",
      "Starting iteration:  99\n",
      "Starting iteration:  100\n",
      "100 0.15236248075962067 0.16127432882785797\n",
      "Starting iteration:  101\n",
      "Starting iteration:  102\n",
      "Starting iteration:  103\n",
      "Starting iteration:  104\n",
      "Starting iteration:  105\n",
      "Starting iteration:  106\n",
      "Starting iteration:  107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a7d1839e1f58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;31m# Finally train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-a7d1839e1f58>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, learning_rate, batch_size, epochs)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# Compute and print loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# Zero gradients, perform backwards pass and update model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultilabel_soft_margin_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmultilabel_soft_margin_loss\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmultilabel_soft_margin_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/_functions/thnn/auto.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, target, *args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         getattr(ctx._backend, update_output.name)(ctx._backend.library_state, input, target,\n\u001b[0;32m---> 47\u001b[0;31m                                                   output, *ctx.additional_args)\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def validate(model):\n",
    "    \"\"\"\n",
    "    Given a model, return the validation loss.\n",
    "    \"\"\"\n",
    "    # Now we prepare the input data, by converting the generated vectors into PyTorch variables.\n",
    "    validation_input = [load_image(val_id_to_file[val_id], volatile=True).squeeze() for val_id in val_ids]\n",
    "\n",
    "\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "\n",
    "    # Create the input/output for the model\n",
    "    x = torch.stack(validation_input)\n",
    "    y = validation_output\n",
    "\n",
    "    # Run it through the prediction\n",
    "    y_pred = model(x)\n",
    "        \n",
    "    # Compute and return loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    return loss.data[0]\n",
    "\n",
    "def create_training(start, end):\n",
    "    training_input = [load_image(train_id_to_file[train_id]).squeeze() for train_id in train_ids[start:end]\n",
    "                      if len(list(train_id_to_categories[train_id])) > 0]\n",
    "\n",
    "    # The output data is prepared by representing each output as a binary vector of categories\n",
    "    training_output = []\n",
    "    for i in range(start,min(len(train_ids),end)):\n",
    "        categories = list(train_id_to_categories[train_ids[i]])\n",
    "        if len(categories) == 0:\n",
    "            continue\n",
    "\n",
    "        training_vector = np.zeros(len(category_to_idx))\n",
    "        indices = [category_to_idx[category] for category in categories]\n",
    "        training_vector[indices] = 1\n",
    "        training_output.append(training_vector)\n",
    "    training_output = Variable(torch.FloatTensor(training_output)).cuda()\n",
    "    \n",
    "    return training_input, training_output\n",
    "\n",
    "def train(model, learning_rate=0.0001, batch_size=50, epochs=1):\n",
    "    \"\"\"\n",
    "    Training function which takes as input a model, a learning rate and a batch size.\n",
    "  \n",
    "    After completing a full pass over the data, the function exists, and the input model will be trained.\n",
    "    \"\"\"\n",
    "    # Define the criterion and optimizer.\n",
    "    criterion = nn.MultiLabelSoftMarginLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Keep track of the losses, for the purposes of plotting.\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # Determine number of minibatches\n",
    "    num_iter = epochs * len(train_id_to_file)//batch_size \n",
    "    for i in range(num_iter):\n",
    "        print(\"Starting iteration: \", i)\n",
    "        \n",
    "        start_idx = i * batch_size % len(train_id_to_file)\n",
    "        \n",
    "        training_input, training_output = create_training(start_idx, start_idx + batch_size)\n",
    "\n",
    "        # Retrieve the next batch of training data.\n",
    "        x = torch.stack(training_input)\n",
    "        y = training_output\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute and print loss\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # Zero gradients, perform backwards pass and update model weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "        \n",
    "        del x,y,y_pred\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            train_losses.append(loss.data[0])\n",
    "            val_losses.append(validate(model))\n",
    "            print(i, train_losses[-1], val_losses[-1])\n",
    "        \n",
    "    return train_losses, val_losses    \n",
    "\n",
    "# Finally train the model\n",
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we repeat step two using the end-to-end classifier.\n",
    "softmax = nn.Softmax()\n",
    "for i,image_id in enumerate(val_ids):\n",
    "    # Display the image.\n",
    "    display.display(display.Image(val_id_to_file[image_id]))\n",
    "\n",
    "    # Print all of the category labels for this image.\n",
    "    print(\"Ground Truth Labels:\")\n",
    "    for i,category in enumerate(val_id_to_categories[image_id]):\n",
    "        print(\"%d. %s\" % (i, category_to_name[category]))\n",
    "  \n",
    "    # Run the image through the model and softmax.\n",
    "    label_likelihoods = softmax(model(validation_input[i].unsqueeze(0))).squeeze()\n",
    "\n",
    "    # Get the top 5 labels, and their corresponding likelihoods.\n",
    "    probs, indices = label_likelihoods.topk(5)\n",
    "\n",
    "    # Iterate and print out the predictions.\n",
    "    print(\"Predictions:\")\n",
    "    for i in range(5):\n",
    "        print(\"%d. %s (%.3f)\" % (i, category_idx_to_name[indices.data[i]], probs.data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Hyper-parameter Tuning\n",
    "\n",
    "Now we do a grid search over the learning rate and batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_params = None\n",
    "best_loss = float('inf')\n",
    "for learning_rate in [0.0001, 0.001, 0.01]:\n",
    "    for batch_size in [50, 50, 5]:\n",
    "        model = EndToEndModel(vgg_model, 4096, 512, 80).cuda()\n",
    "        \n",
    "        train_losses, val_losses = train(model, learning_rate=learning_rate, batch_size=batch_size)\n",
    "\n",
    "        plt.plot(losses)\n",
    "        plt.title('Training Losses (learning rate = %.3f, batch_size = %d' % (learning_rate, batch_size))\n",
    "        plt.show()\n",
    "    \n",
    "        if train_losses[-1] < best_loss:\n",
    "            best_loss = train_losses[-1]\n",
    "            best_params = (learning_rate, batch_size) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
