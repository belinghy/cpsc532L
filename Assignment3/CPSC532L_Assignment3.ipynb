{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the data and extract it into `data/`. The dataset contains two files, both containing a single caption on each line. We should have 415,795 sentences in the training captions and 500 sentences in the validation captions.\n",
    "\n",
    "To download the data, run the following directly on your server: `wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a3_data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414143\n",
      "500\n",
      "A very clean and well decorated empty bathroom.\n"
     ]
    }
   ],
   "source": [
    "# Load the data into memory.\n",
    "train_sentences = [line.strip() for line in open(\"data/mscoco_train_captions.txt\").readlines() if line.strip() != '']\n",
    "val_sentences = [line.strip() for line in open(\"data/mscoco_val_captions.txt\").readlines()]\n",
    "\n",
    "for index, sentence in enumerate(train_sentences):\n",
    "    if sentence[-1] != '.':\n",
    "        train_sentences[index] = sentence + '.'\n",
    "\n",
    "for index, sentence in enumerate(val_sentences):\n",
    "    if sentence[-1] != '.':\n",
    "        val_sentences[index] = sentence + '.'\n",
    "        \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n",
    "\n",
    "All of the packages utilized should be installed on your Azure servers, however you will have to download an NLTK corpus. To do this, follow the instructions below:\n",
    "\n",
    "1. SSH to your Azure server\n",
    "2. Open up Python interpreter\n",
    "3. `import nltk`\n",
    "4. `nltk.download()`\n",
    "\n",
    "    You should now see something that looks like:\n",
    "\n",
    "    ```\n",
    "    >>> nltk.download()\n",
    "    NLTK Downloader\n",
    "    ---------------------------------------------------------------------------\n",
    "        d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
    "    ---------------------------------------------------------------------------\n",
    "    Downloader> \n",
    "\n",
    "    ```\n",
    "\n",
    "5. `d punkt`\n",
    "6. Provided the download finished successfully, you may now exit out of the Python interpreter and close the SSH connection.\n",
    "\n",
    "Please look through the functions provided below **carefully**, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "(1000, 300)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.64252836 -0.04464054 -0.02437208 ...  0.45854273 -0.40346768\n",
      "  -0.58654529]\n",
      " [ 0.29306009  0.19502763  0.38849041 ...  0.50763428 -0.24476127\n",
      "  -0.47664955]\n",
      " ...\n",
      " [-0.57009608  0.53415418 -0.37802663 ... -0.00389384 -0.61393547\n",
      "   0.01855109]\n",
      " [ 0.53694671 -0.15880792  0.51926643 ... -0.20401719 -0.10407556\n",
      "  -0.26090741]\n",
      " [ 0.403263   -1.45369279  0.12051474 ... -0.27986085  0.36217818\n",
      "  -0.32311931]]\n",
      "['<UNK>', 'a', '.', '<SOS>', '<EOS>']\n",
      "A blue and white bathroom with butterfly themed wall tiles.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(11, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(maxSequenceLength)\n",
    "print(w2v_embeddings.shape)\n",
    "print(w2v_embeddings)\n",
    "\n",
    "print(vocabulary[0:5])\n",
    "print(train_sentences[2])\n",
    "print(preprocess_one_hot(train_sentences[0]))\n",
    "print(preprocess_one_hot(train_sentences[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0  ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.6582 -5.8084 -5.8018  ...   1.0613  5.2599  2.9784\n",
      "[torch.cuda.DoubleTensor of size 1x1x1000 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.2716  1.6143 -0.2492 -0.4618 -0.7349\n",
      "-0.2343 -2.1428  0.6304 -1.6372  1.3402\n",
      "-0.6010  1.5082 -1.9139  0.3596 -0.7784\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "Variable containing:\n",
      " 2\n",
      " 1\n",
      " 3\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = preprocess_one_hot(train_sentences[0])\n",
    "input_sentence = torch.from_numpy(input_sentence[0])\n",
    "input_sentence = Variable(input_sentence.float())\n",
    "input_sentence = input_sentence.cuda()\n",
    "input_sentence = input_sentence.view(1, 1, 1000)\n",
    "\n",
    "lstm = nn.LSTM(1000, 300).cuda()\n",
    "output, hidden = lstm(input_sentence)\n",
    "\n",
    "linear = nn.Linear(300, 1000).double().cuda()\n",
    "output = linear(output.double().cuda())\n",
    "print(output)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = Variable(torch.randn(3, 5), requires_grad=True)\n",
    "target = Variable(torch.LongTensor(3).random_(5))\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "torch.cuda.current_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double().cuda()\n",
    "        self.linear = nn.Linear(hidden_size, output_size).double().cuda()\n",
    "        self.softmax = nn.LogSoftmax(dim=2).double().cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.double().cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a Language Decoder\n",
    "\n",
    "We must now train the language decoder we implemented above. An important thing to pay attention to is the [inputs for an LSTM](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 37s (- 261m 20s) (1000 0%) 4.5910\n",
      "1m 16s (- 262m 17s) (2000 0%) 4.1580\n",
      "1m 55s (- 264m 13s) (3000 0%) 4.0781\n",
      "2m 35s (- 264m 54s) (4000 0%) 4.0283\n",
      "3m 13s (- 263m 40s) (5000 1%) 3.9425\n",
      "3m 51s (- 262m 50s) (6000 1%) 3.9330\n",
      "4m 31s (- 263m 1s) (7000 1%) 3.9186\n",
      "5m 10s (- 262m 41s) (8000 1%) 3.8757\n",
      "5m 49s (- 262m 6s) (9000 2%) 3.8457\n",
      "6m 28s (- 261m 39s) (10000 2%) 3.8771\n",
      "7m 7s (- 261m 18s) (11000 2%) 3.8434\n",
      "7m 47s (- 261m 13s) (12000 2%) 3.8914\n",
      "8m 27s (- 260m 45s) (13000 3%) 3.8548\n",
      "9m 5s (- 260m 4s) (14000 3%) 3.8475\n",
      "9m 44s (- 259m 10s) (15000 3%) 3.8205\n",
      "10m 23s (- 258m 36s) (16000 3%) 3.8072\n",
      "11m 2s (- 257m 53s) (17000 4%) 3.8749\n",
      "11m 40s (- 257m 5s) (18000 4%) 3.8085\n",
      "12m 20s (- 256m 43s) (19000 4%) 3.8706\n",
      "13m 0s (- 256m 15s) (20000 4%) 3.8384\n",
      "13m 39s (- 255m 38s) (21000 5%) 3.8192\n",
      "14m 18s (- 254m 58s) (22000 5%) 3.7802\n",
      "14m 56s (- 254m 5s) (23000 5%) 3.7696\n",
      "15m 35s (- 253m 30s) (24000 5%) 3.8240\n",
      "16m 14s (- 252m 47s) (25000 6%) 3.8611\n",
      "16m 52s (- 251m 57s) (26000 6%) 3.8123\n",
      "17m 30s (- 251m 9s) (27000 6%) 3.7500\n",
      "18m 9s (- 250m 27s) (28000 6%) 3.7581\n",
      "18m 48s (- 249m 41s) (29000 7%) 3.7882\n",
      "19m 26s (- 248m 54s) (30000 7%) 3.8025\n",
      "20m 4s (- 248m 9s) (31000 7%) 3.7828\n",
      "20m 43s (- 247m 26s) (32000 7%) 3.7181\n",
      "21m 21s (- 246m 45s) (33000 7%) 3.8824\n",
      "21m 59s (- 245m 54s) (34000 8%) 4.0305\n",
      "22m 38s (- 245m 17s) (35000 8%) 3.9964\n",
      "23m 18s (- 244m 48s) (36000 8%) 3.9432\n",
      "23m 56s (- 244m 3s) (37000 8%) 3.8414\n",
      "24m 34s (- 243m 16s) (38000 9%) 3.8394\n",
      "25m 13s (- 242m 33s) (39000 9%) 3.8691\n",
      "25m 52s (- 241m 57s) (40000 9%) 3.8435\n",
      "26m 30s (- 241m 14s) (41000 9%) 3.8256\n",
      "27m 8s (- 240m 32s) (42000 10%) 3.8432\n",
      "27m 48s (- 239m 57s) (43000 10%) 3.8688\n",
      "28m 27s (- 239m 21s) (44000 10%) 3.8955\n",
      "29m 6s (- 238m 43s) (45000 10%) 3.8825\n",
      "29m 45s (- 238m 8s) (46000 11%) 3.9264\n",
      "30m 23s (- 237m 28s) (47000 11%) 3.8017\n",
      "31m 2s (- 236m 47s) (48000 11%) 3.8625\n",
      "31m 40s (- 236m 5s) (49000 11%) 3.8894\n",
      "32m 19s (- 235m 23s) (50000 12%) 3.8703\n",
      "32m 57s (- 234m 38s) (51000 12%) 3.8337\n",
      "33m 35s (- 233m 57s) (52000 12%) 3.7874\n",
      "34m 14s (- 233m 18s) (53000 12%) 3.8007\n",
      "34m 52s (- 232m 37s) (54000 13%) 3.8484\n",
      "35m 31s (- 231m 57s) (55000 13%) 3.8291\n",
      "36m 9s (- 231m 15s) (56000 13%) 3.7878\n",
      "36m 48s (- 230m 34s) (57000 13%) 3.8007\n",
      "37m 26s (- 229m 54s) (58000 14%) 3.7978\n",
      "38m 4s (- 229m 9s) (59000 14%) 3.7807\n",
      "38m 41s (- 228m 24s) (60000 14%) 3.8211\n",
      "39m 19s (- 227m 42s) (61000 14%) 3.7759\n",
      "39m 57s (- 226m 59s) (62000 14%) 3.7941\n",
      "40m 36s (- 226m 18s) (63000 15%) 3.7777\n",
      "41m 13s (- 225m 34s) (64000 15%) 3.7592\n",
      "41m 52s (- 224m 54s) (65000 15%) 3.7872\n",
      "42m 30s (- 224m 13s) (66000 15%) 3.9073\n",
      "43m 8s (- 223m 31s) (67000 16%) 3.9476\n",
      "43m 47s (- 222m 53s) (68000 16%) 3.8877\n",
      "44m 26s (- 222m 17s) (69000 16%) 3.8564\n",
      "45m 4s (- 221m 37s) (70000 16%) 3.8137\n",
      "45m 43s (- 220m 58s) (71000 17%) 3.7716\n",
      "46m 21s (- 220m 16s) (72000 17%) 3.7791\n",
      "47m 0s (- 219m 38s) (73000 17%) 3.7831\n",
      "47m 38s (- 218m 58s) (74000 17%) 3.7400\n",
      "48m 17s (- 218m 20s) (75000 18%) 3.7658\n",
      "48m 56s (- 217m 43s) (76000 18%) 3.7905\n",
      "49m 34s (- 217m 5s) (77000 18%) 3.7491\n",
      "50m 12s (- 216m 24s) (78000 18%) 3.7512\n",
      "50m 50s (- 215m 42s) (79000 19%) 3.7339\n",
      "51m 29s (- 215m 2s) (80000 19%) 3.7664\n",
      "52m 7s (- 214m 23s) (81000 19%) 3.7044\n",
      "52m 46s (- 213m 44s) (82000 19%) 3.7559\n",
      "53m 24s (- 213m 4s) (83000 20%) 3.7592\n",
      "54m 2s (- 212m 23s) (84000 20%) 3.7142\n",
      "54m 41s (- 211m 46s) (85000 20%) 3.7252\n",
      "55m 19s (- 211m 7s) (86000 20%) 3.7928\n",
      "55m 58s (- 210m 28s) (87000 21%) 3.7672\n",
      "56m 36s (- 209m 48s) (88000 21%) 3.7804\n",
      "57m 15s (- 209m 9s) (89000 21%) 3.7311\n",
      "57m 53s (- 208m 29s) (90000 21%) 3.7545\n",
      "58m 31s (- 207m 49s) (91000 21%) 3.7411\n",
      "59m 9s (- 207m 10s) (92000 22%) 3.7290\n",
      "59m 47s (- 206m 29s) (93000 22%) 3.7461\n",
      "60m 25s (- 205m 48s) (94000 22%) 3.6798\n",
      "61m 3s (- 205m 8s) (95000 22%) 3.7054\n",
      "61m 41s (- 204m 26s) (96000 23%) 3.7273\n",
      "62m 19s (- 203m 45s) (97000 23%) 3.7012\n",
      "62m 57s (- 203m 4s) (98000 23%) 3.7045\n",
      "63m 34s (- 202m 23s) (99000 23%) 3.8385\n",
      "64m 12s (- 201m 41s) (100000 24%) 3.8788\n",
      "64m 50s (- 201m 1s) (101000 24%) 3.8200\n",
      "65m 29s (- 200m 24s) (102000 24%) 3.8439\n",
      "66m 7s (- 199m 44s) (103000 24%) 3.7698\n",
      "66m 45s (- 199m 5s) (104000 25%) 3.7335\n",
      "67m 24s (- 198m 26s) (105000 25%) 3.8018\n",
      "68m 2s (- 197m 47s) (106000 25%) 3.7530\n",
      "68m 41s (- 197m 9s) (107000 25%) 3.7496\n",
      "69m 19s (- 196m 30s) (108000 26%) 3.7653\n",
      "69m 57s (- 195m 50s) (109000 26%) 3.7737\n",
      "70m 35s (- 195m 11s) (110000 26%) 3.7772\n",
      "71m 14s (- 194m 33s) (111000 26%) 3.7913\n",
      "71m 52s (- 193m 52s) (112000 27%) 3.7489\n",
      "72m 30s (- 193m 13s) (113000 27%) 3.8030\n",
      "73m 8s (- 192m 33s) (114000 27%) 3.7364\n",
      "73m 46s (- 191m 53s) (115000 27%) 3.7266\n",
      "74m 24s (- 191m 13s) (116000 28%) 3.7221\n",
      "75m 2s (- 190m 34s) (117000 28%) 3.7018\n",
      "75m 40s (- 189m 56s) (118000 28%) 3.7812\n",
      "76m 19s (- 189m 17s) (119000 28%) 3.8165\n",
      "76m 57s (- 188m 38s) (120000 28%) 3.7421\n",
      "77m 35s (- 187m 58s) (121000 29%) 3.7556\n",
      "78m 13s (- 187m 19s) (122000 29%) 3.7149\n",
      "78m 52s (- 186m 41s) (123000 29%) 3.7184\n",
      "79m 30s (- 186m 1s) (124000 29%) 3.6726\n",
      "80m 7s (- 185m 20s) (125000 30%) 3.7466\n",
      "80m 45s (- 184m 41s) (126000 30%) 3.7052\n",
      "81m 23s (- 184m 1s) (127000 30%) 3.6479\n",
      "82m 1s (- 183m 22s) (128000 30%) 3.7153\n",
      "82m 38s (- 182m 41s) (129000 31%) 3.7241\n",
      "83m 16s (- 182m 1s) (130000 31%) 3.6926\n",
      "83m 54s (- 181m 22s) (131000 31%) 3.6977\n",
      "84m 32s (- 180m 41s) (132000 31%) 3.8567\n",
      "85m 10s (- 180m 3s) (133000 32%) 3.8460\n",
      "85m 49s (- 179m 26s) (134000 32%) 3.8010\n",
      "86m 29s (- 178m 50s) (135000 32%) 3.8231\n",
      "87m 8s (- 178m 12s) (136000 32%) 3.7797\n",
      "87m 47s (- 177m 35s) (137000 33%) 3.7135\n",
      "88m 26s (- 176m 57s) (138000 33%) 3.7301\n",
      "89m 4s (- 176m 19s) (139000 33%) 3.7289\n",
      "89m 44s (- 175m 44s) (140000 33%) 3.6639\n",
      "90m 24s (- 175m 7s) (141000 34%) 3.7289\n",
      "91m 3s (- 174m 30s) (142000 34%) 3.7387\n",
      "91m 42s (- 173m 52s) (143000 34%) 3.7207\n",
      "92m 21s (- 173m 15s) (144000 34%) 3.7436\n",
      "93m 0s (- 172m 38s) (145000 35%) 3.6981\n",
      "93m 39s (- 172m 0s) (146000 35%) 3.7093\n",
      "94m 17s (- 171m 21s) (147000 35%) 3.7002\n",
      "94m 56s (- 170m 44s) (148000 35%) 3.7594\n",
      "95m 36s (- 170m 7s) (149000 35%) 3.7534\n",
      "96m 15s (- 169m 30s) (150000 36%) 3.6753\n",
      "96m 55s (- 168m 53s) (151000 36%) 3.7024\n",
      "97m 34s (- 168m 16s) (152000 36%) 3.7854\n",
      "98m 13s (- 167m 38s) (153000 36%) 3.7288\n",
      "98m 51s (- 166m 59s) (154000 37%) 3.7023\n",
      "99m 30s (- 166m 22s) (155000 37%) 3.7361\n",
      "100m 10s (- 165m 45s) (156000 37%) 3.7096\n",
      "100m 48s (- 165m 6s) (157000 37%) 3.6662\n",
      "101m 27s (- 164m 28s) (158000 38%) 3.7121\n",
      "102m 6s (- 163m 50s) (159000 38%) 3.6846\n",
      "102m 44s (- 163m 11s) (160000 38%) 3.6424\n",
      "103m 22s (- 162m 32s) (161000 38%) 3.6415\n",
      "104m 1s (- 161m 53s) (162000 39%) 3.6379\n",
      "104m 39s (- 161m 15s) (163000 39%) 3.6915\n",
      "105m 18s (- 160m 36s) (164000 39%) 3.7003\n",
      "105m 56s (- 159m 58s) (165000 39%) 3.9391\n",
      "106m 36s (- 159m 21s) (166000 40%) 3.8599\n",
      "107m 15s (- 158m 43s) (167000 40%) 3.8331\n",
      "107m 54s (- 158m 5s) (168000 40%) 3.8030\n",
      "108m 32s (- 157m 27s) (169000 40%) 3.7760\n",
      "109m 11s (- 156m 49s) (170000 41%) 3.7435\n",
      "109m 49s (- 156m 10s) (171000 41%) 3.7200\n",
      "110m 28s (- 155m 31s) (172000 41%) 3.6932\n",
      "111m 7s (- 154m 53s) (173000 41%) 3.7861\n",
      "111m 46s (- 154m 15s) (174000 42%) 3.7946\n",
      "112m 25s (- 153m 38s) (175000 42%) 3.7469\n",
      "113m 5s (- 153m 0s) (176000 42%) 3.7657\n",
      "113m 43s (- 152m 21s) (177000 42%) 3.7195\n",
      "114m 21s (- 151m 42s) (178000 42%) 3.7015\n",
      "115m 0s (- 151m 4s) (179000 43%) 3.7021\n",
      "115m 39s (- 150m 26s) (180000 43%) 3.7546\n"
     ]
    }
   ],
   "source": [
    "def train(target_variable, \n",
    "          decoder, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # target_variable has (batch_size, n_words, n_vocab)\n",
    "    target_length = target_variable.size()[1]\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # First word in sentence needs to be fed h1=0\n",
    "    decoder_input = target_variable[0][1] # First one is SOS\n",
    "    prev_hidden = (decoder.initHidden(), decoder.initHidden())\n",
    "    predicted_word_index = 0\n",
    "\n",
    "    for index_word in range(2, target_length):\n",
    "        decoder_input = decoder_input.view(1, 1, vocabularySize)\n",
    "        decoder_output, prev_hidden = decoder(decoder_input, prev_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_word_index = int(topi[0][0][0])\n",
    "        # print('sum:', decoder_output.sum().data[0])\n",
    "        # print(index_word, predicted_word_index, topv[0][0][0])\n",
    "        # This is the next input, without teacher forcing it's the predicted output\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_word_index])\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "        # This is just to conform with the pytorch format..\n",
    "        # CrossEntropyLoss takes input1: (N, C) and input2: (N).\n",
    "        _, actual_word_index = target_variable[0][index_word].data.topk(1)\n",
    "        actual_word_index = Variable(actual_word_index)\n",
    "\n",
    "        # Compare current output to next \"target\" input\n",
    "        loss += criterion(decoder_output.view(1, decoder_output.size(2)), actual_word_index)\n",
    "        \n",
    "        # Stop on EOS\n",
    "        # if predicted_word_index == word2index['<EOS>']:\n",
    "        #   break\n",
    "            \n",
    "    \n",
    "    # Last word in sentence is fed x=0\n",
    "    # zeros = Variable(torch.zeros(1, 1, vocabularySize).double()).cuda()\n",
    "    # decoder_output, _ = decoder(zeros, prev_hidden)\n",
    "    # loss += criterion(decoder_output, zeros) # What should this be?\n",
    "    \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # index_word keeps track of the current word\n",
    "    # in case of break (EOS) and non-break (teacher-forcing), it'll be the actually count.\n",
    "    return loss.data[0] / index_word\n",
    "    \n",
    "\n",
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "decoder = DecoderLSTM(vocabularySize, 300, vocabularySize)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.NLLLoss()  # Since my LSTM has softmax as final layer, add NLL loss instead\n",
    "\n",
    "n_iters = len(train_sentences)\n",
    "print_every = 1000\n",
    "print_loss_total = 0\n",
    "start = time.time()\n",
    "for s_index in range(1, n_iters):\n",
    "    input_sentence = preprocess_one_hot(train_sentences[s_index])\n",
    "    n_words = input_sentence.shape[0]\n",
    "    input_sentence = torch.from_numpy(input_sentence)\n",
    "    input_sentence = input_sentence.view(1, n_words, vocabularySize)\n",
    "    input_sentence = Variable(input_sentence).cuda()\n",
    "    loss = train(input_sentence, decoder, decoder_optimizer, criterion)\n",
    "    \n",
    "    print_loss_total += loss\n",
    "    \n",
    "    if s_index % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start, s_index / n_iters),\n",
    "                                     s_index, s_index / n_iters * 100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models\n",
    "    1. './model/decoder_noEOS_23000_3_48'\n",
    "\"\"\"\n",
    "torch.save(decoder.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder and test it with a few different starting words. This code will be fairly similar to your training function from part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Your code goes here\n",
    "\n",
    "print(inference(decoder, init_word=\"the\"))\n",
    "print(inference(decoder, init_word=\"man\"))\n",
    "print(inference(decoder, init_word=\"woman\"))\n",
    "print(inference(decoder, init_word=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building Language Decoder Sampling Inference\n",
    "\n",
    "We must now modify the method defined in part 3, to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampling_inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Your code goes here\n",
    "\n",
    "# Print the results with sampling_inference by drawing 5 samples per initial word, requiring to run \n",
    "# the code below 5 times\n",
    "print(sampling_inference(decoder, init_word=\"the\"))\n",
    "print(sampling_inference(decoder, init_word=\"man\"))\n",
    "print(sampling_inference(decoder, init_word=\"woman\"))\n",
    "print(sampling_inference(decoder, init_word=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "\n",
    "# Initialize the encoder with a hidden size of 300. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Connecting Encoder to Decoder and Training End-to-End\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "It's likely that you'll be able to re-use most of your code from part 2. For our purposes, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_inference(sentence, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_encoder_hidden(sentence):\n",
    "    # Your code goes here\n",
    "\n",
    "# Now run all training data and validation data to store hidden states\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now get nearest neighbors and print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Effectiveness of word2vec\n",
    "\n",
    "We now repeat everything done above using word2vec embeddings in place of one-hot embeddings. This will require re-running steps 1-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
