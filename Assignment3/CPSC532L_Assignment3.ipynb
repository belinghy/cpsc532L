{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\belin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from random import random\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "For this assignment, you must download the data and extract it into `data/`. The dataset contains two files, both containing a single caption on each line. We should have 415,795 sentences in the training captions and 500 sentences in the validation captions.\n",
    "\n",
    "To download the data, run the following directly on your server: `wget https://s3-us-west-2.amazonaws.com/cpsc532l-data/a3_data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414143\n",
      "500\n",
      "A very clean and well decorated empty bathroom.\n"
     ]
    }
   ],
   "source": [
    "# Load the data into memory.\n",
    "train_sentences = [line.strip() for line in open(\"data/mscoco_train_captions.txt\").readlines() if line.strip() != '']\n",
    "val_sentences = [line.strip() for line in open(\"data/mscoco_val_captions.txt\").readlines()]\n",
    "\n",
    "for index, sentence in enumerate(train_sentences):\n",
    "    if sentence[-1] != '.':\n",
    "        train_sentences[index] = sentence + '.'\n",
    "\n",
    "for index, sentence in enumerate(val_sentences):\n",
    "    if sentence[-1] != '.':\n",
    "        val_sentences[index] = sentence + '.'\n",
    "        \n",
    "print(len(train_sentences))\n",
    "print(len(val_sentences))\n",
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "The code provided below creates word embeddings for you to use. After creating the vocabulary, we construct both one-hot embeddings and word2vec embeddings. \n",
    "\n",
    "All of the packages utilized should be installed on your Azure servers, however you will have to download an NLTK corpus. To do this, follow the instructions below:\n",
    "\n",
    "1. SSH to your Azure server\n",
    "2. Open up Python interpreter\n",
    "3. `import nltk`\n",
    "4. `nltk.download()`\n",
    "\n",
    "    You should now see something that looks like:\n",
    "\n",
    "    ```\n",
    "    >>> nltk.download()\n",
    "    NLTK Downloader\n",
    "    ---------------------------------------------------------------------------\n",
    "        d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
    "    ---------------------------------------------------------------------------\n",
    "    Downloader> \n",
    "\n",
    "    ```\n",
    "\n",
    "5. `d punkt`\n",
    "6. Provided the download finished successfully, you may now exit out of the Python interpreter and close the SSH connection.\n",
    "\n",
    "Please look through the functions provided below **carefully**, as you will need to use all of them at some point in your assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = train_sentences\n",
    "\n",
    "# Lower-case the sentence, tokenize them and add <SOS> and <EOS> tokens\n",
    "sentences = [[\"<SOS>\"] + word_tokenize(sentence.lower()) + [\"<EOS>\"] for sentence in sentences]\n",
    "\n",
    "# Create the vocabulary. Note that we add an <UNK> token to represent words not in our vocabulary.\n",
    "vocabularySize = 1000\n",
    "word_counts = Counter([word for sentence in sentences for word in sentence])\n",
    "vocabulary = [\"<UNK>\"] + [e[0] for e in word_counts.most_common(vocabularySize-1)]\n",
    "word2index = {word:index for index,word in enumerate(vocabulary)}\n",
    "one_hot_embeddings = np.eye(vocabularySize)\n",
    "\n",
    "# Build the word2vec embeddings\n",
    "wordEncodingSize = 300\n",
    "filtered_sentences = [[word for word in sentence if word in word2index] for sentence in sentences]\n",
    "w2v = Word2Vec(filtered_sentences, min_count=0, size=wordEncodingSize)\n",
    "w2v_embeddings = np.concatenate((np.zeros((1, wordEncodingSize)), w2v.wv.syn0))\n",
    "\n",
    "# Define the max sequence length to be the longest sentence in the training data. \n",
    "maxSequenceLength = max([len(sentence) for sentence in sentences])\n",
    "\n",
    "def preprocess_numberize(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into list of numbers (denoting the index into the vocabulary).\n",
    "    \"\"\"\n",
    "    tokenized = word_tokenize(sentence.lower())\n",
    "        \n",
    "    # Add the <SOS>/<EOS> tokens and numberize (all unknown words are represented as <UNK>).\n",
    "    tokenized = [\"<SOS>\"] + tokenized + [\"<EOS>\"]\n",
    "    numberized = [word2index.get(word, 0) for word in tokenized]\n",
    "    \n",
    "    return numberized\n",
    "\n",
    "def preprocess_one_hot(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of one-hot vectors.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    one_hot_embedded = one_hot_embeddings[numberized]\n",
    "    \n",
    "    return one_hot_embedded\n",
    "\n",
    "def preprocess_word2vec(sentence):\n",
    "    \"\"\"\n",
    "    Given a sentence, in the form of a string, this function will preprocess it\n",
    "    into a numpy array of word2vec embeddings.\n",
    "    \"\"\"\n",
    "    numberized = preprocess_numberize(sentence)\n",
    "    \n",
    "    # Represent each word as it's one-hot embedding\n",
    "    w2v_embedded = w2v_embeddings[numberized]\n",
    "    \n",
    "    return w2v_embedded\n",
    "\n",
    "def compute_bleu(reference_sentence, predicted_sentence):\n",
    "    \"\"\"\n",
    "    Given a reference sentence, and a predicted sentence, compute the BLEU similary between them.\n",
    "    \"\"\"\n",
    "    reference_tokenized = word_tokenize(reference_sentence.lower())\n",
    "    predicted_tokenized = word_tokenize(predicted_sentence.lower())\n",
    "    return sentence_bleu([reference_tokenized], predicted_tokenized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building a Language Decoder\n",
    "\n",
    "We now implement a language decoder. For now, we will have the decoder take a single training sample at a time (as opposed to batching). For our purposes, we will also avoid defining the embeddings as part of the model and instead pass in embedded inputs. While this is sometimes useful, as it learns/tunes the embeddings, we avoid doing it for the sake of simplicity and speed.\n",
    "\n",
    "Remember to use LSTM hidden units!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "(1000, 300)\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.64252836 -0.04464054 -0.02437208 ...  0.45854273 -0.40346768\n",
      "  -0.58654529]\n",
      " [ 0.29306009  0.19502763  0.38849041 ...  0.50763428 -0.24476127\n",
      "  -0.47664955]\n",
      " ...\n",
      " [-0.57009608  0.53415418 -0.37802663 ... -0.00389384 -0.61393547\n",
      "   0.01855109]\n",
      " [ 0.53694671 -0.15880792  0.51926643 ... -0.20401719 -0.10407556\n",
      "  -0.26090741]\n",
      " [ 0.403263   -1.45369279  0.12051474 ... -0.27986085  0.36217818\n",
      "  -0.32311931]]\n",
      "['<UNK>', 'a', '.', '<SOS>', '<EOS>']\n",
      "A blue and white bathroom with butterfly themed wall tiles.\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(11, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(maxSequenceLength)\n",
    "print(w2v_embeddings.shape)\n",
    "print(w2v_embeddings)\n",
    "\n",
    "print(vocabulary[0:5])\n",
    "print(train_sentences[2])\n",
    "print(preprocess_one_hot(train_sentences[0]))\n",
    "print(preprocess_one_hot(train_sentences[0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "( 0  ,.,.) = \n",
      "1.00000e-02 *\n",
      "  1.6582 -5.8084 -5.8018  ...   1.0613  5.2599  2.9784\n",
      "[torch.cuda.DoubleTensor of size 1x1x1000 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      " 0.2716  1.6143 -0.2492 -0.4618 -0.7349\n",
      "-0.2343 -2.1428  0.6304 -1.6372  1.3402\n",
      "-0.6010  1.5082 -1.9139  0.3596 -0.7784\n",
      "[torch.FloatTensor of size 3x5]\n",
      "\n",
      "Variable containing:\n",
      " 2\n",
      " 1\n",
      " 3\n",
      "[torch.LongTensor of size 3]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = preprocess_one_hot(train_sentences[0])\n",
    "input_sentence = torch.from_numpy(input_sentence[0])\n",
    "input_sentence = Variable(input_sentence.float())\n",
    "input_sentence = input_sentence.cuda()\n",
    "input_sentence = input_sentence.view(1, 1, 1000)\n",
    "\n",
    "lstm = nn.LSTM(1000, 300).cuda()\n",
    "output, hidden = lstm(input_sentence)\n",
    "\n",
    "linear = nn.Linear(300, 1000).double().cuda()\n",
    "output = linear(output.double().cuda())\n",
    "print(output)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "input = Variable(torch.randn(3, 5), requires_grad=True)\n",
    "target = Variable(torch.LongTensor(3).random_(5))\n",
    "print(input)\n",
    "print(target)\n",
    "output = loss(input, target)\n",
    "torch.cuda.current_device()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double().cuda()\n",
    "        self.linear = nn.Linear(hidden_size, output_size).double().cuda()\n",
    "        self.softmax = nn.LogSoftmax(dim=2).double().cuda()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        output = self.linear(output)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.double().cuda()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Training a Language Decoder\n",
    "\n",
    "We must now train the language decoder we implemented above. An important thing to pay attention to is the [inputs for an LSTM](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 37s (- 261m 20s) (1000 0%) 4.5910\n",
      "1m 16s (- 262m 17s) (2000 0%) 4.1580\n",
      "1m 55s (- 264m 13s) (3000 0%) 4.0781\n",
      "2m 35s (- 264m 54s) (4000 0%) 4.0283\n",
      "3m 13s (- 263m 40s) (5000 1%) 3.9425\n",
      "3m 51s (- 262m 50s) (6000 1%) 3.9330\n",
      "4m 31s (- 263m 1s) (7000 1%) 3.9186\n",
      "5m 10s (- 262m 41s) (8000 1%) 3.8757\n",
      "5m 49s (- 262m 6s) (9000 2%) 3.8457\n",
      "6m 28s (- 261m 39s) (10000 2%) 3.8771\n",
      "7m 7s (- 261m 18s) (11000 2%) 3.8434\n",
      "7m 47s (- 261m 13s) (12000 2%) 3.8914\n",
      "8m 27s (- 260m 45s) (13000 3%) 3.8548\n",
      "9m 5s (- 260m 4s) (14000 3%) 3.8475\n",
      "9m 44s (- 259m 10s) (15000 3%) 3.8205\n",
      "10m 23s (- 258m 36s) (16000 3%) 3.8072\n",
      "11m 2s (- 257m 53s) (17000 4%) 3.8749\n",
      "11m 40s (- 257m 5s) (18000 4%) 3.8085\n",
      "12m 20s (- 256m 43s) (19000 4%) 3.8706\n",
      "13m 0s (- 256m 15s) (20000 4%) 3.8384\n",
      "13m 39s (- 255m 38s) (21000 5%) 3.8192\n",
      "14m 18s (- 254m 58s) (22000 5%) 3.7802\n",
      "14m 56s (- 254m 5s) (23000 5%) 3.7696\n",
      "15m 35s (- 253m 30s) (24000 5%) 3.8240\n",
      "16m 14s (- 252m 47s) (25000 6%) 3.8611\n",
      "16m 52s (- 251m 57s) (26000 6%) 3.8123\n",
      "17m 30s (- 251m 9s) (27000 6%) 3.7500\n",
      "18m 9s (- 250m 27s) (28000 6%) 3.7581\n",
      "18m 48s (- 249m 41s) (29000 7%) 3.7882\n",
      "19m 26s (- 248m 54s) (30000 7%) 3.8025\n",
      "20m 4s (- 248m 9s) (31000 7%) 3.7828\n",
      "20m 43s (- 247m 26s) (32000 7%) 3.7181\n",
      "21m 21s (- 246m 45s) (33000 7%) 3.8824\n",
      "21m 59s (- 245m 54s) (34000 8%) 4.0305\n",
      "22m 38s (- 245m 17s) (35000 8%) 3.9964\n",
      "23m 18s (- 244m 48s) (36000 8%) 3.9432\n",
      "23m 56s (- 244m 3s) (37000 8%) 3.8414\n",
      "24m 34s (- 243m 16s) (38000 9%) 3.8394\n",
      "25m 13s (- 242m 33s) (39000 9%) 3.8691\n",
      "25m 52s (- 241m 57s) (40000 9%) 3.8435\n",
      "26m 30s (- 241m 14s) (41000 9%) 3.8256\n",
      "27m 8s (- 240m 32s) (42000 10%) 3.8432\n",
      "27m 48s (- 239m 57s) (43000 10%) 3.8688\n",
      "28m 27s (- 239m 21s) (44000 10%) 3.8955\n",
      "29m 6s (- 238m 43s) (45000 10%) 3.8825\n",
      "29m 45s (- 238m 8s) (46000 11%) 3.9264\n",
      "30m 23s (- 237m 28s) (47000 11%) 3.8017\n",
      "31m 2s (- 236m 47s) (48000 11%) 3.8625\n",
      "31m 40s (- 236m 5s) (49000 11%) 3.8894\n",
      "32m 19s (- 235m 23s) (50000 12%) 3.8703\n",
      "32m 57s (- 234m 38s) (51000 12%) 3.8337\n",
      "33m 35s (- 233m 57s) (52000 12%) 3.7874\n",
      "34m 14s (- 233m 18s) (53000 12%) 3.8007\n",
      "34m 52s (- 232m 37s) (54000 13%) 3.8484\n",
      "35m 31s (- 231m 57s) (55000 13%) 3.8291\n",
      "36m 9s (- 231m 15s) (56000 13%) 3.7878\n",
      "36m 48s (- 230m 34s) (57000 13%) 3.8007\n",
      "37m 26s (- 229m 54s) (58000 14%) 3.7978\n",
      "38m 4s (- 229m 9s) (59000 14%) 3.7807\n",
      "38m 41s (- 228m 24s) (60000 14%) 3.8211\n",
      "39m 19s (- 227m 42s) (61000 14%) 3.7759\n",
      "39m 57s (- 226m 59s) (62000 14%) 3.7941\n",
      "40m 36s (- 226m 18s) (63000 15%) 3.7777\n",
      "41m 13s (- 225m 34s) (64000 15%) 3.7592\n",
      "41m 52s (- 224m 54s) (65000 15%) 3.7872\n",
      "42m 30s (- 224m 13s) (66000 15%) 3.9073\n",
      "43m 8s (- 223m 31s) (67000 16%) 3.9476\n",
      "43m 47s (- 222m 53s) (68000 16%) 3.8877\n",
      "44m 26s (- 222m 17s) (69000 16%) 3.8564\n",
      "45m 4s (- 221m 37s) (70000 16%) 3.8137\n",
      "45m 43s (- 220m 58s) (71000 17%) 3.7716\n",
      "46m 21s (- 220m 16s) (72000 17%) 3.7791\n",
      "47m 0s (- 219m 38s) (73000 17%) 3.7831\n",
      "47m 38s (- 218m 58s) (74000 17%) 3.7400\n",
      "48m 17s (- 218m 20s) (75000 18%) 3.7658\n",
      "48m 56s (- 217m 43s) (76000 18%) 3.7905\n",
      "49m 34s (- 217m 5s) (77000 18%) 3.7491\n",
      "50m 12s (- 216m 24s) (78000 18%) 3.7512\n",
      "50m 50s (- 215m 42s) (79000 19%) 3.7339\n",
      "51m 29s (- 215m 2s) (80000 19%) 3.7664\n",
      "52m 7s (- 214m 23s) (81000 19%) 3.7044\n",
      "52m 46s (- 213m 44s) (82000 19%) 3.7559\n",
      "53m 24s (- 213m 4s) (83000 20%) 3.7592\n",
      "54m 2s (- 212m 23s) (84000 20%) 3.7142\n",
      "54m 41s (- 211m 46s) (85000 20%) 3.7252\n",
      "55m 19s (- 211m 7s) (86000 20%) 3.7928\n",
      "55m 58s (- 210m 28s) (87000 21%) 3.7672\n",
      "56m 36s (- 209m 48s) (88000 21%) 3.7804\n",
      "57m 15s (- 209m 9s) (89000 21%) 3.7311\n",
      "57m 53s (- 208m 29s) (90000 21%) 3.7545\n",
      "58m 31s (- 207m 49s) (91000 21%) 3.7411\n",
      "59m 9s (- 207m 10s) (92000 22%) 3.7290\n",
      "59m 47s (- 206m 29s) (93000 22%) 3.7461\n",
      "60m 25s (- 205m 48s) (94000 22%) 3.6798\n",
      "61m 3s (- 205m 8s) (95000 22%) 3.7054\n",
      "61m 41s (- 204m 26s) (96000 23%) 3.7273\n",
      "62m 19s (- 203m 45s) (97000 23%) 3.7012\n",
      "62m 57s (- 203m 4s) (98000 23%) 3.7045\n",
      "63m 34s (- 202m 23s) (99000 23%) 3.8385\n",
      "64m 12s (- 201m 41s) (100000 24%) 3.8788\n",
      "64m 50s (- 201m 1s) (101000 24%) 3.8200\n",
      "65m 29s (- 200m 24s) (102000 24%) 3.8439\n",
      "66m 7s (- 199m 44s) (103000 24%) 3.7698\n",
      "66m 45s (- 199m 5s) (104000 25%) 3.7335\n",
      "67m 24s (- 198m 26s) (105000 25%) 3.8018\n",
      "68m 2s (- 197m 47s) (106000 25%) 3.7530\n",
      "68m 41s (- 197m 9s) (107000 25%) 3.7496\n",
      "69m 19s (- 196m 30s) (108000 26%) 3.7653\n",
      "69m 57s (- 195m 50s) (109000 26%) 3.7737\n",
      "70m 35s (- 195m 11s) (110000 26%) 3.7772\n",
      "71m 14s (- 194m 33s) (111000 26%) 3.7913\n",
      "71m 52s (- 193m 52s) (112000 27%) 3.7489\n",
      "72m 30s (- 193m 13s) (113000 27%) 3.8030\n",
      "73m 8s (- 192m 33s) (114000 27%) 3.7364\n",
      "73m 46s (- 191m 53s) (115000 27%) 3.7266\n",
      "74m 24s (- 191m 13s) (116000 28%) 3.7221\n",
      "75m 2s (- 190m 34s) (117000 28%) 3.7018\n",
      "75m 40s (- 189m 56s) (118000 28%) 3.7812\n",
      "76m 19s (- 189m 17s) (119000 28%) 3.8165\n",
      "76m 57s (- 188m 38s) (120000 28%) 3.7421\n",
      "77m 35s (- 187m 58s) (121000 29%) 3.7556\n",
      "78m 13s (- 187m 19s) (122000 29%) 3.7149\n",
      "78m 52s (- 186m 41s) (123000 29%) 3.7184\n",
      "79m 30s (- 186m 1s) (124000 29%) 3.6726\n",
      "80m 7s (- 185m 20s) (125000 30%) 3.7466\n",
      "80m 45s (- 184m 41s) (126000 30%) 3.7052\n",
      "81m 23s (- 184m 1s) (127000 30%) 3.6479\n",
      "82m 1s (- 183m 22s) (128000 30%) 3.7153\n",
      "82m 38s (- 182m 41s) (129000 31%) 3.7241\n",
      "83m 16s (- 182m 1s) (130000 31%) 3.6926\n",
      "83m 54s (- 181m 22s) (131000 31%) 3.6977\n",
      "84m 32s (- 180m 41s) (132000 31%) 3.8567\n",
      "85m 10s (- 180m 3s) (133000 32%) 3.8460\n",
      "85m 49s (- 179m 26s) (134000 32%) 3.8010\n",
      "86m 29s (- 178m 50s) (135000 32%) 3.8231\n",
      "87m 8s (- 178m 12s) (136000 32%) 3.7797\n",
      "87m 47s (- 177m 35s) (137000 33%) 3.7135\n",
      "88m 26s (- 176m 57s) (138000 33%) 3.7301\n",
      "89m 4s (- 176m 19s) (139000 33%) 3.7289\n",
      "89m 44s (- 175m 44s) (140000 33%) 3.6639\n",
      "90m 24s (- 175m 7s) (141000 34%) 3.7289\n",
      "91m 3s (- 174m 30s) (142000 34%) 3.7387\n",
      "91m 42s (- 173m 52s) (143000 34%) 3.7207\n",
      "92m 21s (- 173m 15s) (144000 34%) 3.7436\n",
      "93m 0s (- 172m 38s) (145000 35%) 3.6981\n",
      "93m 39s (- 172m 0s) (146000 35%) 3.7093\n",
      "94m 17s (- 171m 21s) (147000 35%) 3.7002\n",
      "94m 56s (- 170m 44s) (148000 35%) 3.7594\n",
      "95m 36s (- 170m 7s) (149000 35%) 3.7534\n",
      "96m 15s (- 169m 30s) (150000 36%) 3.6753\n",
      "96m 55s (- 168m 53s) (151000 36%) 3.7024\n",
      "97m 34s (- 168m 16s) (152000 36%) 3.7854\n",
      "98m 13s (- 167m 38s) (153000 36%) 3.7288\n",
      "98m 51s (- 166m 59s) (154000 37%) 3.7023\n",
      "99m 30s (- 166m 22s) (155000 37%) 3.7361\n",
      "100m 10s (- 165m 45s) (156000 37%) 3.7096\n",
      "100m 48s (- 165m 6s) (157000 37%) 3.6662\n",
      "101m 27s (- 164m 28s) (158000 38%) 3.7121\n",
      "102m 6s (- 163m 50s) (159000 38%) 3.6846\n",
      "102m 44s (- 163m 11s) (160000 38%) 3.6424\n",
      "103m 22s (- 162m 32s) (161000 38%) 3.6415\n",
      "104m 1s (- 161m 53s) (162000 39%) 3.6379\n",
      "104m 39s (- 161m 15s) (163000 39%) 3.6915\n",
      "105m 18s (- 160m 36s) (164000 39%) 3.7003\n",
      "105m 56s (- 159m 58s) (165000 39%) 3.9391\n",
      "106m 36s (- 159m 21s) (166000 40%) 3.8599\n",
      "107m 15s (- 158m 43s) (167000 40%) 3.8331\n",
      "107m 54s (- 158m 5s) (168000 40%) 3.8030\n",
      "108m 32s (- 157m 27s) (169000 40%) 3.7760\n",
      "109m 11s (- 156m 49s) (170000 41%) 3.7435\n",
      "109m 49s (- 156m 10s) (171000 41%) 3.7200\n",
      "110m 28s (- 155m 31s) (172000 41%) 3.6932\n",
      "111m 7s (- 154m 53s) (173000 41%) 3.7861\n",
      "111m 46s (- 154m 15s) (174000 42%) 3.7946\n",
      "112m 25s (- 153m 38s) (175000 42%) 3.7469\n",
      "113m 5s (- 153m 0s) (176000 42%) 3.7657\n",
      "113m 43s (- 152m 21s) (177000 42%) 3.7195\n",
      "114m 21s (- 151m 42s) (178000 42%) 3.7015\n",
      "115m 0s (- 151m 4s) (179000 43%) 3.7021\n",
      "115m 39s (- 150m 26s) (180000 43%) 3.7546\n",
      "116m 18s (- 149m 49s) (181000 43%) 3.7633\n",
      "116m 57s (- 149m 10s) (182000 43%) 3.6279\n",
      "117m 36s (- 148m 32s) (183000 44%) 3.7371\n",
      "118m 18s (- 147m 59s) (184000 44%) 3.7707\n",
      "118m 58s (- 147m 21s) (185000 44%) 3.7199\n",
      "119m 37s (- 146m 43s) (186000 44%) 3.7034\n",
      "120m 16s (- 146m 5s) (187000 45%) 3.7177\n",
      "120m 55s (- 145m 27s) (188000 45%) 3.7220\n",
      "121m 35s (- 144m 50s) (189000 45%) 3.7420\n",
      "122m 13s (- 144m 11s) (190000 45%) 3.6722\n",
      "122m 52s (- 143m 33s) (191000 46%) 3.7079\n",
      "123m 31s (- 142m 54s) (192000 46%) 3.6490\n",
      "124m 9s (- 142m 16s) (193000 46%) 3.7338\n",
      "124m 48s (- 141m 37s) (194000 46%) 3.6747\n",
      "125m 27s (- 140m 59s) (195000 47%) 3.6818\n",
      "126m 6s (- 140m 20s) (196000 47%) 3.6754\n",
      "126m 44s (- 139m 42s) (197000 47%) 3.8919\n",
      "127m 22s (- 139m 2s) (198000 47%) 3.9904\n",
      "128m 0s (- 138m 23s) (199000 48%) 3.8527\n",
      "128m 39s (- 137m 45s) (200000 48%) 3.8090\n",
      "129m 17s (- 137m 6s) (201000 48%) 3.7705\n",
      "129m 56s (- 136m 27s) (202000 48%) 3.7675\n",
      "130m 34s (- 135m 48s) (203000 49%) 3.7014\n",
      "131m 13s (- 135m 10s) (204000 49%) 3.7292\n",
      "131m 50s (- 134m 30s) (205000 49%) 3.6613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132m 29s (- 133m 52s) (206000 49%) 3.6969\n",
      "133m 7s (- 133m 13s) (207000 49%) 3.7438\n",
      "133m 46s (- 132m 34s) (208000 50%) 3.7131\n",
      "134m 25s (- 131m 56s) (209000 50%) 3.6926\n",
      "135m 3s (- 131m 17s) (210000 50%) 3.6695\n",
      "135m 41s (- 130m 38s) (211000 50%) 3.6021\n",
      "136m 18s (- 129m 58s) (212000 51%) 3.6214\n",
      "136m 57s (- 129m 19s) (213000 51%) 3.6773\n",
      "137m 35s (- 128m 41s) (214000 51%) 3.6741\n",
      "138m 14s (- 128m 2s) (215000 51%) 3.6372\n",
      "138m 53s (- 127m 24s) (216000 52%) 3.6755\n",
      "139m 32s (- 126m 45s) (217000 52%) 3.7058\n",
      "140m 9s (- 126m 6s) (218000 52%) 3.6152\n",
      "140m 48s (- 125m 27s) (219000 52%) 3.6222\n",
      "141m 26s (- 124m 48s) (220000 53%) 3.6473\n",
      "142m 4s (- 124m 10s) (221000 53%) 3.7172\n",
      "142m 42s (- 123m 31s) (222000 53%) 3.6645\n",
      "143m 20s (- 122m 51s) (223000 53%) 3.6063\n",
      "143m 58s (- 122m 13s) (224000 54%) 3.5773\n",
      "144m 37s (- 121m 34s) (225000 54%) 3.6502\n",
      "145m 16s (- 120m 56s) (226000 54%) 3.6489\n",
      "145m 53s (- 120m 16s) (227000 54%) 3.5710\n",
      "146m 31s (- 119m 37s) (228000 55%) 3.6006\n",
      "147m 9s (- 118m 58s) (229000 55%) 3.6585\n",
      "147m 47s (- 118m 19s) (230000 55%) 3.9401\n",
      "148m 25s (- 117m 40s) (231000 55%) 3.9272\n",
      "149m 3s (- 117m 1s) (232000 56%) 3.8677\n",
      "149m 43s (- 116m 23s) (233000 56%) 3.8788\n",
      "150m 21s (- 115m 44s) (234000 56%) 3.7265\n",
      "150m 59s (- 115m 6s) (235000 56%) 3.7749\n",
      "151m 37s (- 114m 27s) (236000 56%) 3.7709\n",
      "152m 16s (- 113m 49s) (237000 57%) 3.7186\n",
      "152m 54s (- 113m 10s) (238000 57%) 3.6971\n",
      "153m 33s (- 112m 31s) (239000 57%) 3.6908\n",
      "154m 12s (- 111m 53s) (240000 57%) 3.7208\n",
      "154m 50s (- 111m 14s) (241000 58%) 3.7553\n",
      "155m 29s (- 110m 36s) (242000 58%) 3.7271\n",
      "156m 7s (- 109m 57s) (243000 58%) 3.6829\n",
      "156m 45s (- 109m 18s) (244000 58%) 3.7293\n",
      "157m 23s (- 108m 39s) (245000 59%) 3.7362\n",
      "158m 1s (- 108m 0s) (246000 59%) 3.7212\n",
      "158m 40s (- 107m 22s) (247000 59%) 3.7207\n",
      "159m 18s (- 106m 43s) (248000 59%) 3.7035\n",
      "159m 57s (- 106m 5s) (249000 60%) 3.7413\n",
      "160m 35s (- 105m 26s) (250000 60%) 3.7458\n",
      "161m 14s (- 104m 48s) (251000 60%) 3.6731\n",
      "161m 52s (- 104m 9s) (252000 60%) 3.7205\n",
      "162m 30s (- 103m 30s) (253000 61%) 3.7071\n",
      "163m 9s (- 102m 51s) (254000 61%) 3.7034\n",
      "163m 47s (- 102m 13s) (255000 61%) 3.7472\n",
      "164m 25s (- 101m 34s) (256000 61%) 3.7091\n",
      "165m 2s (- 100m 55s) (257000 62%) 3.6491\n",
      "165m 40s (- 100m 16s) (258000 62%) 3.6647\n",
      "166m 19s (- 99m 37s) (259000 62%) 3.6458\n",
      "166m 57s (- 98m 58s) (260000 62%) 3.6420\n",
      "167m 34s (- 98m 19s) (261000 63%) 3.6373\n",
      "168m 13s (- 97m 41s) (262000 63%) 3.6604\n",
      "168m 51s (- 97m 2s) (263000 63%) 3.9148\n",
      "169m 30s (- 96m 23s) (264000 63%) 3.8560\n",
      "170m 9s (- 95m 45s) (265000 63%) 3.8731\n",
      "170m 49s (- 95m 8s) (266000 64%) 3.7969\n",
      "171m 27s (- 94m 29s) (267000 64%) 3.7390\n",
      "172m 6s (- 93m 50s) (268000 64%) 3.7226\n",
      "172m 44s (- 93m 12s) (269000 64%) 3.7144\n",
      "173m 23s (- 92m 33s) (270000 65%) 3.6658\n",
      "174m 1s (- 91m 55s) (271000 65%) 3.7004\n",
      "174m 41s (- 91m 17s) (272000 65%) 3.7256\n",
      "175m 19s (- 90m 38s) (273000 65%) 3.7507\n",
      "175m 59s (- 90m 0s) (274000 66%) 3.7271\n",
      "176m 37s (- 89m 22s) (275000 66%) 3.7030\n",
      "177m 16s (- 88m 43s) (276000 66%) 3.7159\n",
      "177m 55s (- 88m 5s) (277000 66%) 3.7096\n",
      "178m 33s (- 87m 26s) (278000 67%) 3.7068\n",
      "179m 12s (- 86m 48s) (279000 67%) 3.7434\n",
      "179m 51s (- 86m 9s) (280000 67%) 3.7145\n",
      "180m 30s (- 85m 31s) (281000 67%) 3.6506\n",
      "181m 9s (- 84m 53s) (282000 68%) 3.6968\n",
      "181m 48s (- 84m 15s) (283000 68%) 3.7017\n",
      "182m 27s (- 83m 36s) (284000 68%) 3.6304\n",
      "183m 6s (- 82m 58s) (285000 68%) 3.6896\n",
      "183m 44s (- 82m 19s) (286000 69%) 3.6253\n",
      "184m 22s (- 81m 40s) (287000 69%) 3.7228\n",
      "185m 0s (- 81m 2s) (288000 69%) 3.7200\n",
      "185m 39s (- 80m 23s) (289000 69%) 3.6908\n",
      "186m 17s (- 79m 44s) (290000 70%) 3.6858\n",
      "186m 55s (- 79m 6s) (291000 70%) 3.6646\n",
      "187m 33s (- 78m 27s) (292000 70%) 3.6512\n",
      "188m 11s (- 77m 48s) (293000 70%) 3.6899\n",
      "188m 50s (- 77m 10s) (294000 70%) 3.5802\n",
      "189m 28s (- 76m 31s) (295000 71%) 3.6909\n",
      "190m 6s (- 75m 52s) (296000 71%) 4.0242\n",
      "190m 45s (- 75m 14s) (297000 71%) 3.8707\n",
      "191m 24s (- 74m 36s) (298000 71%) 3.8130\n",
      "192m 3s (- 73m 57s) (299000 72%) 3.7192\n",
      "192m 42s (- 73m 19s) (300000 72%) 3.7502\n",
      "193m 21s (- 72m 40s) (301000 72%) 3.7108\n",
      "194m 0s (- 72m 2s) (302000 72%) 3.6948\n",
      "194m 39s (- 71m 24s) (303000 73%) 3.7125\n",
      "195m 17s (- 70m 45s) (304000 73%) 3.6508\n",
      "195m 56s (- 70m 7s) (305000 73%) 3.7331\n",
      "196m 36s (- 69m 28s) (306000 73%) 3.6928\n",
      "197m 15s (- 68m 50s) (307000 74%) 3.7119\n",
      "197m 55s (- 68m 12s) (308000 74%) 3.6533\n",
      "198m 33s (- 67m 33s) (309000 74%) 3.6230\n",
      "199m 12s (- 66m 55s) (310000 74%) 3.6029\n",
      "199m 51s (- 66m 16s) (311000 75%) 3.6553\n",
      "200m 30s (- 65m 38s) (312000 75%) 3.6834\n",
      "201m 8s (- 64m 59s) (313000 75%) 3.6319\n",
      "201m 48s (- 64m 21s) (314000 75%) 3.6557\n",
      "202m 27s (- 63m 43s) (315000 76%) 3.6551\n",
      "203m 6s (- 63m 4s) (316000 76%) 3.5998\n",
      "203m 44s (- 62m 26s) (317000 76%) 3.5914\n",
      "204m 24s (- 61m 47s) (318000 76%) 3.6679\n",
      "205m 2s (- 61m 9s) (319000 77%) 3.6548\n",
      "205m 41s (- 60m 30s) (320000 77%) 3.6727\n",
      "206m 20s (- 59m 52s) (321000 77%) 3.5913\n",
      "206m 58s (- 59m 13s) (322000 77%) 3.6124\n",
      "207m 37s (- 58m 35s) (323000 77%) 3.5825\n",
      "208m 16s (- 57m 56s) (324000 78%) 3.6094\n",
      "208m 55s (- 57m 18s) (325000 78%) 3.6195\n",
      "209m 33s (- 56m 39s) (326000 78%) 3.6130\n",
      "210m 12s (- 56m 1s) (327000 78%) 3.6073\n",
      "210m 51s (- 55m 22s) (328000 79%) 3.8651\n",
      "211m 29s (- 54m 43s) (329000 79%) 4.1400\n",
      "212m 8s (- 54m 5s) (330000 79%) 3.8778\n",
      "212m 47s (- 53m 27s) (331000 79%) 3.9199\n",
      "213m 27s (- 52m 48s) (332000 80%) 3.8676\n",
      "214m 6s (- 52m 10s) (333000 80%) 3.7639\n",
      "214m 45s (- 51m 31s) (334000 80%) 3.7704\n",
      "215m 23s (- 50m 53s) (335000 80%) 3.7402\n",
      "216m 3s (- 50m 14s) (336000 81%) 3.7937\n",
      "216m 42s (- 49m 36s) (337000 81%) 3.7426\n",
      "217m 21s (- 48m 58s) (338000 81%) 3.8068\n",
      "218m 1s (- 48m 19s) (339000 81%) 3.7373\n",
      "218m 41s (- 47m 41s) (340000 82%) 3.7750\n",
      "219m 20s (- 47m 2s) (341000 82%) 3.7434\n",
      "219m 59s (- 46m 24s) (342000 82%) 3.7323\n",
      "220m 37s (- 45m 45s) (343000 82%) 3.7620\n",
      "221m 16s (- 45m 7s) (344000 83%) 3.7288\n",
      "221m 55s (- 44m 28s) (345000 83%) 3.7560\n",
      "222m 33s (- 43m 49s) (346000 83%) 3.7190\n",
      "223m 12s (- 43m 11s) (347000 83%) 3.7432\n",
      "223m 52s (- 42m 33s) (348000 84%) 3.7467\n",
      "224m 32s (- 41m 54s) (349000 84%) 3.7668\n",
      "225m 11s (- 41m 16s) (350000 84%) 3.7167\n",
      "225m 50s (- 40m 37s) (351000 84%) 3.6965\n",
      "226m 29s (- 39m 59s) (352000 84%) 3.7386\n",
      "227m 8s (- 39m 20s) (353000 85%) 3.7473\n",
      "227m 47s (- 38m 42s) (354000 85%) 3.7490\n",
      "228m 26s (- 38m 3s) (355000 85%) 3.6934\n",
      "229m 5s (- 37m 24s) (356000 85%) 3.7034\n",
      "229m 45s (- 36m 46s) (357000 86%) 3.7231\n",
      "230m 25s (- 36m 8s) (358000 86%) 3.7156\n",
      "231m 4s (- 35m 29s) (359000 86%) 3.6509\n",
      "231m 44s (- 34m 51s) (360000 86%) 3.6825\n",
      "232m 24s (- 34m 12s) (361000 87%) 3.9488\n",
      "233m 3s (- 33m 34s) (362000 87%) 3.8610\n",
      "233m 43s (- 32m 55s) (363000 87%) 3.8465\n",
      "234m 23s (- 32m 17s) (364000 87%) 3.7957\n",
      "235m 3s (- 31m 38s) (365000 88%) 3.8007\n",
      "235m 42s (- 31m 0s) (366000 88%) 3.7456\n",
      "236m 22s (- 30m 21s) (367000 88%) 3.8074\n",
      "237m 2s (- 29m 43s) (368000 88%) 3.7314\n",
      "237m 41s (- 29m 4s) (369000 89%) 3.7073\n",
      "238m 21s (- 28m 26s) (370000 89%) 3.7517\n",
      "239m 2s (- 27m 47s) (371000 89%) 3.8157\n",
      "239m 42s (- 27m 9s) (372000 89%) 3.7606\n",
      "240m 23s (- 26m 30s) (373000 90%) 3.7729\n",
      "241m 2s (- 25m 52s) (374000 90%) 3.7627\n",
      "241m 41s (- 25m 13s) (375000 90%) 3.7056\n",
      "242m 21s (- 24m 35s) (376000 90%) 3.7513\n",
      "243m 0s (- 23m 56s) (377000 91%) 3.7559\n",
      "243m 41s (- 23m 18s) (378000 91%) 3.7330\n",
      "244m 20s (- 22m 39s) (379000 91%) 3.7274\n",
      "245m 1s (- 22m 0s) (380000 91%) 3.8433\n",
      "245m 41s (- 21m 22s) (381000 91%) 3.7362\n",
      "246m 19s (- 20m 43s) (382000 92%) 3.7140\n",
      "246m 58s (- 20m 4s) (383000 92%) 3.6625\n",
      "247m 37s (- 19m 26s) (384000 92%) 3.6892\n",
      "248m 17s (- 18m 47s) (385000 92%) 3.7149\n",
      "248m 56s (- 18m 9s) (386000 93%) 3.7612\n",
      "249m 35s (- 17m 30s) (387000 93%) 3.7068\n",
      "250m 14s (- 16m 51s) (388000 93%) 3.7368\n",
      "250m 53s (- 16m 12s) (389000 93%) 3.6956\n",
      "251m 32s (- 15m 34s) (390000 94%) 3.7100\n",
      "252m 11s (- 14m 55s) (391000 94%) 3.6923\n",
      "252m 50s (- 14m 16s) (392000 94%) 3.7143\n",
      "253m 29s (- 13m 38s) (393000 94%) 3.7412\n",
      "254m 9s (- 12m 59s) (394000 95%) 3.8628\n",
      "254m 48s (- 12m 20s) (395000 95%) 3.7966\n",
      "255m 28s (- 11m 42s) (396000 95%) 3.7592\n",
      "256m 7s (- 11m 3s) (397000 95%) 3.7258\n",
      "256m 46s (- 10m 24s) (398000 96%) 3.7293\n",
      "257m 25s (- 9m 46s) (399000 96%) 3.7089\n",
      "258m 4s (- 9m 7s) (400000 96%) 3.7417\n",
      "258m 43s (- 8m 28s) (401000 96%) 3.7019\n",
      "259m 22s (- 7m 50s) (402000 97%) 3.6883\n",
      "260m 1s (- 7m 11s) (403000 97%) 3.6737\n",
      "260m 40s (- 6m 32s) (404000 97%) 3.7157\n",
      "261m 19s (- 5m 53s) (405000 97%) 3.6995\n",
      "261m 59s (- 5m 15s) (406000 98%) 3.6589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262m 38s (- 4m 36s) (407000 98%) 3.6639\n",
      "263m 17s (- 3m 57s) (408000 98%) 3.6860\n",
      "263m 56s (- 3m 19s) (409000 98%) 3.7168\n",
      "264m 35s (- 2m 40s) (410000 98%) 3.6655\n",
      "265m 14s (- 2m 1s) (411000 99%) 3.6563\n",
      "265m 53s (- 1m 22s) (412000 99%) 3.6704\n",
      "266m 31s (- 0m 44s) (413000 99%) 3.6663\n",
      "267m 11s (- 0m 5s) (414000 99%) 3.6633\n"
     ]
    }
   ],
   "source": [
    "def train(target_variable, \n",
    "          decoder, \n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings): \n",
    "    \"\"\"\n",
    "    Given a single training sample, go through a single step of training.\n",
    "    \"\"\"\n",
    "    \n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # target_variable has (batch_size, n_words, n_vocab)\n",
    "    target_length = target_variable.size()[1]\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # First word in sentence needs to be fed h1=0\n",
    "    decoder_input = target_variable[0][1] # First one is SOS\n",
    "    prev_hidden = (decoder.initHidden(), decoder.initHidden())\n",
    "    predicted_word_index = 0\n",
    "\n",
    "    for index_word in range(2, target_length):\n",
    "        decoder_input = decoder_input.view(1, 1, vocabularySize)\n",
    "        decoder_output, prev_hidden = decoder(decoder_input, prev_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_word_index = int(topi[0][0][0])\n",
    "        # print('sum:', decoder_output.sum().data[0])\n",
    "        # print(index_word, predicted_word_index, topv[0][0][0])\n",
    "        # This is the next input, without teacher forcing it's the predicted output\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_word_index])\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "        # This is just to conform with the pytorch format..\n",
    "        # CrossEntropyLoss takes input1: (N, C) and input2: (N).\n",
    "        _, actual_word_index = target_variable[0][index_word].data.topk(1)\n",
    "        actual_word_index = Variable(actual_word_index)\n",
    "\n",
    "        # Compare current output to next \"target\" input\n",
    "        loss += criterion(decoder_output.view(1, decoder_output.size(2)), actual_word_index)\n",
    "        \n",
    "        # Stop on EOS\n",
    "        # NOTE: Saw training is better without this, so commented out\n",
    "        # if predicted_word_index == word2index['<EOS>']:\n",
    "        #   break\n",
    "            \n",
    "    \n",
    "    # Last word in sentence is fed x=0\n",
    "    # zeros = Variable(torch.zeros(1, 1, vocabularySize).double()).cuda()\n",
    "    # decoder_output, _ = decoder(zeros, prev_hidden)\n",
    "    # loss += criterion(decoder_output, zeros) # What should this be?\n",
    "    \n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # index_word keeps track of the current word\n",
    "    # in case of break (EOS) and non-break (teacher-forcing), it'll be the actually count.\n",
    "    return loss.data[0] / index_word\n",
    "    \n",
    "\n",
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "decoder = DecoderLSTM(vocabularySize, 300, vocabularySize)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.NLLLoss()  # Since my DecoderLSTM has LogSoftmax as final layer, use NLL loss here\n",
    "\n",
    "n_iters = len(train_sentences)\n",
    "print_every = 1000\n",
    "print_loss_total = 0\n",
    "start = time.time()\n",
    "for s_index in range(1, n_iters):\n",
    "    input_sentence = preprocess_one_hot(train_sentences[s_index])\n",
    "    n_words = input_sentence.shape[0]\n",
    "    input_sentence = torch.from_numpy(input_sentence)\n",
    "    input_sentence = input_sentence.view(1, n_words, vocabularySize)\n",
    "    input_sentence = Variable(input_sentence).cuda()\n",
    "    loss = train(input_sentence, decoder, decoder_optimizer, criterion)\n",
    "    \n",
    "    print_loss_total += loss\n",
    "    \n",
    "    if s_index % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start, s_index / n_iters),\n",
    "                                     s_index, s_index / n_iters * 100, print_loss_avg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models\n",
    "    1. './model/decoder_noEOS_23000_3_48'  -- lr = 0.0001\n",
    "    2. './model/decoder_EOS_23000_3_48'    -- lr = 0.0001\n",
    "    3. './model/decoder_noEOS_414000_3_66' -- lr = 0.0001\n",
    "\"\"\"\n",
    "torch.save(decoder.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading\n",
    "decoder = DecoderLSTM(vocabularySize, 300, vocabularySize)\n",
    "decoder.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building Language Decoder MAP Inference\n",
    "\n",
    "We now define a method to perform inference with our decoder and test it with a few different starting words. This code will be fairly similar to your training function from part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the <UNK> is is <UNK> <UNK> <UNK> <UNK> <UNK> . . <EOS>\n",
      "man <UNK> a a <UNK> <UNK> a <UNK> . . <EOS>\n",
      "woman <UNK> a a <UNK> <UNK> a <UNK> . . <EOS>\n",
      "dog <UNK> a a <UNK> <UNK> <UNK> <UNK> . . <EOS>\n"
     ]
    }
   ],
   "source": [
    "def inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Your code goes here\n",
    "    \n",
    "    # Initialize\n",
    "    sentence_word_list = []\n",
    "    predicted_word_index = word2index[init_word]\n",
    "    sentence_word_list.append(vocabulary[predicted_word_index])\n",
    "    prev_hidden = (decoder.initHidden(), decoder.initHidden())\n",
    "    \n",
    "    # Convert to one hot\n",
    "    one_hot = embeddings[predicted_word_index]\n",
    "    decoder_input = torch.from_numpy(one_hot)\n",
    "    decoder_input = Variable(decoder_input).double().cuda()\n",
    "    \n",
    "    while predicted_word_index != word2index['<EOS>']:\n",
    "        # prediction\n",
    "        decoder_input = decoder_input.view(1, 1, vocabularySize)\n",
    "        decoder_output, prev_hidden = decoder(decoder_input, prev_hidden)\n",
    "        \n",
    "        # Process output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_word_index = int(topi[0][0][0])\n",
    "        sentence_word_list.append(vocabulary[predicted_word_index])\n",
    "        \n",
    "        # Package input for next loop\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_word_index])\n",
    "        decoder_input = Variable(decoder_input).double().cuda()\n",
    "    \n",
    "    return ' '.join(sentence_word_list)\n",
    "\n",
    "print(inference(decoder, init_word=\"the\"))\n",
    "print(inference(decoder, init_word=\"man\"))\n",
    "print(inference(decoder, init_word=\"woman\"))\n",
    "print(inference(decoder, init_word=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Building Language Decoder Sampling Inference\n",
    "\n",
    "We must now modify the method defined in part 3, to sample from the distribution outputted by the LSTM rather than taking the most probable word.\n",
    "\n",
    "It might be useful to take a look at the output of your model and (depending on your implementation) modify it so that the outputs sum to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeat 1.\n",
      "a. Starting with `the`:\n",
      "\t the boy surfing and land front to down on a a woman green green a black <SOS> <SOS> a <EOS>\n",
      "\n",
      "b. Starting with `man`:\n",
      "\t man cellphone one one floating on toilet the a a <SOS> <SOS> <SOS> blue <SOS> motorcycle on standing having <SOS> out <SOS> a <SOS> <SOS> a woman <EOS>\n",
      "\n",
      "c. Starting with `woman`:\n",
      "\t woman skateboard sunglasses hanging woman standing doing a <SOS> <UNK> a <SOS> on <SOS> horse <SOS> a <SOS> a person pole woman <SOS> make <SOS> working holding with a baseball <SOS> two <SOS> shower top posing player <SOS> <SOS> filled a tree player sheep in two black <EOS>\n",
      "\n",
      "d. Starting with `dog`:\n",
      "\t dog <UNK> in down are food a a base <EOS>\n",
      "Repeat 2.\n",
      "a. Starting with `the`:\n",
      "\t the mounted and and soup with wooden of and a <SOS> a to a a elephant a <SOS> three <SOS> having branch salad bikes and and <UNK> standing on <UNK> <UNK> a of the <SOS> <SOS> woman a holding a of toilet bridge down a other plane tour male drives branch center and and <UNK> in motorcycle <EOS>\n",
      "\n",
      "b. Starting with `man`:\n",
      "\t man on broccoli skateboards lights different bottles video a a with <SOS> carrying signs <SOS> <SOS> <SOS> a <SOS> on sign <SOS> man two boy tracks bicycles stand <EOS>\n",
      "\n",
      "c. Starting with `woman`:\n",
      "\t woman while swinging sandwich standing of <UNK> skiing a <SOS> <EOS>\n",
      "\n",
      "d. Starting with `dog`:\n",
      "\t dog two posing with skiing blue side a a <SOS> a throwing <SOS> other a is woman in a elephant orange other <SOS> close a other sinks are people <UNK> with people one on on a <SOS> blue the <SOS> <SOS> <SOS> laptops a a light other video from sinks drives sinks cement girl <EOS>\n",
      "Repeat 3.\n",
      "a. Starting with `the`:\n",
      "\t the other that orange wrapped and reading <UNK> <SOS> a a <UNK> <EOS>\n",
      "\n",
      "b. Starting with `man`:\n",
      "\t man with <UNK> screen rice and standing pitcher woman a dock <SOS> a <SOS> <SOS> <SOS> bridge a <SOS> the <SOS> tricks a top decker foot fruits throw in <UNK> <EOS>\n",
      "\n",
      "c. Starting with `woman`:\n",
      "\t woman to the <EOS>\n",
      "\n",
      "d. Starting with `dog`:\n",
      "\t dog shown has one hanging <UNK> <UNK> lights <UNK> river <UNK> <SOS> several <SOS> a <SOS> on elephant <SOS> holding a background sides adults at <EOS>\n",
      "Repeat 4.\n",
      "a. Starting with `the`:\n",
      "\t the white dressed little ground <EOS>\n",
      "\n",
      "b. Starting with `man`:\n",
      "\t man front <UNK> <UNK> couple the on laptop <SOS> a <SOS> trail decker veggies a <SOS> sunny <EOS>\n",
      "\n",
      "c. Starting with `woman`:\n",
      "\t woman that ski something <EOS>\n",
      "\n",
      "d. Starting with `dog`:\n",
      "\t dog that <UNK> <EOS>\n",
      "Repeat 5.\n",
      "a. Starting with `the`:\n",
      "\t the red player little <UNK> cows <EOS>\n",
      "\n",
      "b. Starting with `man`:\n",
      "\t man zebras <UNK> long branch glasses on <UNK> oven a wooden a woman <SOS> holding a <SOS> holding <SOS> ski player <SOS> <SOS> holding his desk <SOS> out <SOS> legs brown a next stopped <UNK> match with with in street a <UNK> on sheep cross on a player cement ground she sign light cut tie <UNK> winter from get with next tennis <SOS> vases <SOS> <SOS> <UNK> two the park four middle of big next a <SOS> lights <UNK> desk <SOS> <SOS> a of <SOS> piece square two <SOS> a player red table table of truck holding holding cross two large tall baseball through with a <UNK> two get of video from two next sitting surfboard with <SOS> woman slope a <SOS> truck out are two a a sheep bowls having tarmac with are all parked in <SOS> a a plate <SOS> <SOS> racket lady <SOS> floors orange that <UNK> lady on platform people with two on on on <SOS> with side woman and have other that brushing two <SOS> slice bow on motorcycle two a in laptop bridge couple path on from with that an the with and on <UNK> <SOS> a <UNK> very pole a is man man <UNK> two on <EOS>\n",
      "\n",
      "c. Starting with `woman`:\n",
      "\t woman walking <UNK> <UNK> hit <EOS>\n",
      "\n",
      "d. Starting with `dog`:\n",
      "\t dog in to one that a <UNK> <SOS> a <SOS> snow wood <SOS> <SOS> <SOS> covered woman on <UNK> a is <SOS> the two served bun single is are standing are open the a couple a adults with <SOS> <SOS> <SOS> <UNK> <SOS> man of of <SOS> body a other trail shirt <UNK> <SOS> sides holding signs woman two carrying a up light in laying <EOS>\n"
     ]
    }
   ],
   "source": [
    "def sampling_inference(decoder, init_word, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Your code goes here\n",
    "    \n",
    "    # Initialize\n",
    "    sentence_word_list = []\n",
    "    predicted_word_index = word2index[init_word]\n",
    "    sentence_word_list.append(vocabulary[predicted_word_index])\n",
    "    prev_hidden = (decoder.initHidden(), decoder.initHidden())\n",
    "    \n",
    "    # Convert to one hot\n",
    "    one_hot = embeddings[predicted_word_index]\n",
    "    decoder_input = torch.from_numpy(one_hot)\n",
    "    decoder_input = Variable(decoder_input).double().cuda()\n",
    "    \n",
    "    while predicted_word_index != word2index['<EOS>']:\n",
    "        # prediction\n",
    "        decoder_input = decoder_input.view(1, 1, vocabularySize)\n",
    "        decoder_output, prev_hidden = decoder(decoder_input, prev_hidden)\n",
    "        \n",
    "        # Process output\n",
    "        _numpy_array = decoder_output.squeeze().data.cpu().numpy()\n",
    "        probs = np.exp(_numpy_array) # original output was LogSoftmax, apply exp() to get probs\n",
    "        assert(np.isclose(np.sum(probs), 1.0)) # assert that probability sums to 1\n",
    "        \n",
    "        # Sample for a word according to probs\n",
    "        cdf = np.cumsum(probs) # Cumulative sum on probs to produce CDF\n",
    "        uniform_sample = np.random.uniform()\n",
    "        for _index, item in enumerate(cdf):\n",
    "            if uniform_sample > item and uniform_sample <= cdf[_index+1]:\n",
    "                # This is ok, because we'll never get to the last item in cdf\n",
    "                sentence_word_list.append(vocabulary[_index])\n",
    "                predicted_word_index = _index\n",
    "                break\n",
    "                \n",
    "        # Package input for next loop\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_word_index])\n",
    "        decoder_input = Variable(decoder_input).double().cuda()\n",
    "    \n",
    "    return ' '.join(sentence_word_list)\n",
    "\n",
    "# Print the results with sampling_inference by drawing 5 samples per initial word, requiring to run \n",
    "# the code below 5 times\n",
    "for repeat in range(1, 5+1):\n",
    "    print('Repeat {}.'.format(repeat))\n",
    "    print('a. Starting with `the`:')\n",
    "    print('\\t %s' % sampling_inference(decoder, init_word=\"the\"))\n",
    "    print('\\nb. Starting with `man`:')\n",
    "    print('\\t %s' % sampling_inference(decoder, init_word=\"man\"))\n",
    "    print('\\nc. Starting with `woman`:')\n",
    "    print('\\t %s' % sampling_inference(decoder, init_word=\"woman\"))\n",
    "    print('\\nd. Starting with `dog`:')\n",
    "    print('\\t %s' % sampling_inference(decoder, init_word=\"dog\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.  Building Language Encoder\n",
    "\n",
    "We now build a language encoder, which will encode an input word by word, and ultimately output a hidden state that we can then be used by our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    # Your code goes here\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size).double().cuda()\n",
    "\n",
    "    def forward(self, input, hidden_in):\n",
    "        _, hidden_out = self.lstm(input, hidden_in) # encoder only outputs hidden\n",
    "        return hidden_out\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result.double().cuda()\n",
    "        \n",
    "# Initialize the encoder with a hidden size of 300. \n",
    "encoder = EncoderLSTM(1000, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Connecting Encoder to Decoder and Training End-to-End\n",
    "\n",
    "We now connect our newly created encoder with our decoder, to train an end-to-end seq2seq architecture. \n",
    "\n",
    "It's likely that you'll be able to re-use most of your code from part 2. For our purposes, the only interaction between the encoder and the decoder is that the *last hidden state of the encoder is used as the initial hidden state of the decoder*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with old decoder\n",
    "decoder = DecoderLSTM(vocabularySize, 300, vocabularySize)\n",
    "decoder.load_state_dict(torch.load('./model/decoder_noEOS_414000_3_66'))\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = EncoderLSTM(1000, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 11s (- 494m 51s) (1000 0%) 3.7468\n",
      "2m 20s (- 481m 49s) (2000 0%) 3.8289\n",
      "3m 30s (- 480m 45s) (3000 0%) 3.8144\n",
      "4m 39s (- 477m 43s) (4000 0%) 3.7436\n",
      "5m 46s (- 472m 7s) (5000 1%) 3.6125\n",
      "6m 53s (- 468m 21s) (6000 1%) 3.5653\n",
      "8m 0s (- 465m 56s) (7000 1%) 3.5430\n",
      "9m 7s (- 463m 27s) (8000 1%) 3.4199\n",
      "10m 14s (- 460m 53s) (9000 2%) 3.3200\n",
      "11m 23s (- 460m 3s) (10000 2%) 3.3572\n",
      "12m 32s (- 459m 29s) (11000 2%) 3.2435\n",
      "13m 42s (- 459m 30s) (12000 2%) 3.2991\n",
      "14m 51s (- 458m 36s) (13000 3%) 3.1864\n",
      "16m 2s (- 458m 22s) (14000 3%) 3.0665\n",
      "17m 8s (- 456m 11s) (15000 3%) 3.1133\n",
      "18m 16s (- 454m 35s) (16000 3%) 2.9929\n",
      "19m 23s (- 452m 49s) (17000 4%) 3.0342\n",
      "20m 28s (- 450m 44s) (18000 4%) 2.8683\n",
      "21m 37s (- 449m 35s) (19000 4%) 2.9113\n",
      "22m 44s (- 448m 17s) (20000 4%) 2.8265\n",
      "23m 53s (- 447m 16s) (21000 5%) 2.8179\n",
      "25m 2s (- 446m 19s) (22000 5%) 2.7092\n",
      "26m 9s (- 444m 49s) (23000 5%) 2.6536\n",
      "27m 19s (- 444m 6s) (24000 5%) 2.6802\n",
      "28m 27s (- 442m 59s) (25000 6%) 2.7522\n",
      "29m 35s (- 441m 40s) (26000 6%) 2.6193\n",
      "30m 43s (- 440m 26s) (27000 6%) 2.4821\n",
      "31m 51s (- 439m 19s) (28000 6%) 2.4828\n",
      "32m 59s (- 438m 6s) (29000 7%) 2.4460\n",
      "34m 6s (- 436m 40s) (30000 7%) 2.4672\n",
      "35m 13s (- 435m 16s) (31000 7%) 2.4146\n",
      "36m 19s (- 433m 49s) (32000 7%) 2.3047\n",
      "37m 28s (- 432m 45s) (33000 7%) 2.4699\n",
      "38m 34s (- 431m 18s) (34000 8%) 2.6220\n",
      "39m 43s (- 430m 22s) (35000 8%) 2.6395\n",
      "40m 54s (- 429m 38s) (36000 8%) 2.5224\n",
      "42m 1s (- 428m 18s) (37000 8%) 2.3890\n",
      "43m 8s (- 426m 58s) (38000 9%) 2.2933\n",
      "44m 15s (- 425m 47s) (39000 9%) 2.2900\n",
      "45m 24s (- 424m 45s) (40000 9%) 2.2299\n",
      "46m 32s (- 423m 32s) (41000 9%) 2.1492\n",
      "47m 40s (- 422m 23s) (42000 10%) 2.1566\n",
      "48m 49s (- 421m 26s) (43000 10%) 2.1928\n",
      "49m 59s (- 420m 33s) (44000 10%) 2.1420\n",
      "51m 9s (- 419m 37s) (45000 10%) 2.0944\n",
      "52m 18s (- 418m 40s) (46000 11%) 2.1529\n",
      "53m 27s (- 417m 35s) (47000 11%) 1.9253\n",
      "54m 36s (- 416m 30s) (48000 11%) 2.0626\n",
      "55m 44s (- 415m 22s) (49000 11%) 2.0440\n",
      "56m 52s (- 414m 13s) (50000 12%) 1.9605\n",
      "58m 0s (- 413m 1s) (51000 12%) 1.8978\n",
      "59m 9s (- 411m 57s) (52000 12%) 1.8067\n",
      "60m 17s (- 410m 51s) (53000 12%) 1.7330\n",
      "61m 24s (- 409m 33s) (54000 13%) 1.7978\n",
      "62m 31s (- 408m 14s) (55000 13%) 1.7843\n",
      "63m 37s (- 406m 51s) (56000 13%) 1.7098\n",
      "64m 45s (- 405m 44s) (57000 13%) 1.6992\n",
      "65m 53s (- 404m 36s) (58000 14%) 1.7258\n",
      "67m 0s (- 403m 23s) (59000 14%) 1.6701\n",
      "68m 8s (- 402m 11s) (60000 14%) 1.6403\n",
      "69m 16s (- 401m 2s) (61000 14%) 1.6023\n",
      "70m 23s (- 399m 48s) (62000 14%) 1.5512\n",
      "71m 31s (- 398m 40s) (63000 15%) 1.6019\n",
      "72m 38s (- 397m 25s) (64000 15%) 1.5514\n",
      "73m 46s (- 396m 19s) (65000 15%) 1.5115\n",
      "74m 54s (- 395m 8s) (66000 15%) 1.7067\n",
      "76m 1s (- 393m 52s) (67000 16%) 1.7098\n",
      "77m 8s (- 392m 39s) (68000 16%) 1.7048\n",
      "78m 16s (- 391m 31s) (69000 16%) 1.6465\n",
      "79m 23s (- 390m 20s) (70000 16%) 1.5692\n",
      "80m 31s (- 389m 12s) (71000 17%) 1.4970\n",
      "81m 39s (- 388m 0s) (72000 17%) 1.4406\n",
      "82m 46s (- 386m 49s) (73000 17%) 1.4726\n",
      "83m 52s (- 385m 33s) (74000 17%) 1.3641\n",
      "84m 59s (- 384m 21s) (75000 18%) 1.4404\n",
      "86m 8s (- 383m 14s) (76000 18%) 1.4155\n",
      "87m 16s (- 382m 9s) (77000 18%) 1.3518\n",
      "88m 24s (- 380m 58s) (78000 18%) 1.3179\n",
      "89m 31s (- 379m 49s) (79000 19%) 1.2999\n",
      "90m 40s (- 378m 44s) (80000 19%) 1.3300\n",
      "91m 48s (- 377m 35s) (81000 19%) 1.2178\n",
      "92m 56s (- 376m 28s) (82000 19%) 1.2914\n",
      "94m 4s (- 375m 19s) (83000 20%) 1.2212\n",
      "95m 11s (- 374m 8s) (84000 20%) 1.1924\n",
      "96m 20s (- 373m 3s) (85000 20%) 1.1718\n",
      "97m 28s (- 371m 53s) (86000 20%) 1.2206\n",
      "98m 34s (- 370m 41s) (87000 21%) 1.2001\n",
      "99m 41s (- 369m 26s) (88000 21%) 1.1671\n",
      "100m 47s (- 368m 14s) (89000 21%) 1.1232\n",
      "101m 53s (- 366m 59s) (90000 21%) 1.1743\n",
      "103m 0s (- 365m 46s) (91000 21%) 1.1505\n",
      "104m 8s (- 364m 39s) (92000 22%) 1.1125\n",
      "105m 15s (- 363m 26s) (93000 22%) 1.0668\n",
      "106m 21s (- 362m 13s) (94000 22%) 0.9880\n",
      "107m 28s (- 361m 2s) (95000 22%) 1.0030\n",
      "108m 34s (- 359m 50s) (96000 23%) 1.0844\n",
      "109m 42s (- 358m 41s) (97000 23%) 1.0168\n",
      "110m 50s (- 357m 33s) (98000 23%) 0.9744\n",
      "111m 56s (- 356m 21s) (99000 23%) 1.1901\n",
      "113m 2s (- 355m 5s) (100000 24%) 1.2087\n",
      "114m 8s (- 353m 54s) (101000 24%) 1.2026\n",
      "115m 16s (- 352m 47s) (102000 24%) 1.2045\n",
      "116m 23s (- 351m 35s) (103000 24%) 1.1113\n",
      "117m 30s (- 350m 24s) (104000 25%) 1.0847\n",
      "118m 38s (- 349m 19s) (105000 25%) 1.0986\n",
      "119m 47s (- 348m 14s) (106000 25%) 1.0173\n",
      "120m 55s (- 347m 7s) (107000 25%) 0.9903\n",
      "122m 4s (- 346m 1s) (108000 26%) 1.0706\n",
      "123m 12s (- 344m 55s) (109000 26%) 0.9947\n",
      "124m 20s (- 343m 49s) (110000 26%) 1.0295\n",
      "125m 29s (- 342m 43s) (111000 26%) 0.9890\n",
      "126m 35s (- 341m 31s) (112000 27%) 0.9413\n",
      "127m 42s (- 340m 20s) (113000 27%) 1.0160\n",
      "128m 48s (- 339m 8s) (114000 27%) 0.9249\n",
      "129m 56s (- 338m 0s) (115000 27%) 0.9261\n",
      "131m 4s (- 336m 53s) (116000 28%) 0.8688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-82ab00dfb971>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[0minput_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_sentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocabularySize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[0minput_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-82ab00dfb971>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(target_variable, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion, embeddings)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m#   break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m     \u001b[0mencoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mdecoder_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\variable.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \"\"\"\n\u001b[1;32m--> 167\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m---> 99\u001b[1;33m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Your code goes here\n",
    "\n",
    "# Helper to flip a tensor\n",
    "# Taken from: https://github.com/pytorch/pytorch/issues/229\n",
    "def flip(x, dim):\n",
    "    xsize = x.size()\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    x = x.view(-1, *xsize[dim:])\n",
    "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
    "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
    "    return x.view(xsize)\n",
    "\n",
    "# One training step\n",
    "def train(target_variable,\n",
    "          encoder,\n",
    "          encoder_optimizer,\n",
    "          decoder,\n",
    "          decoder_optimizer, \n",
    "          criterion, \n",
    "          embeddings=one_hot_embeddings):\n",
    "    \n",
    "    # Some initilization\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    \n",
    "    # target_variable has (batch_size, n_words, n_vocab)\n",
    "    # Without minibatch, this is just one sentence\n",
    "    target_length = target_variable.size()[1]\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    # Reverse input sentence to help training\n",
    "    # For performance, don't actually do this, just reverse in loop\n",
    "    # flipped_target = flip(target_variable, 1)\n",
    "    \n",
    "    # Encoder is fed from the flipped sentence\n",
    "    encoder_input = target_variable[0][-1] # Starting from last\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    encoder_hidden = (encoder_hidden, encoder_hidden) # Need a tuple\n",
    "    \n",
    "    # Feeding encoder in a loop, in reverse order\n",
    "    # Starting from length - 2, since we set the last word above.\n",
    "    # Ending on index=1 to skip SOS as suggested in handout \n",
    "    for index_word in np.arange(target_length-2, 0, -1):\n",
    "        encoder_input = encoder_input.view(1, 1, vocabularySize)\n",
    "        encoder_hidden = encoder(encoder_input, encoder_hidden) # Gets hidden for next input    \n",
    "        # Get input for next loop from sentence\n",
    "        encoder_input = target_variable[0][index_word]\n",
    "    \n",
    "    # Do the same as part 2 for decoder, but feed encoder_hidden instead\n",
    "    decoder_input = target_variable[0][0]\n",
    "    decoder_hidden = encoder_hidden\n",
    "    predicted_word_index = 0\n",
    "    \n",
    "    for index_word in range(1, target_length):\n",
    "        decoder_input = decoder_input.view(1, 1, vocabularySize)\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "        \n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        predicted_word_index = int(topi[0][0][0])\n",
    "\n",
    "        # This is the next input, without teacher forcing it's the predicted output\n",
    "        decoder_input = torch.from_numpy(embeddings[predicted_word_index])\n",
    "        decoder_input = Variable(decoder_input).cuda()\n",
    "        \n",
    "        # This is just to conform with the pytorch format..\n",
    "        # CrossEntropyLoss takes input1: (N, C) and input2: (N).\n",
    "        _, actual_word_index = target_variable[0][index_word].data.topk(1)\n",
    "        actual_word_index = Variable(actual_word_index)\n",
    "\n",
    "        # Compare current output to next \"target\" input\n",
    "        loss += criterion(decoder_output.view(1, decoder_output.size(2)), actual_word_index)\n",
    "        \n",
    "        # Stop on EOS\n",
    "        # Saw training went better without this\n",
    "        # if predicted_word_index == word2index['<EOS>']:\n",
    "        #   break\n",
    "            \n",
    "    loss.backward()\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    # index_word keeps track of the current word\n",
    "    # in case of break (EOS) and non-break (teacher-forcing), it'll be the actually count.\n",
    "    return loss.data[0] / index_word\n",
    "\n",
    "    \n",
    "    \n",
    "# Train the model and monitor the loss. Remember to use Adam optimizer and CrossEntropyLoss\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=0.0001)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=0.0001)\n",
    "criterion = nn.NLLLoss()  # Since my DecoderLSTM has LogSoftmax as final layer, use NLL loss here\n",
    "\n",
    "n_iters = len(train_sentences)\n",
    "print_every = 1000\n",
    "print_loss_total = 0\n",
    "start = time.time()\n",
    "\n",
    "for s_index in range(1, n_iters):\n",
    "    input_sentence = preprocess_one_hot(train_sentences[s_index])\n",
    "    n_words = input_sentence.shape[0]\n",
    "    input_sentence = torch.from_numpy(input_sentence)\n",
    "    input_sentence = input_sentence.view(1, n_words, vocabularySize)\n",
    "    input_sentence = Variable(input_sentence).cuda()\n",
    "    loss = train(input_sentence, encoder, encoder_optimizer, decoder, decoder_optimizer, criterion)\n",
    "    \n",
    "    print_loss_total += loss\n",
    "    \n",
    "    if s_index % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print('%s (%d %d%%) %.4f' % (timeSince(start, s_index / n_iters),\n",
    "                                     s_index, s_index / n_iters * 100, print_loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models\n",
    "    1. \n",
    "\"\"\"\n",
    "torch.save(encoder.state_dict(), './model/q6_encoder_116000')\n",
    "torch.save(decoder.state_dict(), './model/q6_decoder_116000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Testing \n",
    "\n",
    "We must now define a method that allows us to do inference using the seq2seq architecture. We then run the 500 validation captions through this method, and ultimately compare the **reference** and **generated** sentences using our **BLEU** similarity score method defined above, to identify the average BLEU score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_inference(sentence, embeddings=one_hot_embeddings, max_length=maxSequenceLength):\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform inference for all validation sequences and report the average BLEU score\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Encoding as Generic Feature Representation\n",
    "\n",
    "We now use the final hidden state of our encoder, to identify the nearest neighbor amongst the training sentences for each sentence in our validation data.\n",
    "\n",
    "It would be effective to first define a method that would generate all of the hidden states and store these hidden states **on the CPU**, and then loop over the generated hidden states to identify/output the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_encoder_hidden(sentence):\n",
    "    # Your code goes here\n",
    "\n",
    "# Now run all training data and validation data to store hidden states\n",
    "    # Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now get nearest neighbors and print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Effectiveness of word2vec\n",
    "\n",
    "We now repeat everything done above using word2vec embeddings in place of one-hot embeddings. This will require re-running steps 1-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
