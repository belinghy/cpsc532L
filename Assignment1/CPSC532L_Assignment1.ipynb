{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics in Artificial Intelligence (CPSC 532L)\n",
    "# Assignment 1: Intriduction to Deep Learning\n",
    "\n",
    "\n",
    "This assignment is inspired and adopted from the Deep Learning Lab from Vicente Ordonez's course on Language and Vision at the University of Virginia. The link to the original lab is <a href=\"http://www.cs.virginia.edu/~vicente/vislang/notebooks/deep_learning_lab.html\">here</a>. The first two parts of this notebook are taken nearly verbatim from his lab. Section 1 is provided for you and implements a single-layer supervised neural network; no coding is required for Section 1. Section 2 and Section 3 are the ones where you need to code the solutions and provide answers to questions.\n",
    "\n",
    "# 1. Single-layer neural network\n",
    "This section provides simple implementation of the single-layer supervised neural network that has 4 inputs and 3 outputs. First, let's review the skeleton of a single linear layer neural network. The inputs of the network are the variables $x_1, x_2, x_3, x_4$, or the input vector $\\mathbf{x}=[x_1, x_2, x_3, x_4]$, the outputs of the network are $\\widehat{y}_1,\\widehat{y}_2,\\widehat{y}_3$, or the output vector $\\widehat{\\mathbf{y}}=[$$\\widehat{y}$$_1,\\widehat{y}_2,\\widehat{y}_3]$:\n",
    "\n",
    "<img src=\"1_layer_net.png\" width=\"450\"/>\n",
    "\n",
    "The given $j$-th output $\\widehat{y}_j$ of this single linear layer + activation function is computed as follows:\n",
    "\n",
    "$$\\widehat{y}_j= \\text{sigmoid}(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + w_{4j}x_4 + b_j) = \\text{sigmoid}\\Big(\\sum_{i=1}^{i=4}w_{ij}x_{i} + b_j\\Big)$$\n",
    "\n",
    "In matrix notation, this would be: \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  \\widehat{y}_{1} \\\\ \n",
    "  \\widehat{y}_{2} \\\\\n",
    "  \\widehat{y}_{3} \n",
    "\\end{bmatrix}^T=\\mathbf{Sigmoid}\\Bigg(\n",
    "\\begin{bmatrix}\n",
    "  x_{1} \\\\\n",
    "  x_{2} \\\\\n",
    "  x_{3} \\\\\n",
    "  x_{4}\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "  w_{1,1} & w_{1,2} & w_{1,3}\\\\\n",
    "  w_{2,1} & w_{3,2} & w_{2,3}\\\\\n",
    "  w_{3,1} & w_{3,2} & w_{3,3}\\\\\n",
    "  w_{4,1} & w_{4,2} & w_{4,3}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "  b_{1} \\\\\n",
    "  b_{2} \\\\\n",
    "  b_{3}\n",
    "\\end{bmatrix}^T\\Bigg)\n",
    "\\end{equation}\n",
    "\n",
    "or more compactly:\n",
    "\n",
    "\\begin{equation}\n",
    "\\widehat{\\mathbf{y}}^T = \\mathbf{Sigmoid}(\\mathbf{x}^T \\cdot \\mathbf{W} + \\mathbf{b}^T)\n",
    "\\end{equation}\n",
    "\n",
    "The element-wise sigmoid function is: $\\mathbf{Sigmoid}(\\mathbf{x}) = 1 \\;/\\; (1 + exp(-\\mathbf{x}))$, or alternatively: $\\mathbf{Sigmoid}(\\mathbf{x}) = exp(\\mathbf{x})\\;/\\;(1 + exp(\\mathbf{x}))$. Here the sigmoid is separated logically into an activation layer $\\sigma(x)$ and a linear layer $\\text{linear}(3,4)$ as illustrated in figure. \n",
    "\n",
    "Training these weights $\\mathbf{W}$ and biases $\\mathbf{b}$ requires having many training pairs $(\\widehat{\\mathbf{y}}^{(m)}, \\mathbf{x}^{(m)})$. The inputs $\\mathbf{x}$ can be the pixels of an image, indices of words, the entries in a database, and the outputs $\\widehat{\\mathbf{y}}$ can also be literally anything, including a number indicating a category, a set of numbers indicating the indices of words composing a sentence, an output image itself, etc.\n",
    "\n",
    "## 1.1. Forward-propagation\n",
    "\n",
    "Computing the outputs $\\widehat{\\mathbf{y}}$ from the inputs $\\mathbf{x}$ in this network composed of a single linear layer, and a sigmoid layer, is called forward-propagation. Below is the code that implements these two operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.001\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.001\n",
    "        \n",
    "    # y = Wx + b\n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def getParameters(self):\n",
    "        return [self.weight, self.bias]\n",
    "\n",
    "# Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y_hat1 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x1))\n",
    "print('x[1] = '+ str(x1))\n",
    "print('y_hat[1] = ' + str(y_hat1) + '\\n')\n",
    "\n",
    "# Let's test the composition of the two functions (forward-propagation in the neural network).\n",
    "x2 = np.array([[4, 5, 2, 1]])\n",
    "y_hat2 = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x2))\n",
    "print('x[2] = '+ str(x2))\n",
    "print('y_hat[2] = ' + str(y_hat2) + '\\n')\n",
    "\n",
    "# We can also compute both at once, which could be more efficient since it requires a single matrix multiplication.\n",
    "x = np.concatenate((x1, x2), axis = 0)\n",
    "y_hat = nn_Sigmoid().forward(nn_Linear(4, 3).forward(x))\n",
    "print('x = ' + str(x))\n",
    "print('y_hat = ' + str(y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Loss functions.\n",
    "\n",
    "After computing the output predictions $\\widehat{\\mathbf{y}}$ it is necessary to compare these against the true values of $\\mathbf{y}$. Let's call these true, correct, or desired values $\\mathbf{y}$. Typically, a simple loss or cost function is used to measure the degree by which the prediction $\\widehat{\\mathbf{y}}$ is wrong with respect to $\\mathbf{y}$. A common loss function for regression is the sum of squared differences between the prediction and its true value. Assuming a prediction $\\widehat{\\mathbf{y}}^{(d)}$ for our training sample $\\mathbf{x}^{(d)}$ with true value $\\mathbf{y}^{(d)}$, then the loss can be computed as:\n",
    "\n",
    "$$loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)}) = (\\widehat{y}^{(d)}_1 - y^{(d)}_1)^2 + (\\widehat{y}^{(d)}_2 - y^{(d)}_2)^2 + (\\widehat{y}^{(d)}_3 - y^{(d)}_3)^2 = \\sum_{j=1}^{j=3}(\\widehat{y}^{(d)}_j - y^{(d)}_j)^2$$\n",
    "\n",
    "The goal is to modify the parameters [$\\mathbf{W}, \\mathbf{b}$] in the Linear layer so that the value of $loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)})$ becomes as small as possible for all training samples in a set $D=\\{(\\mathbf{x}^{(d)},\\mathbf{y}^{(d)})\\}$. This would in turn ensure that predictions $\\widehat{\\mathbf{y}}$ are as similar as possible to the true values $\\mathbf{y}$. To achieve this we need to minimize the following function:\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{W}, \\mathbf{b}) = \\sum_{d=1}^{d=|D|} loss(\\widehat{\\mathbf{y}}^{(d)}, \\mathbf{y}^{(d)})$$\n",
    "\n",
    "The only two variables for our model in the function $\\mathcal{L}(\\mathbf{W}, \\mathbf{b})$ are $\\mathbf{W}$ and $\\mathbf{b}$, this is because the training dataset $D$ is fixed. Finding the values of $\\mathbf{W}$ and $\\mathbf{b}$ that minimize the the loss, particularly for complex functions, is typically done using gradient based optimization, like Stochastic Gradient Descent (SGD). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nn_MSECriterion:  # MSE = mean squared error.\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "\n",
    "# Let's test the loss function.\n",
    "y_hat = np.array([[0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33], [0.23, 0.25, 0.33]])\n",
    "y_true = np.array([[0.25, 0.25, 0.25], [0.33, 0.33, 0.33], [0.77, 0.77, 0.77], [0.80, 0.80, 0.80]])\n",
    "\n",
    "nn_MSECriterion().forward(y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3. Backward-propagation (Backpropagation)\n",
    "\n",
    "As we discussed in class, backpropagation is just applying the chain-rule in calculus to compute the derivative of a function which is the composition of many functions (this is essentially definition of the neural network). \n",
    "\n",
    "Below is the implementation of required derivative computations for our simple network. You are highly advised to derive the derivatives implemented here to make sure you understand how one arrives at them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is referred above as f(u).\n",
    "class nn_MSECriterion:\n",
    "    def forward(self, predictions, labels):\n",
    "        return np.sum(np.square(predictions - labels))\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        num_samples = labels.shape[0]\n",
    "        return num_samples * 2 * (predictions - labels)\n",
    "\n",
    "# This is referred above as g(v).\n",
    "class nn_Sigmoid:\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # It is usually a good idea to use gv from the forward pass and not recompute it again here.\n",
    "        gv = 1 / (1 + np.exp(-x))  \n",
    "        return np.multiply(np.multiply(gv, (1 - gv)), gradOutput)\n",
    "\n",
    "# This is referred above as h(W, b)\n",
    "class nn_Linear:\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        # Initialized with random numbers from a gaussian N(0, 0.001)\n",
    "        self.weight = np.matlib.randn(input_dim, output_dim) * 0.01\n",
    "        self.bias = np.matlib.randn((1, output_dim)) * 0.01\n",
    "        self.gradWeight = np.zeros_like(self.weight)\n",
    "        self.gradBias = np.zeros_like(self.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return np.dot(x, self.weight) + self.bias\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # dL/dw = dh/dw * dL/dv\n",
    "        self.gradWeight = np.dot(x.T, gradOutput)\n",
    "        # dL/db = dh/db * dL/dv\n",
    "        self.gradBias = np.copy(gradOutput)\n",
    "        # return dL/dx = dh/dx * dL/dv\n",
    "        return np.dot(gradOutput, self.weight.T)\n",
    "    \n",
    "    def getParameters(self):\n",
    "        params = [self.weight, self.bias]\n",
    "        gradParams = [self.gradWeight, self.gradBias]\n",
    "        return params, gradParams\n",
    "    \n",
    "\n",
    "# Let's test some dummy inputs for a full pass of forward and backward propagation.\n",
    "x1 = np.array([[1, 2, 2, 3]])\n",
    "y1 = np.array([[0.25, 0.25, 0.25]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)  # h(W, b)\n",
    "sigmoid = nn_Sigmoid()  # g(v)\n",
    "loss = nn_MSECriterion()  # f(u)\n",
    "\n",
    "# Forward-propagation.\n",
    "lin = linear.forward(x1)\n",
    "y_hat = sigmoid.forward(lin)\n",
    "loss_val = loss.forward(y_hat, y1) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "dy_hat = loss.backward(y_hat, y1)\n",
    "dlin = sigmoid.backward(lin, dy_hat)\n",
    "dx1 = linear.backward(x1, dlin)\n",
    "\n",
    "# Show parameters of the linear layer.\n",
    "print('\\nW = ' + str(linear.weight))\n",
    "print('B = ' + str(linear.bias))\n",
    "\n",
    "# Show the intermediate outputs in the forward pass.\n",
    "print('\\nx1    = '+ str(x1))\n",
    "print('lin   = ' + str(lin))\n",
    "print('y_hat = ' + str(y_hat))\n",
    "\n",
    "print('\\nloss = ' + str(loss_val))\n",
    "\n",
    "# Show the intermediate gradients with respect to inputs in the backward pass.\n",
    "print('\\ndy_hat = ' + str(dy_hat))\n",
    "print('dlin   = ' + str(dlin))\n",
    "print('dx1    = ' + str(dx1))\n",
    "\n",
    "# Show the gradients with respect to parameters.\n",
    "print('\\ndW = ' + str(linear.gradWeight))\n",
    "print('dB = ' + str(linear.gradBias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1.4. Gradient checking \n",
    "\n",
    "The gradients can also be computed with numerical approximation using the definition of derivatives. Let a single input pair $(\\mathbf{x}, \\mathbf{y})$ be the input, for each entry $w_{ij}$ in the weight matrix $\\mathbf{W}$, the partial derivatives can be computed as follows:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial w_{ij}} = \\frac{\\mathcal{L}(\\mathbf{W} + \\mathbf{E}_{ij},b) - \\mathcal{L}(\\mathbf{W} - \\mathbf{E}_{ij}, b)}{2\\epsilon}, $$\n",
    "\n",
    "where $\\mathbf{E}_{ij}$ is a matrix that has $\\epsilon$ in its $(i,j)$ entry and zeros everywhere else. Intuitively this gradient tells how would the value of the loss changes if we change a particular weight $w_{ij}$ by an $\\epsilon$ amount. We can do the same to compute derivatives with respect to the bias parameters $b_i$. Below is the code that checks for a given input $(\\mathbf{x}, \\mathbf{y})$, the gradients for the matrix $\\mathbf{W}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We will compute derivatives with respect to a single data pair (x,y)\n",
    "x = np.array([[2.34, 3.8, 34.44, 5.33]])\n",
    "y = np.array([[3.2, 4.2, 5.3]])\n",
    "\n",
    "# Define the operations.\n",
    "linear = nn_Linear(4, 3)\n",
    "sigmoid = nn_Sigmoid()\n",
    "criterion = nn_MSECriterion()\n",
    "\n",
    "# Forward-propagation.\n",
    "a0 = linear.forward(x)\n",
    "a1 = sigmoid.forward(a0)\n",
    "loss = criterion.forward(a1, y) # Loss function.\n",
    "\n",
    "# Backward-propagation.\n",
    "da1 = criterion.backward(a1, y)\n",
    "da0 = sigmoid.backward(a0, da1)\n",
    "dx = linear.backward(x, da0)\n",
    "\n",
    "gradWeight = linear.gradWeight\n",
    "gradBias = linear.gradBias\n",
    "\n",
    "approxGradWeight = np.zeros_like(linear.weight)\n",
    "approxGradBias = np.zeros_like(linear.bias)\n",
    "\n",
    "# We will verify here that gradWeights are correct and leave it as an excercise\n",
    "# to verify the gradBias.\n",
    "epsilon = 0.0001\n",
    "for i in range(0, linear.weight.shape[0]):\n",
    "    for j in range(0, linear.weight.shape[1]):\n",
    "        # Compute f(w)\n",
    "        fw = criterion.forward(sigmoid.forward(linear.forward(x)), y) # Loss function.\n",
    "        # Compute f(w + eps)\n",
    "        shifted_weight = np.copy(linear.weight)\n",
    "        shifted_weight[i, j] = shifted_weight[i, j] + epsilon\n",
    "        shifted_linear = nn_Linear(4, 3)\n",
    "        shifted_linear.bias = linear.bias\n",
    "        shifted_linear.weight = shifted_weight\n",
    "        fw_epsilon = criterion.forward(sigmoid.forward(shifted_linear.forward(x)), y) # Loss function\n",
    "        # Compute (f(w + eps) - f(w)) / eps\n",
    "        approxGradWeight[i, j] = (fw_epsilon - fw) / epsilon\n",
    "\n",
    "# These two outputs should be similar up to some precision.\n",
    "print('gradWeight: ' + str(gradWeight))\n",
    "print('\\napproxGradWeight: ' + str(approxGradWeight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Stochastic Gradient Descent.\n",
    "\n",
    "The code below creates a dummy dataset that will be used for training. The inputs are 1000 vectors of size 4, and the outputs are 1000 vectors of size 3. The focus here is on training, however, in a real task one would check accuracy of the model on test (unseen) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_size = 1000\n",
    "\n",
    "# Generate random inputs within some range.\n",
    "x = np.random.uniform(0, 6, (dataset_size, 4))\n",
    "# Generate outputs based on the inputs using some function.\n",
    "y1 = np.sin(x.sum(axis = 1))\n",
    "y2 = np.sin(x[:, 1] * 6)\n",
    "y3 = np.sin(x[:, 1] + x[:, 3])\n",
    "y = np.array([y1, y2, y3]).T\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn the parameters efficiently we will implement the stochastic gradient descent loop that moves the weights according to the gradients. In each iteration we sample an $(\\mathbf{x}, \\mathbf{y})$ pair and compute the gradients of the parameters, then we update the parameters according to the following gradient descent rules:\n",
    "\n",
    "$$w_{ij} = w_{ij} - \\lambda\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial w_{ij}}$$\n",
    "\n",
    "$$b_i = b_i - \\lambda\\frac{\\partial \\mathcal{L}(\\mathbf{W},\\mathbf{b})}{\\partial b_i}$$\n",
    "\n",
    "where $\\lambda$ is the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learningRate = 0.1\n",
    "\n",
    "model = {}\n",
    "model['linear'] = nn_Linear(4, 3)\n",
    "model['sigmoid'] = nn_Sigmoid()\n",
    "model['loss'] = nn_MSECriterion()\n",
    "\n",
    "for epoch in range(0, 400):\n",
    "    loss = 0\n",
    "    for i in range(0, dataset_size):\n",
    "        xi = x[i:i+1, :]\n",
    "        yi = y[i:i+1, :]\n",
    "\n",
    "        # Forward.\n",
    "        a0 = model['linear'].forward(xi)\n",
    "        a1 = model['sigmoid'].forward(a0)\n",
    "        loss += model['loss'].forward(a1, yi)\n",
    "\n",
    "        # Backward.\n",
    "        da1 = model['loss'].backward(a1, yi)\n",
    "        da0 = model['sigmoid'].backward(a0, da1)\n",
    "        model['linear'].backward(xi, da0)\n",
    "\n",
    "        model['linear'].weight = model['linear'].weight - learningRate * model['linear'].gradWeight\n",
    "        model['linear'].bias = model['linear'].bias - learningRate * model['linear'].gradBias\n",
    "    \n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, loss / dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Two-layer neural network with 1-hidden layer.\n",
    "Section 1 implemented a single layer neural network that takes as input vectors of size 4, and outputs vectors of size 3. Here your task is to modify the code to train a two layer network with one hidden layer of size hidden_state_size (note that this is a parameter and should be something you can change). You will need to handin your code for this and remaining parts of the notebook; the notebook you hand in should also have code executed and result saved.  \n",
    "\n",
    "<img src=\"2_layer_net.png\" width=\"650\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Implementing the network and SGD learning [20 points].\n",
    "Please modify the code of Section 1.5 to implement a two-layer network and the SGD training procedure for it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 5;\n",
    "\n",
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Gradient checking [10 points].\n",
    "Check the gradients of the above network for both linear layer parameters $W_1$ and $W_2$ using some sample input pair ($\\mathbf{x}$, $\\mathbf{y}$). You will likely want to look and model this after Section 1.4 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Activation functions [10 points].\n",
    "\n",
    "As was covered in class, there are other activation functions that can be used instead of sigmoid. Implement below the forward and backward operation for two popular activation functions.\n",
    "\n",
    "$$\\text{ReLU}(\\mathbf{x}) = \\text{max}(0, \\mathbf{x})$$\n",
    "\n",
    "$$\\text{Tanh($\\mathbf{x}$)} = \\text{tanh}(\\mathbf{x}) = \\frac{e^{\\mathbf{x}} - e^{-\\mathbf{x}}}{e^{\\mathbf{x}} + e^{-\\mathbf{x}}}$$\n",
    "\n",
    "Note, that in the above activations are applied element-wise on the input vector $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Rectified linear unit\n",
    "class nn_ReLU:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # Backward pass\n",
    "        \n",
    "# Hyperbolic tangent.\n",
    "class nn_Tanh:\n",
    "    def forward(self, x):\n",
    "        # Forward pass.\n",
    "    \n",
    "    def backward(self, x, gradOutput):\n",
    "        # Backward pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Loss functions [10 points].\n",
    "\n",
    "As discussed in class, there are other loss functions that can be used instead of a mean squared error. Implement the forward and backward operations for the following very common loss function where $\\widehat{\\mathbf{y}}$ is a vector of predicted values, and $\\mathbf{y}$ is the vector with ground-truth labels. Assume both vectors are of size $n$. \n",
    "\n",
    "$$\\text{BinaryCrossEntropy}(\\mathbf{y}, \\widehat{\\mathbf{y}}) = \\frac{1}{n}\\sum_{i=1}^{i=n} [y_i\\text{log}(\\widehat{y}_i) + (1 - y_i)\\text{log}(1 - \\widehat{y}_i)]$$,\n",
    "\n",
    "The binary cross entropy loss assumes that the vector $\\mathbf{y}$ only has values that are either 0 and 1, and the prediction vector $\\widehat{\\mathbf{y}}$ contains values between 0 and 1 (e.g. the output of a $\\text{sigmoid}$ layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy criterion. Useful for classification as opposed to regression.\n",
    "class nn_BCECriterion:\n",
    "    def forward(self, predictions, labels):\n",
    "        # Forward pass.\n",
    "        \n",
    "    def backward(self, predictions, labels):\n",
    "        # Backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional 1</b> [15 points]: Most deep learning libraries support batches, meaning you can forward, and backward a set of inputs. So far the code supports batches in the forward pass. However, the backward pass does not support batches. Modify the code in backward function of the nn_Linear class to support batches. Then test the implementation by training the network in Section 2.1 using a batch size of 32. <span style=\"color:#666\">(Keep in mind that the gradWeight and gradBias vectors should accumulate the gradients of each input in the batch. This is because the gradient of the loss with respect to the batch is the sum of the gradients with respect to each sample in the batch).</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Behaviour of neural networks [10 points].\n",
    "\n",
    "Prior to this section, all experiments were done in a dummy dataset where it is difficult to see how neural networks behave on more realistic data. In this section the goal is to get a feel for how newural networks behave and what effect hidden statest may play. Below is the code that generates and visualizes a classification dataset of 400 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import sklearn.datasets\n",
    "\n",
    "def load_planar_dataset():\n",
    "    np.random.seed(1)\n",
    "    m = 400 # number of examples\n",
    "    N = int(m/2) # number of points per class\n",
    "    D = 2 # dimensionality\n",
    "    X = np.zeros((m,D)) # data matrix where each row is a single example\n",
    "    Y = np.zeros((m,1), dtype='uint8') # labels vector (0 for red, 1 for blue)\n",
    "    a = 4 # maximum ray of the flower\n",
    "\n",
    "    for j in range(2):\n",
    "        ix = range(N*j,N*(j+1))\n",
    "        t = np.linspace(j*3.12,(j+1)*3.12,N) + np.random.randn(N)*0.2 # theta\n",
    "        r = a*np.sin(4*t) + np.random.randn(N)*0.2 # radius\n",
    "        X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "        Y[ix] = j\n",
    "        \n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "# Load the data\n",
    "X, Y = load_planar_dataset();\n",
    "\n",
    "# Visualize the data:\n",
    "plt.scatter(X[0, :], X[1, :], c=Y[0, :], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Two-layer neural network with 1-hidden layer of size = 1.\n",
    "\n",
    "Reimplement the network from Section 2.1 here, train it and then display the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build a two-layer neural network (so one hidden layer) with sigmoid activations \n",
    "# and MSE loss. The hidden_state_dimensionality should be set to 1 using the variable\n",
    "# below.\n",
    "hidden_state_size = 1; \n",
    "\n",
    "# Define the 2-layer network here (fill in yout code)\n",
    "\n",
    "\n",
    "# Optimize the parameters of the neural network using stochastic gradient descent\n",
    "# using the following parameters\n",
    "\n",
    "learningRate = 0.01\n",
    "numberEpochs = 300\n",
    "\n",
    "for epoch in range(0, numberEpochs):\n",
    "    loss = 0\n",
    "    for i in range(0, Y.size):        \n",
    "        # Forward pass (fill in your code)\n",
    "       \n",
    "        # Backward pass (fill in your code)\n",
    "        \n",
    "        # Update gradients (fill in your code)\n",
    "\n",
    "    if epoch % 10 == 0: print('epoch[%d] = %.8f' % (epoch, loss / dataset_size))\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "classEstimate = np.zeros((400,1), dtype='uint8')\n",
    "\n",
    "for i in range(0, 400):        \n",
    "    # Forward pass (fill in your code)\n",
    "       \n",
    "        \n",
    "    classEstimate[i,0] = (y_hat > 0.5)\n",
    "\n",
    "plt.scatter(X[0, :], X[1, :], c=classEstimate[:,0], s=40, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Two-layer neural network with 1-hidden layer of size = 5.\n",
    "\n",
    "Redo the experiment with the hidden layer of size 5 and visualize the result. <b>Describe in a separate cell of the notebook what is different between the two runs </b>. What behaviout did network with largerr hidden state exhibit that the one with smaller one did not? Why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_state_size = 5; \n",
    "\n",
    "# Rest should be taken from above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:0.8em;color:#888;text-align:center;padding-top:20px;\">If you find any errors or have questions about the assignment please contact instructor at lsigal@cs.ubc.ca or post the question on Piazza. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
